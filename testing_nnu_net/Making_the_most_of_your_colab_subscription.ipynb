{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "zKJmz5nY9Tn5"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZ_5zZtZ9Tkb",
        "outputId": "818838bc-7aa0-45bc-9f69-4ab21433576c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Dec 22 21:33:39 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   28C    P0             47W /  400W |       0MiB /  40960MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install -U nnunetv2 nibabel tifffile simpleitk blosc2 kaggle matplotlib\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "sDfesVEd9Tfv",
        "outputId": "7618e06f-d37e-46d2-9bfb-8db2aa6610b2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/211.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m204.8/211.5 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.8/52.8 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.4/86.4 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.0/232.0 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.6/52.6 MB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m256.4/256.4 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m134.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m88.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.6/13.6 MB\u001b[0m \u001b[31m134.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.2/23.2 MB\u001b[0m \u001b[31m103.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.3/159.3 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.0/189.0 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m83.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.4/96.4 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for nnunetv2 (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for acvl-utils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for batchgenerators (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for batchgeneratorsv2 (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for dynamic-network-architectures (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 3 — Set up Kaggle authentication\n",
        "\n",
        "Why: Kaggle downloads require kaggle.json API key.\n",
        "\n",
        "What you must do:\n",
        "\n",
        "Kaggle → Account → Create New API Token\n",
        "\n",
        "It downloads kaggle.json\n",
        "\n",
        "Upload kaggle.json into Colab (Files panel)"
      ],
      "metadata": {
        "id": "uExZ9Eul_FWn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, pathlib, stat\n",
        "\n",
        "kaggle_json = \"/content/kaggle.json\"\n",
        "assert os.path.exists(kaggle_json), \"Upload kaggle.json to /content/ first (Files panel).\"\n",
        "\n",
        "pathlib.Path(\"/root/.kaggle\").mkdir(parents=True, exist_ok=True)\n",
        "!cp /content/kaggle.json /root/.kaggle/kaggle.json\n",
        "!chmod 600 /root/.kaggle/kaggle.json\n",
        "\n",
        "print(\"Kaggle configured.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDLqV3GY9TW8",
        "outputId": "290b38f0-1bed-47d9-db5c-cd5aff39cae8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kaggle configured.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download dataset from Kaggle + unzip\n",
        "\n",
        "Why: brings the data into Colab storage."
      ],
      "metadata": {
        "id": "Bv45BFJL_U6W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "KAGGLE_DATASET = \"murillobouzon/kssd2025-kidney-stone-segmentation-dataset\"  # <-- CHANGE THIS\n",
        "OUT_DIR = \"/content/KSSD2025_kaggle\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "!kaggle datasets download -d {KAGGLE_DATASET} -p {OUT_DIR} --unzip\n",
        "\n",
        "print(\"Downloaded and unzipped to:\", OUT_DIR)\n",
        "!find {OUT_DIR} -maxdepth 3 -type d | head -n 50"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XNuiEDbV_JmH",
        "outputId": "95e89c1f-8940-4887-8957-55d172cdfde8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/murillobouzon/kssd2025-kidney-stone-segmentation-dataset\n",
            "License(s): unknown\n",
            "Downloading kssd2025-kidney-stone-segmentation-dataset.zip to /content/KSSD2025_kaggle\n",
            " 93% 73.0M/78.5M [00:00<00:00, 143MB/s]\n",
            "100% 78.5M/78.5M [00:00<00:00, 146MB/s]\n",
            "Downloaded and unzipped to: /content/KSSD2025_kaggle\n",
            "/content/KSSD2025_kaggle\n",
            "/content/KSSD2025_kaggle/data\n",
            "/content/KSSD2025_kaggle/data/label\n",
            "/content/KSSD2025_kaggle/data/image\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Locate images/labels folders\n",
        "\n",
        "Why: Kaggle zips sometimes have different folder names. We auto-detect."
      ],
      "metadata": {
        "id": "l0b0meHh_zuM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, glob\n",
        "\n",
        "def find_folder(root, candidates):\n",
        "    for c in candidates:\n",
        "        p = os.path.join(root, c)\n",
        "        if os.path.isdir(p):\n",
        "            return p\n",
        "    return None\n",
        "\n",
        "root = OUT_DIR\n",
        "\n",
        "# Common patterns (edit if your Kaggle zip uses different names)\n",
        "image_candidates = [\n",
        "    \"dataset/image\", \"dataset/images\", \"images\", \"image\", \"data/image\", \"data/images\"\n",
        "]\n",
        "label_candidates = [\n",
        "    \"dataset/label\", \"dataset/labels\", \"labels\", \"label\", \"data/label\", \"data/labels\", \"mask\", \"masks\"\n",
        "]\n",
        "\n",
        "img_dir = find_folder(root, image_candidates)\n",
        "lbl_dir = find_folder(root, label_candidates)\n",
        "\n",
        "# If not found, try recursive search for folders containing .tif\n",
        "if img_dir is None:\n",
        "    tif_dirs = {}\n",
        "    for p in glob.glob(root + \"/**/*.tif\", recursive=True):\n",
        "        d = os.path.dirname(p)\n",
        "        tif_dirs[d] = tif_dirs.get(d, 0) + 1\n",
        "    # pick top tif folder as image guess\n",
        "    if tif_dirs:\n",
        "        img_dir = sorted(tif_dirs.items(), key=lambda x: -x[1])[0][0]\n",
        "\n",
        "print(\"img_dir:\", img_dir)\n",
        "print(\"lbl_dir:\", lbl_dir)\n",
        "\n",
        "# sanity list\n",
        "if img_dir:\n",
        "    print(\"Sample images:\", sorted(os.listdir(img_dir))[:10])\n",
        "if lbl_dir:\n",
        "    print(\"Sample labels:\", sorted(os.listdir(lbl_dir))[:10])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7R_ixMLA_dhD",
        "outputId": "f64330b3-4edf-4b13-caec-e61ec468c0bb"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "img_dir: /content/KSSD2025_kaggle/data/image\n",
            "lbl_dir: /content/KSSD2025_kaggle/data/label\n",
            "Sample images: ['1.tif', '10.tif', '1000.tif', '1001.tif', '1002.tif', '1003.tif', '1012.tif', '1013.tif', '1014.tif', '1015.tif']\n",
            "Sample labels: ['1.tif', '10.tif', '1000.tif', '1001.tif', '1002.tif', '1003.tif', '1012.tif', '1013.tif', '1014.tif', '1015.tif']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set nnU-Net directories"
      ],
      "metadata": {
        "id": "JTHWYJnb_7BD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "BASE = \"/content/nnUNet\"\n",
        "os.environ[\"nnUNet_raw\"] = f\"{BASE}/nnUNet_raw\"\n",
        "os.environ[\"nnUNet_preprocessed\"] = f\"{BASE}/nnUNet_preprocessed\"\n",
        "os.environ[\"nnUNet_results\"] = f\"{BASE}/nnUNet_results\"\n",
        "\n",
        "for p in [os.environ[\"nnUNet_raw\"], os.environ[\"nnUNet_preprocessed\"], os.environ[\"nnUNet_results\"]]:\n",
        "    os.makedirs(p, exist_ok=True)\n",
        "\n",
        "print(\"nnUNet_raw:\", os.environ[\"nnUNet_raw\"])\n",
        "print(\"nnUNet_preprocessed:\", os.environ[\"nnUNet_preprocessed\"])\n",
        "print(\"nnUNet_results:\", os.environ[\"nnUNet_results\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pe83EEGq_3pE",
        "outputId": "98fbdc49-d84f-4239-e4f1-3d79166b9076"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nnUNet_raw: /content/nnUNet/nnUNet_raw\n",
            "nnUNet_preprocessed: /content/nnUNet/nnUNet_preprocessed\n",
            "nnUNet_results: /content/nnUNet/nnUNet_results\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create nnU-Net dataset folders"
      ],
      "metadata": {
        "id": "mt7CxUOEABM3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "DATASET_ID = 501\n",
        "DATASET_NAME = \"KSSD\"\n",
        "\n",
        "nn_folder = f\"{os.environ['nnUNet_raw']}/Dataset{DATASET_ID:03d}_{DATASET_NAME}\"\n",
        "imagesTr = f\"{nn_folder}/imagesTr\"\n",
        "labelsTr = f\"{nn_folder}/labelsTr\"\n",
        "\n",
        "os.makedirs(imagesTr, exist_ok=True)\n",
        "os.makedirs(labelsTr, exist_ok=True)\n",
        "\n",
        "print(\"nn_folder:\", nn_folder)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3wu2V7Nb_9Yt",
        "outputId": "0a0e2041-ab29-472b-9571-1eab13e8de24"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nn_folder: /content/nnUNet/nnUNet_raw/Dataset501_KSSD\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "convert .tif → .nii.gz with label remap"
      ],
      "metadata": {
        "id": "5a37YvokAKOx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import numpy as np\n",
        "import tifffile as tiff\n",
        "import nibabel as nib\n",
        "\n",
        "img_files = sorted(glob.glob(os.path.join(img_dir, \"*.tif\")))\n",
        "lbl_files = sorted(glob.glob(os.path.join(lbl_dir, \"*.tif\")))\n",
        "\n",
        "print(\"Found images:\", len(img_files))\n",
        "print(\"Found labels:\", len(lbl_files))\n",
        "assert len(img_files) == len(lbl_files), \"Mismatch counts. Check img_dir/lbl_dir.\"\n",
        "\n",
        "for i, (img_p, lbl_p) in enumerate(zip(img_files, lbl_files)):\n",
        "    img = tiff.imread(img_p).astype(np.float32)\n",
        "    lbl = tiff.imread(lbl_p).astype(np.uint8)\n",
        "\n",
        "    # IMPORTANT: binary remap based on your earlier discovery\n",
        "    lbl = (lbl >= 251).astype(np.uint8)\n",
        "\n",
        "    if img.ndim == 2:\n",
        "        img = img[..., None]\n",
        "    if lbl.ndim == 2:\n",
        "        lbl = lbl[..., None]\n",
        "\n",
        "    affine = np.eye(4)\n",
        "    case_id = f\"case_{i:04d}\"\n",
        "\n",
        "    nib.save(nib.Nifti1Image(img, affine), f\"{imagesTr}/{case_id}_0000.nii.gz\")\n",
        "    nib.save(nib.Nifti1Image(lbl, affine), f\"{labelsTr}/{case_id}.nii.gz\")\n",
        "\n",
        "print(\"Conversion finished.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KxgUNUUbAFGf",
        "outputId": "6200dce4-f38e-4812-8b99-313702f5b491"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found images: 838\n",
            "Found labels: 838\n",
            "Conversion finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Confirm labels are clean {0,1}"
      ],
      "metadata": {
        "id": "HOk50sGAARNs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import glob, numpy as np\n",
        "import nibabel as nib\n",
        "\n",
        "sample = sorted(glob.glob(labelsTr + \"/*.nii.gz\"))[0]\n",
        "u = np.unique(nib.load(sample).get_fdata())\n",
        "print(\"Unique label values in converted label:\", u)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gtvc8ScpAHqQ",
        "outputId": "22b2dd05-d024-479c-d71e-09570aca04d7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique label values in converted label: [0. 1.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write dataset.json\n",
        "\n",
        "Why: nnU-Net needs class names and file ending."
      ],
      "metadata": {
        "id": "ubFL8BVHAYPG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json, glob\n",
        "\n",
        "dataset_json = {\n",
        "    \"channel_names\": {\"0\": \"CT\"},\n",
        "    \"labels\": {\"background\": 0, \"stone\": 1},\n",
        "    \"numTraining\": len(glob.glob(imagesTr + \"/*.nii.gz\")),\n",
        "    \"file_ending\": \".nii.gz\"\n",
        "}\n",
        "\n",
        "with open(f\"{nn_folder}/dataset.json\", \"w\") as f:\n",
        "    json.dump(dataset_json, f, indent=2)\n",
        "\n",
        "print(\"Wrote:\", f\"{nn_folder}/dataset.json\")\n",
        "print(dataset_json)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2u-h02iAQc7",
        "outputId": "34f8e707-1a3e-4885-b3ee-a7381b7f0e76"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote: /content/nnUNet/nnUNet_raw/Dataset501_KSSD/dataset.json\n",
            "{'channel_names': {'0': 'CT'}, 'labels': {'background': 0, 'stone': 1}, 'numTraining': 838, 'file_ending': '.nii.gz'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plan + preprocess\n",
        "\n",
        "Why: nnU-Net auto-configures everything."
      ],
      "metadata": {
        "id": "EV9-PVz4Ag8x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nnUNetv2_plan_and_preprocess -d 501 --verify_dataset_integrity\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LDM9iTuPAa4X",
        "outputId": "bdb01d39-9eb1-4a3e-df47-21420241f917"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fingerprint extraction...\n",
            "Dataset501_KSSD\n",
            "Using <class 'nnunetv2.imageio.simpleitk_reader_writer.SimpleITKIO'> as reader/writer\n",
            "\n",
            "####################\n",
            "verify_dataset_integrity Done. \n",
            "If you didn't see any error messages then your dataset is most likely OK!\n",
            "####################\n",
            "\n",
            "Using <class 'nnunetv2.imageio.simpleitk_reader_writer.SimpleITKIO'> as reader/writer\n",
            "100% 838/838 [00:07<00:00, 109.23it/s]\n",
            "Experiment planning...\n",
            "\n",
            "############################\n",
            "INFO: You are using the old nnU-Net default planner. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md\n",
            "############################\n",
            "\n",
            "2D U-Net configuration:\n",
            "{'data_identifier': 'nnUNetPlans_2d', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 14, 'patch_size': (np.int64(512), np.int64(448)), 'median_image_size_in_voxels': array([512., 416.]), 'spacing': array([1., 1.]), 'normalization_schemes': ['CTNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 7, 'features_per_stage': (32, 64, 128, 256, 512, 512, 512), 'conv_op': 'torch.nn.modules.conv.Conv2d', 'kernel_sizes': ((3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3)), 'strides': ((1, 1), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2)), 'n_conv_per_stage': (2, 2, 2, 2, 2, 2, 2), 'n_conv_per_stage_decoder': (2, 2, 2, 2, 2, 2), 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm2d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}}, '_kw_requires_import': ('conv_op', 'norm_op', 'dropout_op', 'nonlin')}, 'batch_dice': True}\n",
            "\n",
            "Using <class 'nnunetv2.imageio.simpleitk_reader_writer.SimpleITKIO'> as reader/writer\n",
            "Plans were saved to /content/nnUNet/nnUNet_preprocessed/Dataset501_KSSD/nnUNetPlans.json\n",
            "Preprocessing...\n",
            "Preprocessing dataset Dataset501_KSSD\n",
            "Configuration: 2d...\n",
            "100% 838/838 [00:34<00:00, 24.46it/s]\n",
            "Configuration: 3d_fullres...\n",
            "INFO: Configuration 3d_fullres not found in plans file nnUNetPlans.json of dataset Dataset501_KSSD. Skipping.\n",
            "Configuration: 3d_lowres...\n",
            "INFO: Configuration 3d_lowres not found in plans file nnUNetPlans.json of dataset Dataset501_KSSD. Skipping.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nnUNetv2_train 501 2d 0\n"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h4y52u1iAfy3",
        "outputId": "5fcfba7e-957d-4473-9d40-e4ffc10b166a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "2025-12-23 00:02:37.455714: val_loss -0.9842\n",
            "2025-12-23 00:02:37.455827: Pseudo dice [np.float32(0.9882)]\n",
            "2025-12-23 00:02:37.455936: Epoch time: 22.41 s\n",
            "2025-12-23 00:02:38.898661: \n",
            "2025-12-23 00:02:38.898940: Epoch 344\n",
            "2025-12-23 00:02:38.899075: Current learning rate: 0.00684\n",
            "2025-12-23 00:03:01.339194: train_loss -0.9667\n",
            "2025-12-23 00:03:01.339449: val_loss -0.9781\n",
            "2025-12-23 00:03:01.339546: Pseudo dice [np.float32(0.984)]\n",
            "2025-12-23 00:03:01.339637: Epoch time: 22.44 s\n",
            "2025-12-23 00:03:02.768365: \n",
            "2025-12-23 00:03:02.768631: Epoch 345\n",
            "2025-12-23 00:03:02.768795: Current learning rate: 0.00683\n",
            "2025-12-23 00:03:25.214919: train_loss -0.9656\n",
            "2025-12-23 00:03:25.215166: val_loss -0.9825\n",
            "2025-12-23 00:03:25.215299: Pseudo dice [np.float32(0.987)]\n",
            "2025-12-23 00:03:25.215400: Epoch time: 22.45 s\n",
            "2025-12-23 00:03:26.587313: \n",
            "2025-12-23 00:03:26.587517: Epoch 346\n",
            "2025-12-23 00:03:26.587672: Current learning rate: 0.00682\n",
            "2025-12-23 00:03:48.975145: train_loss -0.9692\n",
            "2025-12-23 00:03:48.975554: val_loss -0.9813\n",
            "2025-12-23 00:03:48.975756: Pseudo dice [np.float32(0.9878)]\n",
            "2025-12-23 00:03:48.975903: Epoch time: 22.39 s\n",
            "2025-12-23 00:03:50.390834: \n",
            "2025-12-23 00:03:50.391152: Epoch 347\n",
            "2025-12-23 00:03:50.391328: Current learning rate: 0.00681\n",
            "2025-12-23 00:04:12.845727: train_loss -0.9691\n",
            "2025-12-23 00:04:12.846044: val_loss -0.9846\n",
            "2025-12-23 00:04:12.846137: Pseudo dice [np.float32(0.9882)]\n",
            "2025-12-23 00:04:12.846263: Epoch time: 22.46 s\n",
            "2025-12-23 00:04:14.232367: \n",
            "2025-12-23 00:04:14.232564: Epoch 348\n",
            "2025-12-23 00:04:14.232692: Current learning rate: 0.0068\n",
            "2025-12-23 00:04:36.689053: train_loss -0.9697\n",
            "2025-12-23 00:04:36.689414: val_loss -0.9829\n",
            "2025-12-23 00:04:36.689522: Pseudo dice [np.float32(0.9873)]\n",
            "2025-12-23 00:04:36.689614: Epoch time: 22.46 s\n",
            "2025-12-23 00:04:38.069088: \n",
            "2025-12-23 00:04:38.069478: Epoch 349\n",
            "2025-12-23 00:04:38.069626: Current learning rate: 0.0068\n",
            "2025-12-23 00:05:00.489141: train_loss -0.9689\n",
            "2025-12-23 00:05:00.489449: val_loss -0.9847\n",
            "2025-12-23 00:05:00.489549: Pseudo dice [np.float32(0.9881)]\n",
            "2025-12-23 00:05:00.489640: Epoch time: 22.42 s\n",
            "2025-12-23 00:05:02.327439: \n",
            "2025-12-23 00:05:02.327746: Epoch 350\n",
            "2025-12-23 00:05:02.327903: Current learning rate: 0.00679\n",
            "2025-12-23 00:05:24.799818: train_loss -0.9694\n",
            "2025-12-23 00:05:24.800243: val_loss -0.984\n",
            "2025-12-23 00:05:24.800363: Pseudo dice [np.float32(0.9882)]\n",
            "2025-12-23 00:05:24.800464: Epoch time: 22.47 s\n",
            "2025-12-23 00:05:26.176292: \n",
            "2025-12-23 00:05:26.176459: Epoch 351\n",
            "2025-12-23 00:05:26.176581: Current learning rate: 0.00678\n",
            "2025-12-23 00:05:48.625088: train_loss -0.97\n",
            "2025-12-23 00:05:48.625325: val_loss -0.9814\n",
            "2025-12-23 00:05:48.625428: Pseudo dice [np.float32(0.9879)]\n",
            "2025-12-23 00:05:48.625531: Epoch time: 22.45 s\n",
            "2025-12-23 00:05:48.625607: Yayy! New best EMA pseudo Dice: 0.9873999953269958\n",
            "2025-12-23 00:05:50.474282: \n",
            "2025-12-23 00:05:50.474648: Epoch 352\n",
            "2025-12-23 00:05:50.474787: Current learning rate: 0.00677\n",
            "2025-12-23 00:06:12.932883: train_loss -0.9692\n",
            "2025-12-23 00:06:12.933098: val_loss -0.9832\n",
            "2025-12-23 00:06:12.933194: Pseudo dice [np.float32(0.9879)]\n",
            "2025-12-23 00:06:12.933329: Epoch time: 22.46 s\n",
            "2025-12-23 00:06:12.933512: Yayy! New best EMA pseudo Dice: 0.9873999953269958\n",
            "2025-12-23 00:06:14.767960: \n",
            "2025-12-23 00:06:14.768308: Epoch 353\n",
            "2025-12-23 00:06:14.768443: Current learning rate: 0.00676\n",
            "2025-12-23 00:06:37.204491: train_loss -0.9703\n",
            "2025-12-23 00:06:37.204691: val_loss -0.982\n",
            "2025-12-23 00:06:37.204785: Pseudo dice [np.float32(0.9873)]\n",
            "2025-12-23 00:06:37.204877: Epoch time: 22.44 s\n",
            "2025-12-23 00:06:38.613402: \n",
            "2025-12-23 00:06:38.613773: Epoch 354\n",
            "2025-12-23 00:06:38.613930: Current learning rate: 0.00675\n",
            "2025-12-23 00:07:01.055713: train_loss -0.971\n",
            "2025-12-23 00:07:01.056053: val_loss -0.9831\n",
            "2025-12-23 00:07:01.056278: Pseudo dice [np.float32(0.9868)]\n",
            "2025-12-23 00:07:01.056457: Epoch time: 22.44 s\n",
            "2025-12-23 00:07:02.464087: \n",
            "2025-12-23 00:07:02.464305: Epoch 355\n",
            "2025-12-23 00:07:02.464437: Current learning rate: 0.00674\n",
            "2025-12-23 00:07:24.921930: train_loss -0.9706\n",
            "2025-12-23 00:07:24.922271: val_loss -0.982\n",
            "2025-12-23 00:07:24.922389: Pseudo dice [np.float32(0.9868)]\n",
            "2025-12-23 00:07:24.922519: Epoch time: 22.46 s\n",
            "2025-12-23 00:07:26.334996: \n",
            "2025-12-23 00:07:26.335354: Epoch 356\n",
            "2025-12-23 00:07:26.335505: Current learning rate: 0.00673\n",
            "2025-12-23 00:07:48.843652: train_loss -0.9696\n",
            "2025-12-23 00:07:48.843887: val_loss -0.9836\n",
            "2025-12-23 00:07:48.843976: Pseudo dice [np.float32(0.9878)]\n",
            "2025-12-23 00:07:48.844065: Epoch time: 22.51 s\n",
            "2025-12-23 00:07:50.229601: \n",
            "2025-12-23 00:07:50.229753: Epoch 357\n",
            "2025-12-23 00:07:50.230084: Current learning rate: 0.00672\n",
            "2025-12-23 00:08:12.714101: train_loss -0.967\n",
            "2025-12-23 00:08:12.714407: val_loss -0.9823\n",
            "2025-12-23 00:08:12.714512: Pseudo dice [np.float32(0.9869)]\n",
            "2025-12-23 00:08:12.714604: Epoch time: 22.49 s\n",
            "2025-12-23 00:08:14.808198: \n",
            "2025-12-23 00:08:14.808660: Epoch 358\n",
            "2025-12-23 00:08:14.808817: Current learning rate: 0.00671\n",
            "2025-12-23 00:08:37.327170: train_loss -0.9648\n",
            "2025-12-23 00:08:37.327483: val_loss -0.9804\n",
            "2025-12-23 00:08:37.327598: Pseudo dice [np.float32(0.9861)]\n",
            "2025-12-23 00:08:37.327693: Epoch time: 22.52 s\n",
            "2025-12-23 00:08:38.698340: \n",
            "2025-12-23 00:08:38.698585: Epoch 359\n",
            "2025-12-23 00:08:38.698719: Current learning rate: 0.0067\n",
            "2025-12-23 00:09:01.231040: train_loss -0.9567\n",
            "2025-12-23 00:09:01.231347: val_loss -0.9741\n",
            "2025-12-23 00:09:01.231446: Pseudo dice [np.float32(0.9782)]\n",
            "2025-12-23 00:09:01.231541: Epoch time: 22.53 s\n",
            "2025-12-23 00:09:02.612051: \n",
            "2025-12-23 00:09:02.612316: Epoch 360\n",
            "2025-12-23 00:09:02.612449: Current learning rate: 0.00669\n",
            "2025-12-23 00:09:25.067763: train_loss -0.9633\n",
            "2025-12-23 00:09:25.068007: val_loss -0.9822\n",
            "2025-12-23 00:09:25.068097: Pseudo dice [np.float32(0.9865)]\n",
            "2025-12-23 00:09:25.068186: Epoch time: 22.46 s\n",
            "2025-12-23 00:09:26.449564: \n",
            "2025-12-23 00:09:26.449953: Epoch 361\n",
            "2025-12-23 00:09:26.450085: Current learning rate: 0.00668\n",
            "2025-12-23 00:09:48.962620: train_loss -0.9635\n",
            "2025-12-23 00:09:48.962861: val_loss -0.9785\n",
            "2025-12-23 00:09:48.962956: Pseudo dice [np.float32(0.9835)]\n",
            "2025-12-23 00:09:48.963045: Epoch time: 22.51 s\n",
            "2025-12-23 00:09:50.374277: \n",
            "2025-12-23 00:09:50.374601: Epoch 362\n",
            "2025-12-23 00:09:50.374748: Current learning rate: 0.00667\n",
            "2025-12-23 00:10:12.850961: train_loss -0.9627\n",
            "2025-12-23 00:10:12.851192: val_loss -0.9807\n",
            "2025-12-23 00:10:12.851324: Pseudo dice [np.float32(0.9863)]\n",
            "2025-12-23 00:10:12.851628: Epoch time: 22.48 s\n",
            "2025-12-23 00:10:14.232849: \n",
            "2025-12-23 00:10:14.233073: Epoch 363\n",
            "2025-12-23 00:10:14.233235: Current learning rate: 0.00666\n",
            "2025-12-23 00:10:36.748862: train_loss -0.9659\n",
            "2025-12-23 00:10:36.749093: val_loss -0.9817\n",
            "2025-12-23 00:10:36.749192: Pseudo dice [np.float32(0.9859)]\n",
            "2025-12-23 00:10:36.749336: Epoch time: 22.52 s\n",
            "2025-12-23 00:10:38.182893: \n",
            "2025-12-23 00:10:38.183187: Epoch 364\n",
            "2025-12-23 00:10:38.183358: Current learning rate: 0.00665\n",
            "2025-12-23 00:11:00.641933: train_loss -0.9675\n",
            "2025-12-23 00:11:00.642374: val_loss -0.982\n",
            "2025-12-23 00:11:00.642476: Pseudo dice [np.float32(0.9876)]\n",
            "2025-12-23 00:11:00.642580: Epoch time: 22.46 s\n",
            "2025-12-23 00:11:02.018160: \n",
            "2025-12-23 00:11:02.018459: Epoch 365\n",
            "2025-12-23 00:11:02.018597: Current learning rate: 0.00665\n",
            "2025-12-23 00:11:24.457573: train_loss -0.9675\n",
            "2025-12-23 00:11:24.457853: val_loss -0.9825\n",
            "2025-12-23 00:11:24.457953: Pseudo dice [np.float32(0.9874)]\n",
            "2025-12-23 00:11:24.458044: Epoch time: 22.44 s\n",
            "2025-12-23 00:11:25.853904: \n",
            "2025-12-23 00:11:25.854180: Epoch 366\n",
            "2025-12-23 00:11:25.854369: Current learning rate: 0.00664\n",
            "2025-12-23 00:11:48.303949: train_loss -0.9673\n",
            "2025-12-23 00:11:48.304398: val_loss -0.9831\n",
            "2025-12-23 00:11:48.304624: Pseudo dice [np.float32(0.9877)]\n",
            "2025-12-23 00:11:48.304824: Epoch time: 22.45 s\n",
            "2025-12-23 00:11:49.684550: \n",
            "2025-12-23 00:11:49.684951: Epoch 367\n",
            "2025-12-23 00:11:49.685138: Current learning rate: 0.00663\n",
            "2025-12-23 00:12:12.175998: train_loss -0.969\n",
            "2025-12-23 00:12:12.176348: val_loss -0.9827\n",
            "2025-12-23 00:12:12.176452: Pseudo dice [np.float32(0.9866)]\n",
            "2025-12-23 00:12:12.176543: Epoch time: 22.49 s\n",
            "2025-12-23 00:12:13.541608: \n",
            "2025-12-23 00:12:13.541943: Epoch 368\n",
            "2025-12-23 00:12:13.542074: Current learning rate: 0.00662\n",
            "2025-12-23 00:12:35.966168: train_loss -0.9679\n",
            "2025-12-23 00:12:35.966413: val_loss -0.9816\n",
            "2025-12-23 00:12:35.966513: Pseudo dice [np.float32(0.9867)]\n",
            "2025-12-23 00:12:35.966606: Epoch time: 22.43 s\n",
            "2025-12-23 00:12:37.352921: \n",
            "2025-12-23 00:12:37.353179: Epoch 369\n",
            "2025-12-23 00:12:37.353348: Current learning rate: 0.00661\n",
            "2025-12-23 00:12:59.804400: train_loss -0.9687\n",
            "2025-12-23 00:12:59.804605: val_loss -0.9843\n",
            "2025-12-23 00:12:59.804695: Pseudo dice [np.float32(0.988)]\n",
            "2025-12-23 00:12:59.804803: Epoch time: 22.45 s\n",
            "2025-12-23 00:13:01.183584: \n",
            "2025-12-23 00:13:01.183760: Epoch 370\n",
            "2025-12-23 00:13:01.183888: Current learning rate: 0.0066\n",
            "2025-12-23 00:13:23.659731: train_loss -0.9688\n",
            "2025-12-23 00:13:23.660144: val_loss -0.9835\n",
            "2025-12-23 00:13:23.660327: Pseudo dice [np.float32(0.9875)]\n",
            "2025-12-23 00:13:23.660498: Epoch time: 22.48 s\n",
            "2025-12-23 00:13:25.044600: \n",
            "2025-12-23 00:13:25.044866: Epoch 371\n",
            "2025-12-23 00:13:25.044993: Current learning rate: 0.00659\n",
            "2025-12-23 00:13:47.481871: train_loss -0.9703\n",
            "2025-12-23 00:13:47.482087: val_loss -0.9829\n",
            "2025-12-23 00:13:47.482172: Pseudo dice [np.float32(0.9871)]\n",
            "2025-12-23 00:13:47.482301: Epoch time: 22.44 s\n",
            "2025-12-23 00:13:48.861504: \n",
            "2025-12-23 00:13:48.861719: Epoch 372\n",
            "2025-12-23 00:13:48.861849: Current learning rate: 0.00658\n",
            "2025-12-23 00:14:11.339667: train_loss -0.9684\n",
            "2025-12-23 00:14:11.339955: val_loss -0.9838\n",
            "2025-12-23 00:14:11.340052: Pseudo dice [np.float32(0.9877)]\n",
            "2025-12-23 00:14:11.340148: Epoch time: 22.48 s\n",
            "2025-12-23 00:14:12.736675: \n",
            "2025-12-23 00:14:12.736864: Epoch 373\n",
            "2025-12-23 00:14:12.737028: Current learning rate: 0.00657\n",
            "2025-12-23 00:14:35.218559: train_loss -0.9695\n",
            "2025-12-23 00:14:35.218908: val_loss -0.9836\n",
            "2025-12-23 00:14:35.219017: Pseudo dice [np.float32(0.9871)]\n",
            "2025-12-23 00:14:35.219112: Epoch time: 22.48 s\n",
            "2025-12-23 00:14:36.603408: \n",
            "2025-12-23 00:14:36.603622: Epoch 374\n",
            "2025-12-23 00:14:36.603761: Current learning rate: 0.00656\n",
            "2025-12-23 00:14:59.014522: train_loss -0.9708\n",
            "2025-12-23 00:14:59.014757: val_loss -0.9837\n",
            "2025-12-23 00:14:59.014936: Pseudo dice [np.float32(0.9885)]\n",
            "2025-12-23 00:14:59.015035: Epoch time: 22.41 s\n",
            "2025-12-23 00:15:01.063972: \n",
            "2025-12-23 00:15:01.064284: Epoch 375\n",
            "2025-12-23 00:15:01.064435: Current learning rate: 0.00655\n",
            "2025-12-23 00:15:23.552341: train_loss -0.9689\n",
            "2025-12-23 00:15:23.552571: val_loss -0.9821\n",
            "2025-12-23 00:15:23.552698: Pseudo dice [np.float32(0.9871)]\n",
            "2025-12-23 00:15:23.552827: Epoch time: 22.49 s\n",
            "2025-12-23 00:15:24.906724: \n",
            "2025-12-23 00:15:24.907069: Epoch 376\n",
            "2025-12-23 00:15:24.907198: Current learning rate: 0.00654\n",
            "2025-12-23 00:15:47.376314: train_loss -0.9624\n",
            "2025-12-23 00:15:47.376553: val_loss -0.9762\n",
            "2025-12-23 00:15:47.376642: Pseudo dice [np.float32(0.9834)]\n",
            "2025-12-23 00:15:47.376734: Epoch time: 22.47 s\n",
            "2025-12-23 00:15:48.740753: \n",
            "2025-12-23 00:15:48.740990: Epoch 377\n",
            "2025-12-23 00:15:48.741114: Current learning rate: 0.00653\n",
            "2025-12-23 00:16:11.215883: train_loss -0.9573\n",
            "2025-12-23 00:16:11.216133: val_loss -0.975\n",
            "2025-12-23 00:16:11.216303: Pseudo dice [np.float32(0.9827)]\n",
            "2025-12-23 00:16:11.216426: Epoch time: 22.48 s\n",
            "2025-12-23 00:16:12.609915: \n",
            "2025-12-23 00:16:12.610281: Epoch 378\n",
            "2025-12-23 00:16:12.610440: Current learning rate: 0.00652\n",
            "2025-12-23 00:16:35.083773: train_loss -0.9578\n",
            "2025-12-23 00:16:35.084166: val_loss -0.9709\n",
            "2025-12-23 00:16:35.084358: Pseudo dice [np.float32(0.9779)]\n",
            "2025-12-23 00:16:35.084523: Epoch time: 22.48 s\n",
            "2025-12-23 00:16:36.454941: \n",
            "2025-12-23 00:16:36.455211: Epoch 379\n",
            "2025-12-23 00:16:36.455381: Current learning rate: 0.00651\n",
            "2025-12-23 00:16:58.950041: train_loss -0.962\n",
            "2025-12-23 00:16:58.950516: val_loss -0.9765\n",
            "2025-12-23 00:16:58.950718: Pseudo dice [np.float32(0.9822)]\n",
            "2025-12-23 00:16:58.950893: Epoch time: 22.5 s\n",
            "2025-12-23 00:17:00.322441: \n",
            "2025-12-23 00:17:00.322766: Epoch 380\n",
            "2025-12-23 00:17:00.322898: Current learning rate: 0.0065\n",
            "2025-12-23 00:17:22.841698: train_loss -0.9643\n",
            "2025-12-23 00:17:22.841970: val_loss -0.9802\n",
            "2025-12-23 00:17:22.842084: Pseudo dice [np.float32(0.9857)]\n",
            "2025-12-23 00:17:22.842201: Epoch time: 22.52 s\n",
            "2025-12-23 00:17:24.252064: \n",
            "2025-12-23 00:17:24.252373: Epoch 381\n",
            "2025-12-23 00:17:24.252526: Current learning rate: 0.00649\n",
            "2025-12-23 00:17:46.721767: train_loss -0.9603\n",
            "2025-12-23 00:17:46.722040: val_loss -0.9807\n",
            "2025-12-23 00:17:46.722136: Pseudo dice [np.float32(0.9849)]\n",
            "2025-12-23 00:17:46.722250: Epoch time: 22.47 s\n",
            "2025-12-23 00:17:48.131811: \n",
            "2025-12-23 00:17:48.132205: Epoch 382\n",
            "2025-12-23 00:17:48.132363: Current learning rate: 0.00648\n",
            "2025-12-23 00:18:10.589776: train_loss -0.9625\n",
            "2025-12-23 00:18:10.589983: val_loss -0.9816\n",
            "2025-12-23 00:18:10.590073: Pseudo dice [np.float32(0.9857)]\n",
            "2025-12-23 00:18:10.590203: Epoch time: 22.46 s\n",
            "2025-12-23 00:18:11.967208: \n",
            "2025-12-23 00:18:11.967483: Epoch 383\n",
            "2025-12-23 00:18:11.967621: Current learning rate: 0.00648\n",
            "2025-12-23 00:18:34.434309: train_loss -0.9659\n",
            "2025-12-23 00:18:34.434747: val_loss -0.9825\n",
            "2025-12-23 00:18:34.434914: Pseudo dice [np.float32(0.9868)]\n",
            "2025-12-23 00:18:34.435030: Epoch time: 22.47 s\n",
            "2025-12-23 00:18:35.842063: \n",
            "2025-12-23 00:18:35.842271: Epoch 384\n",
            "2025-12-23 00:18:35.842428: Current learning rate: 0.00647\n",
            "2025-12-23 00:18:58.348928: train_loss -0.9645\n",
            "2025-12-23 00:18:58.349144: val_loss -0.982\n",
            "2025-12-23 00:18:58.349345: Pseudo dice [np.float32(0.9869)]\n",
            "2025-12-23 00:18:58.349480: Epoch time: 22.51 s\n",
            "2025-12-23 00:18:59.736427: \n",
            "2025-12-23 00:18:59.736668: Epoch 385\n",
            "2025-12-23 00:18:59.736821: Current learning rate: 0.00646\n",
            "2025-12-23 00:19:22.239358: train_loss -0.9661\n",
            "2025-12-23 00:19:22.239578: val_loss -0.9755\n",
            "2025-12-23 00:19:22.239666: Pseudo dice [np.float32(0.9821)]\n",
            "2025-12-23 00:19:22.239763: Epoch time: 22.5 s\n",
            "2025-12-23 00:19:23.614652: \n",
            "2025-12-23 00:19:23.614856: Epoch 386\n",
            "2025-12-23 00:19:23.614982: Current learning rate: 0.00645\n",
            "2025-12-23 00:19:46.030312: train_loss -0.9639\n",
            "2025-12-23 00:19:46.030596: val_loss -0.9817\n",
            "2025-12-23 00:19:46.030721: Pseudo dice [np.float32(0.9863)]\n",
            "2025-12-23 00:19:46.030816: Epoch time: 22.42 s\n",
            "2025-12-23 00:19:47.428444: \n",
            "2025-12-23 00:19:47.428725: Epoch 387\n",
            "2025-12-23 00:19:47.428879: Current learning rate: 0.00644\n",
            "2025-12-23 00:20:09.924970: train_loss -0.9661\n",
            "2025-12-23 00:20:09.925172: val_loss -0.983\n",
            "2025-12-23 00:20:09.925292: Pseudo dice [np.float32(0.9877)]\n",
            "2025-12-23 00:20:09.925395: Epoch time: 22.5 s\n",
            "2025-12-23 00:20:11.322871: \n",
            "2025-12-23 00:20:11.323103: Epoch 388\n",
            "2025-12-23 00:20:11.323252: Current learning rate: 0.00643\n",
            "2025-12-23 00:20:33.694727: train_loss -0.9681\n",
            "2025-12-23 00:20:33.694957: val_loss -0.9838\n",
            "2025-12-23 00:20:33.695048: Pseudo dice [np.float32(0.9876)]\n",
            "2025-12-23 00:20:33.695175: Epoch time: 22.37 s\n",
            "2025-12-23 00:20:35.125536: \n",
            "2025-12-23 00:20:35.125812: Epoch 389\n",
            "2025-12-23 00:20:35.125947: Current learning rate: 0.00642\n",
            "2025-12-23 00:20:57.552644: train_loss -0.9699\n",
            "2025-12-23 00:20:57.552842: val_loss -0.9839\n",
            "2025-12-23 00:20:57.552932: Pseudo dice [np.float32(0.988)]\n",
            "2025-12-23 00:20:57.553173: Epoch time: 22.43 s\n",
            "2025-12-23 00:20:58.956661: \n",
            "2025-12-23 00:20:58.956995: Epoch 390\n",
            "2025-12-23 00:20:58.957129: Current learning rate: 0.00641\n",
            "2025-12-23 00:21:21.374317: train_loss -0.9709\n",
            "2025-12-23 00:21:21.374638: val_loss -0.9828\n",
            "2025-12-23 00:21:21.374844: Pseudo dice [np.float32(0.9873)]\n",
            "2025-12-23 00:21:21.375056: Epoch time: 22.42 s\n",
            "2025-12-23 00:21:22.778511: \n",
            "2025-12-23 00:21:22.778787: Epoch 391\n",
            "2025-12-23 00:21:22.778935: Current learning rate: 0.0064\n",
            "2025-12-23 00:21:45.163663: train_loss -0.97\n",
            "2025-12-23 00:21:45.163912: val_loss -0.9848\n",
            "2025-12-23 00:21:45.164018: Pseudo dice [np.float32(0.9893)]\n",
            "2025-12-23 00:21:45.164106: Epoch time: 22.39 s\n",
            "2025-12-23 00:21:47.247671: \n",
            "2025-12-23 00:21:47.247960: Epoch 392\n",
            "2025-12-23 00:21:47.248138: Current learning rate: 0.00639\n",
            "2025-12-23 00:22:09.747111: train_loss -0.971\n",
            "2025-12-23 00:22:09.747587: val_loss -0.9828\n",
            "2025-12-23 00:22:09.747748: Pseudo dice [np.float32(0.9859)]\n",
            "2025-12-23 00:22:09.747881: Epoch time: 22.5 s\n",
            "2025-12-23 00:22:11.136014: \n",
            "2025-12-23 00:22:11.136384: Epoch 393\n",
            "2025-12-23 00:22:11.136550: Current learning rate: 0.00638\n",
            "2025-12-23 00:22:33.592145: train_loss -0.9706\n",
            "2025-12-23 00:22:33.592576: val_loss -0.9839\n",
            "2025-12-23 00:22:33.592712: Pseudo dice [np.float32(0.9877)]\n",
            "2025-12-23 00:22:33.592836: Epoch time: 22.46 s\n",
            "2025-12-23 00:22:34.963829: \n",
            "2025-12-23 00:22:34.964205: Epoch 394\n",
            "2025-12-23 00:22:34.964374: Current learning rate: 0.00637\n",
            "2025-12-23 00:22:57.463089: train_loss -0.9691\n",
            "2025-12-23 00:22:57.463336: val_loss -0.9849\n",
            "2025-12-23 00:22:57.463453: Pseudo dice [np.float32(0.9886)]\n",
            "2025-12-23 00:22:57.463574: Epoch time: 22.5 s\n",
            "2025-12-23 00:22:58.863959: \n",
            "2025-12-23 00:22:58.864181: Epoch 395\n",
            "2025-12-23 00:22:58.864345: Current learning rate: 0.00636\n",
            "2025-12-23 00:23:21.359275: train_loss -0.9709\n",
            "2025-12-23 00:23:21.359744: val_loss -0.9832\n",
            "2025-12-23 00:23:21.359930: Pseudo dice [np.float32(0.9874)]\n",
            "2025-12-23 00:23:21.360124: Epoch time: 22.5 s\n",
            "2025-12-23 00:23:22.750580: \n",
            "2025-12-23 00:23:22.750959: Epoch 396\n",
            "2025-12-23 00:23:22.751113: Current learning rate: 0.00635\n",
            "2025-12-23 00:23:45.194888: train_loss -0.9702\n",
            "2025-12-23 00:23:45.195184: val_loss -0.9836\n",
            "2025-12-23 00:23:45.195304: Pseudo dice [np.float32(0.9878)]\n",
            "2025-12-23 00:23:45.195396: Epoch time: 22.45 s\n",
            "2025-12-23 00:23:46.609417: \n",
            "2025-12-23 00:23:46.609725: Epoch 397\n",
            "2025-12-23 00:23:46.609910: Current learning rate: 0.00634\n",
            "2025-12-23 00:24:09.108587: train_loss -0.9707\n",
            "2025-12-23 00:24:09.108796: val_loss -0.9847\n",
            "2025-12-23 00:24:09.108888: Pseudo dice [np.float32(0.9883)]\n",
            "2025-12-23 00:24:09.108978: Epoch time: 22.5 s\n",
            "2025-12-23 00:24:10.507422: \n",
            "2025-12-23 00:24:10.507795: Epoch 398\n",
            "2025-12-23 00:24:10.507933: Current learning rate: 0.00633\n",
            "2025-12-23 00:24:32.901801: train_loss -0.9696\n",
            "2025-12-23 00:24:32.902064: val_loss -0.9836\n",
            "2025-12-23 00:24:32.902183: Pseudo dice [np.float32(0.9875)]\n",
            "2025-12-23 00:24:32.902400: Epoch time: 22.4 s\n",
            "2025-12-23 00:24:34.299610: \n",
            "2025-12-23 00:24:34.299966: Epoch 399\n",
            "2025-12-23 00:24:34.300117: Current learning rate: 0.00632\n",
            "2025-12-23 00:24:56.738507: train_loss -0.9693\n",
            "2025-12-23 00:24:56.738720: val_loss -0.983\n",
            "2025-12-23 00:24:56.738894: Pseudo dice [np.float32(0.9865)]\n",
            "2025-12-23 00:24:56.739093: Epoch time: 22.44 s\n",
            "2025-12-23 00:24:58.642988: \n",
            "2025-12-23 00:24:58.643199: Epoch 400\n",
            "2025-12-23 00:24:58.643362: Current learning rate: 0.00631\n",
            "2025-12-23 00:25:21.095778: train_loss -0.9707\n",
            "2025-12-23 00:25:21.096020: val_loss -0.984\n",
            "2025-12-23 00:25:21.096197: Pseudo dice [np.float32(0.9869)]\n",
            "2025-12-23 00:25:21.096402: Epoch time: 22.45 s\n",
            "2025-12-23 00:25:22.525468: \n",
            "2025-12-23 00:25:22.525771: Epoch 401\n",
            "2025-12-23 00:25:22.525901: Current learning rate: 0.0063\n",
            "2025-12-23 00:25:44.938782: train_loss -0.9708\n",
            "2025-12-23 00:25:44.939015: val_loss -0.9833\n",
            "2025-12-23 00:25:44.939111: Pseudo dice [np.float32(0.9886)]\n",
            "2025-12-23 00:25:44.939209: Epoch time: 22.41 s\n",
            "2025-12-23 00:25:46.345078: \n",
            "2025-12-23 00:25:46.345305: Epoch 402\n",
            "2025-12-23 00:25:46.345447: Current learning rate: 0.0063\n",
            "2025-12-23 00:26:08.799018: train_loss -0.9713\n",
            "2025-12-23 00:26:08.799252: val_loss -0.9837\n",
            "2025-12-23 00:26:08.799364: Pseudo dice [np.float32(0.9871)]\n",
            "2025-12-23 00:26:08.799457: Epoch time: 22.46 s\n",
            "2025-12-23 00:26:10.190771: \n",
            "2025-12-23 00:26:10.191097: Epoch 403\n",
            "2025-12-23 00:26:10.191239: Current learning rate: 0.00629\n",
            "2025-12-23 00:26:32.673794: train_loss -0.9704\n",
            "2025-12-23 00:26:32.674003: val_loss -0.9832\n",
            "2025-12-23 00:26:32.674156: Pseudo dice [np.float32(0.9869)]\n",
            "2025-12-23 00:26:32.674276: Epoch time: 22.48 s\n",
            "2025-12-23 00:26:34.038287: \n",
            "2025-12-23 00:26:34.038450: Epoch 404\n",
            "2025-12-23 00:26:34.038612: Current learning rate: 0.00628\n",
            "2025-12-23 00:26:56.494115: train_loss -0.9727\n",
            "2025-12-23 00:26:56.494378: val_loss -0.9846\n",
            "2025-12-23 00:26:56.494468: Pseudo dice [np.float32(0.9881)]\n",
            "2025-12-23 00:26:56.494559: Epoch time: 22.46 s\n",
            "2025-12-23 00:26:57.896290: \n",
            "2025-12-23 00:26:57.896463: Epoch 405\n",
            "2025-12-23 00:26:57.896640: Current learning rate: 0.00627\n",
            "2025-12-23 00:27:20.343261: train_loss -0.9721\n",
            "2025-12-23 00:27:20.343529: val_loss -0.9855\n",
            "2025-12-23 00:27:20.343639: Pseudo dice [np.float32(0.9891)]\n",
            "2025-12-23 00:27:20.343732: Epoch time: 22.45 s\n",
            "2025-12-23 00:27:21.744384: \n",
            "2025-12-23 00:27:21.744634: Epoch 406\n",
            "2025-12-23 00:27:21.744763: Current learning rate: 0.00626\n",
            "2025-12-23 00:27:44.177601: train_loss -0.9713\n",
            "2025-12-23 00:27:44.177798: val_loss -0.9842\n",
            "2025-12-23 00:27:44.177884: Pseudo dice [np.float32(0.988)]\n",
            "2025-12-23 00:27:44.177981: Epoch time: 22.43 s\n",
            "2025-12-23 00:27:44.178057: Yayy! New best EMA pseudo Dice: 0.987500011920929\n",
            "2025-12-23 00:27:46.017341: \n",
            "2025-12-23 00:27:46.017523: Epoch 407\n",
            "2025-12-23 00:27:46.017652: Current learning rate: 0.00625\n",
            "2025-12-23 00:28:08.460106: train_loss -0.9713\n",
            "2025-12-23 00:28:08.460319: val_loss -0.9836\n",
            "2025-12-23 00:28:08.460435: Pseudo dice [np.float32(0.9876)]\n",
            "2025-12-23 00:28:08.460563: Epoch time: 22.44 s\n",
            "2025-12-23 00:28:08.460768: Yayy! New best EMA pseudo Dice: 0.987500011920929\n",
            "2025-12-23 00:28:10.974709: \n",
            "2025-12-23 00:28:10.975011: Epoch 408\n",
            "2025-12-23 00:28:10.975171: Current learning rate: 0.00624\n",
            "2025-12-23 00:28:33.471909: train_loss -0.9709\n",
            "2025-12-23 00:28:33.472150: val_loss -0.9838\n",
            "2025-12-23 00:28:33.472335: Pseudo dice [np.float32(0.9885)]\n",
            "2025-12-23 00:28:33.472442: Epoch time: 22.5 s\n",
            "2025-12-23 00:28:33.472564: Yayy! New best EMA pseudo Dice: 0.9876000285148621\n",
            "2025-12-23 00:28:35.299679: \n",
            "2025-12-23 00:28:35.299984: Epoch 409\n",
            "2025-12-23 00:28:35.300137: Current learning rate: 0.00623\n",
            "2025-12-23 00:28:57.799286: train_loss -0.9723\n",
            "2025-12-23 00:28:57.799534: val_loss -0.9841\n",
            "2025-12-23 00:28:57.799625: Pseudo dice [np.float32(0.9892)]\n",
            "2025-12-23 00:28:57.799715: Epoch time: 22.5 s\n",
            "2025-12-23 00:28:57.799790: Yayy! New best EMA pseudo Dice: 0.9878000020980835\n",
            "2025-12-23 00:28:59.674432: \n",
            "2025-12-23 00:28:59.674755: Epoch 410\n",
            "2025-12-23 00:28:59.674960: Current learning rate: 0.00622\n",
            "2025-12-23 00:29:22.143380: train_loss -0.9709\n",
            "2025-12-23 00:29:22.143665: val_loss -0.9847\n",
            "2025-12-23 00:29:22.143759: Pseudo dice [np.float32(0.9887)]\n",
            "2025-12-23 00:29:22.143847: Epoch time: 22.47 s\n",
            "2025-12-23 00:29:22.143921: Yayy! New best EMA pseudo Dice: 0.9879000186920166\n",
            "2025-12-23 00:29:23.957611: \n",
            "2025-12-23 00:29:23.957874: Epoch 411\n",
            "2025-12-23 00:29:23.958016: Current learning rate: 0.00621\n",
            "2025-12-23 00:29:46.403082: train_loss -0.9703\n",
            "2025-12-23 00:29:46.403313: val_loss -0.984\n",
            "2025-12-23 00:29:46.403404: Pseudo dice [np.float32(0.9876)]\n",
            "2025-12-23 00:29:46.403566: Epoch time: 22.45 s\n",
            "2025-12-23 00:29:47.744571: \n",
            "2025-12-23 00:29:47.744931: Epoch 412\n",
            "2025-12-23 00:29:47.745064: Current learning rate: 0.0062\n",
            "2025-12-23 00:30:10.222672: train_loss -0.9711\n",
            "2025-12-23 00:30:10.222877: val_loss -0.9841\n",
            "2025-12-23 00:30:10.222967: Pseudo dice [np.float32(0.9881)]\n",
            "2025-12-23 00:30:10.223064: Epoch time: 22.48 s\n",
            "2025-12-23 00:30:10.223282: Yayy! New best EMA pseudo Dice: 0.9879000186920166\n",
            "2025-12-23 00:30:12.102722: \n",
            "2025-12-23 00:30:12.103117: Epoch 413\n",
            "2025-12-23 00:30:12.103290: Current learning rate: 0.00619\n",
            "2025-12-23 00:30:34.591440: train_loss -0.9708\n",
            "2025-12-23 00:30:34.591754: val_loss -0.9825\n",
            "2025-12-23 00:30:34.591853: Pseudo dice [np.float32(0.9874)]\n",
            "2025-12-23 00:30:34.591946: Epoch time: 22.49 s\n",
            "2025-12-23 00:30:35.965540: \n",
            "2025-12-23 00:30:35.965878: Epoch 414\n",
            "2025-12-23 00:30:35.966030: Current learning rate: 0.00618\n",
            "2025-12-23 00:30:58.444063: train_loss -0.9713\n",
            "2025-12-23 00:30:58.444309: val_loss -0.9837\n",
            "2025-12-23 00:30:58.444409: Pseudo dice [np.float32(0.9879)]\n",
            "2025-12-23 00:30:58.444504: Epoch time: 22.48 s\n",
            "2025-12-23 00:30:59.839649: \n",
            "2025-12-23 00:30:59.839860: Epoch 415\n",
            "2025-12-23 00:30:59.840021: Current learning rate: 0.00617\n",
            "2025-12-23 00:31:22.261062: train_loss -0.9715\n",
            "2025-12-23 00:31:22.261282: val_loss -0.9862\n",
            "2025-12-23 00:31:22.261388: Pseudo dice [np.float32(0.989)]\n",
            "2025-12-23 00:31:22.261480: Epoch time: 22.42 s\n",
            "2025-12-23 00:31:22.261560: Yayy! New best EMA pseudo Dice: 0.9879000186920166\n",
            "2025-12-23 00:31:24.127652: \n",
            "2025-12-23 00:31:24.127958: Epoch 416\n",
            "2025-12-23 00:31:24.128104: Current learning rate: 0.00616\n",
            "2025-12-23 00:31:46.567699: train_loss -0.9718\n",
            "2025-12-23 00:31:46.568081: val_loss -0.9844\n",
            "2025-12-23 00:31:46.568175: Pseudo dice [np.float32(0.9878)]\n",
            "2025-12-23 00:31:46.568332: Epoch time: 22.44 s\n",
            "2025-12-23 00:31:47.916648: \n",
            "2025-12-23 00:31:47.916832: Epoch 417\n",
            "2025-12-23 00:31:47.916973: Current learning rate: 0.00615\n",
            "2025-12-23 00:32:10.324033: train_loss -0.9709\n",
            "2025-12-23 00:32:10.324359: val_loss -0.9839\n",
            "2025-12-23 00:32:10.324458: Pseudo dice [np.float32(0.9883)]\n",
            "2025-12-23 00:32:10.324553: Epoch time: 22.41 s\n",
            "2025-12-23 00:32:10.324636: Yayy! New best EMA pseudo Dice: 0.9879999756813049\n",
            "2025-12-23 00:32:12.225912: \n",
            "2025-12-23 00:32:12.226419: Epoch 418\n",
            "2025-12-23 00:32:12.226579: Current learning rate: 0.00614\n",
            "2025-12-23 00:32:34.681282: train_loss -0.9703\n",
            "2025-12-23 00:32:34.681718: val_loss -0.984\n",
            "2025-12-23 00:32:34.681901: Pseudo dice [np.float32(0.9885)]\n",
            "2025-12-23 00:32:34.682049: Epoch time: 22.46 s\n",
            "2025-12-23 00:32:34.682163: Yayy! New best EMA pseudo Dice: 0.9879999756813049\n",
            "2025-12-23 00:32:36.540425: \n",
            "2025-12-23 00:32:36.540749: Epoch 419\n",
            "2025-12-23 00:32:36.540879: Current learning rate: 0.00613\n",
            "2025-12-23 00:32:58.969291: train_loss -0.9714\n",
            "2025-12-23 00:32:58.969584: val_loss -0.9838\n",
            "2025-12-23 00:32:58.969679: Pseudo dice [np.float32(0.9886)]\n",
            "2025-12-23 00:32:58.969770: Epoch time: 22.43 s\n",
            "2025-12-23 00:32:58.969857: Yayy! New best EMA pseudo Dice: 0.988099992275238\n",
            "2025-12-23 00:33:00.793128: \n",
            "2025-12-23 00:33:00.793293: Epoch 420\n",
            "2025-12-23 00:33:00.793418: Current learning rate: 0.00612\n",
            "2025-12-23 00:33:23.260412: train_loss -0.9709\n",
            "2025-12-23 00:33:23.260624: val_loss -0.9855\n",
            "2025-12-23 00:33:23.260734: Pseudo dice [np.float32(0.9889)]\n",
            "2025-12-23 00:33:23.260830: Epoch time: 22.47 s\n",
            "2025-12-23 00:33:23.260998: Yayy! New best EMA pseudo Dice: 0.9882000088691711\n",
            "2025-12-23 00:33:25.102139: \n",
            "2025-12-23 00:33:25.102330: Epoch 421\n",
            "2025-12-23 00:33:25.102592: Current learning rate: 0.00612\n",
            "2025-12-23 00:33:47.556766: train_loss -0.972\n",
            "2025-12-23 00:33:47.557023: val_loss -0.9834\n",
            "2025-12-23 00:33:47.557142: Pseudo dice [np.float32(0.9876)]\n",
            "2025-12-23 00:33:47.557272: Epoch time: 22.46 s\n",
            "2025-12-23 00:33:48.918385: \n",
            "2025-12-23 00:33:48.918564: Epoch 422\n",
            "2025-12-23 00:33:48.918690: Current learning rate: 0.00611\n",
            "2025-12-23 00:34:11.354303: train_loss -0.9726\n",
            "2025-12-23 00:34:11.354531: val_loss -0.9831\n",
            "2025-12-23 00:34:11.354622: Pseudo dice [np.float32(0.988)]\n",
            "2025-12-23 00:34:11.354721: Epoch time: 22.44 s\n",
            "2025-12-23 00:34:12.731772: \n",
            "2025-12-23 00:34:12.732054: Epoch 423\n",
            "2025-12-23 00:34:12.732199: Current learning rate: 0.0061\n",
            "2025-12-23 00:34:35.191511: train_loss -0.9727\n",
            "2025-12-23 00:34:35.191705: val_loss -0.985\n",
            "2025-12-23 00:34:35.191795: Pseudo dice [np.float32(0.9892)]\n",
            "2025-12-23 00:34:35.191884: Epoch time: 22.46 s\n",
            "2025-12-23 00:34:35.191957: Yayy! New best EMA pseudo Dice: 0.9882000088691711\n",
            "2025-12-23 00:34:37.025399: \n",
            "2025-12-23 00:34:37.025703: Epoch 424\n",
            "2025-12-23 00:34:37.025863: Current learning rate: 0.00609\n",
            "2025-12-23 00:34:59.472127: train_loss -0.9718\n",
            "2025-12-23 00:34:59.472341: val_loss -0.9835\n",
            "2025-12-23 00:34:59.472430: Pseudo dice [np.float32(0.9873)]\n",
            "2025-12-23 00:34:59.472529: Epoch time: 22.45 s\n",
            "2025-12-23 00:35:00.830056: \n",
            "2025-12-23 00:35:00.830368: Epoch 425\n",
            "2025-12-23 00:35:00.830503: Current learning rate: 0.00608\n",
            "2025-12-23 00:35:23.233721: train_loss -0.9723\n",
            "2025-12-23 00:35:23.234099: val_loss -0.9847\n",
            "2025-12-23 00:35:23.234239: Pseudo dice [np.float32(0.9889)]\n",
            "2025-12-23 00:35:23.234363: Epoch time: 22.4 s\n",
            "2025-12-23 00:35:25.212611: \n",
            "2025-12-23 00:35:25.212937: Epoch 426\n",
            "2025-12-23 00:35:25.213108: Current learning rate: 0.00607\n",
            "2025-12-23 00:35:47.656507: train_loss -0.9724\n",
            "2025-12-23 00:35:47.656711: val_loss -0.9847\n",
            "2025-12-23 00:35:47.656831: Pseudo dice [np.float32(0.9892)]\n",
            "2025-12-23 00:35:47.656935: Epoch time: 22.45 s\n",
            "2025-12-23 00:35:47.657012: Yayy! New best EMA pseudo Dice: 0.9883000254631042\n",
            "2025-12-23 00:35:49.454349: \n",
            "2025-12-23 00:35:49.454741: Epoch 427\n",
            "2025-12-23 00:35:49.454892: Current learning rate: 0.00606\n",
            "2025-12-23 00:36:11.906656: train_loss -0.9717\n",
            "2025-12-23 00:36:11.906954: val_loss -0.984\n",
            "2025-12-23 00:36:11.907138: Pseudo dice [np.float32(0.988)]\n",
            "2025-12-23 00:36:11.907271: Epoch time: 22.45 s\n",
            "2025-12-23 00:36:13.236737: \n",
            "2025-12-23 00:36:13.237033: Epoch 428\n",
            "2025-12-23 00:36:13.237204: Current learning rate: 0.00605\n",
            "2025-12-23 00:36:35.681801: train_loss -0.9723\n",
            "2025-12-23 00:36:35.682037: val_loss -0.9843\n",
            "2025-12-23 00:36:35.682126: Pseudo dice [np.float32(0.9881)]\n",
            "2025-12-23 00:36:35.682230: Epoch time: 22.45 s\n",
            "2025-12-23 00:36:37.049269: \n",
            "2025-12-23 00:36:37.049566: Epoch 429\n",
            "2025-12-23 00:36:37.049698: Current learning rate: 0.00604\n",
            "2025-12-23 00:36:59.537200: train_loss -0.9723\n",
            "2025-12-23 00:36:59.537466: val_loss -0.9849\n",
            "2025-12-23 00:36:59.537564: Pseudo dice [np.float32(0.9885)]\n",
            "2025-12-23 00:36:59.537656: Epoch time: 22.49 s\n",
            "2025-12-23 00:37:00.887745: \n",
            "2025-12-23 00:37:00.888078: Epoch 430\n",
            "2025-12-23 00:37:00.888211: Current learning rate: 0.00603\n",
            "2025-12-23 00:37:23.392877: train_loss -0.9714\n",
            "2025-12-23 00:37:23.393144: val_loss -0.9851\n",
            "2025-12-23 00:37:23.393284: Pseudo dice [np.float32(0.9882)]\n",
            "2025-12-23 00:37:23.393402: Epoch time: 22.51 s\n",
            "2025-12-23 00:37:24.741674: \n",
            "2025-12-23 00:37:24.741951: Epoch 431\n",
            "2025-12-23 00:37:24.742080: Current learning rate: 0.00602\n",
            "2025-12-23 00:37:47.215209: train_loss -0.9721\n",
            "2025-12-23 00:37:47.215499: val_loss -0.9849\n",
            "2025-12-23 00:37:47.215608: Pseudo dice [np.float32(0.9885)]\n",
            "2025-12-23 00:37:47.215719: Epoch time: 22.47 s\n",
            "2025-12-23 00:37:48.583924: \n",
            "2025-12-23 00:37:48.584308: Epoch 432\n",
            "2025-12-23 00:37:48.584483: Current learning rate: 0.00601\n",
            "2025-12-23 00:38:11.062148: train_loss -0.9713\n",
            "2025-12-23 00:38:11.062409: val_loss -0.9854\n",
            "2025-12-23 00:38:11.062501: Pseudo dice [np.float32(0.9891)]\n",
            "2025-12-23 00:38:11.062607: Epoch time: 22.48 s\n",
            "2025-12-23 00:38:11.062704: Yayy! New best EMA pseudo Dice: 0.9883999824523926\n",
            "2025-12-23 00:38:12.876392: \n",
            "2025-12-23 00:38:12.876672: Epoch 433\n",
            "2025-12-23 00:38:12.876804: Current learning rate: 0.006\n",
            "2025-12-23 00:38:35.414089: train_loss -0.9727\n",
            "2025-12-23 00:38:35.414344: val_loss -0.9859\n",
            "2025-12-23 00:38:35.414516: Pseudo dice [np.float32(0.9895)]\n",
            "2025-12-23 00:38:35.414620: Epoch time: 22.54 s\n",
            "2025-12-23 00:38:35.414698: Yayy! New best EMA pseudo Dice: 0.9884999990463257\n",
            "2025-12-23 00:38:37.296611: \n",
            "2025-12-23 00:38:37.296888: Epoch 434\n",
            "2025-12-23 00:38:37.297022: Current learning rate: 0.00599\n",
            "2025-12-23 00:38:59.749456: train_loss -0.9725\n",
            "2025-12-23 00:38:59.749735: val_loss -0.9836\n",
            "2025-12-23 00:38:59.749827: Pseudo dice [np.float32(0.9874)]\n",
            "2025-12-23 00:38:59.749918: Epoch time: 22.45 s\n",
            "2025-12-23 00:39:01.073185: \n",
            "2025-12-23 00:39:01.073574: Epoch 435\n",
            "2025-12-23 00:39:01.073708: Current learning rate: 0.00598\n",
            "2025-12-23 00:39:23.519522: train_loss -0.9706\n",
            "2025-12-23 00:39:23.519792: val_loss -0.9842\n",
            "2025-12-23 00:39:23.519886: Pseudo dice [np.float32(0.9877)]\n",
            "2025-12-23 00:39:23.519976: Epoch time: 22.45 s\n",
            "2025-12-23 00:39:24.903136: \n",
            "2025-12-23 00:39:24.903323: Epoch 436\n",
            "2025-12-23 00:39:24.903465: Current learning rate: 0.00597\n",
            "2025-12-23 00:39:47.340885: train_loss -0.9696\n",
            "2025-12-23 00:39:47.341113: val_loss -0.9836\n",
            "2025-12-23 00:39:47.341209: Pseudo dice [np.float32(0.9873)]\n",
            "2025-12-23 00:39:47.341319: Epoch time: 22.44 s\n",
            "2025-12-23 00:39:48.688614: \n",
            "2025-12-23 00:39:48.688854: Epoch 437\n",
            "2025-12-23 00:39:48.688981: Current learning rate: 0.00596\n",
            "2025-12-23 00:40:11.102088: train_loss -0.9711\n",
            "2025-12-23 00:40:11.102520: val_loss -0.9825\n",
            "2025-12-23 00:40:11.102629: Pseudo dice [np.float32(0.9868)]\n",
            "2025-12-23 00:40:11.102737: Epoch time: 22.41 s\n",
            "2025-12-23 00:40:12.463611: \n",
            "2025-12-23 00:40:12.464004: Epoch 438\n",
            "2025-12-23 00:40:12.464152: Current learning rate: 0.00595\n",
            "2025-12-23 00:40:34.890501: train_loss -0.9686\n",
            "2025-12-23 00:40:34.890693: val_loss -0.9839\n",
            "2025-12-23 00:40:34.890785: Pseudo dice [np.float32(0.9882)]\n",
            "2025-12-23 00:40:34.890874: Epoch time: 22.43 s\n",
            "2025-12-23 00:40:36.238155: \n",
            "2025-12-23 00:40:36.238423: Epoch 439\n",
            "2025-12-23 00:40:36.238560: Current learning rate: 0.00594\n",
            "2025-12-23 00:40:58.649834: train_loss -0.9681\n",
            "2025-12-23 00:40:58.650036: val_loss -0.9823\n",
            "2025-12-23 00:40:58.650126: Pseudo dice [np.float32(0.9874)]\n",
            "2025-12-23 00:40:58.650234: Epoch time: 22.41 s\n",
            "2025-12-23 00:40:59.988324: \n",
            "2025-12-23 00:40:59.988508: Epoch 440\n",
            "2025-12-23 00:40:59.988642: Current learning rate: 0.00593\n",
            "2025-12-23 00:41:22.405194: train_loss -0.9682\n",
            "2025-12-23 00:41:22.405458: val_loss -0.9801\n",
            "2025-12-23 00:41:22.405557: Pseudo dice [np.float32(0.9854)]\n",
            "2025-12-23 00:41:22.405654: Epoch time: 22.42 s\n",
            "2025-12-23 00:41:23.757651: \n",
            "2025-12-23 00:41:23.758041: Epoch 441\n",
            "2025-12-23 00:41:23.758178: Current learning rate: 0.00592\n",
            "2025-12-23 00:41:46.191004: train_loss -0.9614\n",
            "2025-12-23 00:41:46.191198: val_loss -0.9753\n",
            "2025-12-23 00:41:46.191338: Pseudo dice [np.float32(0.9801)]\n",
            "2025-12-23 00:41:46.191435: Epoch time: 22.43 s\n",
            "2025-12-23 00:41:47.577268: \n",
            "2025-12-23 00:41:47.577523: Epoch 442\n",
            "2025-12-23 00:41:47.577703: Current learning rate: 0.00592\n",
            "2025-12-23 00:42:10.010644: train_loss -0.9616\n",
            "2025-12-23 00:42:10.011058: val_loss -0.9809\n",
            "2025-12-23 00:42:10.011247: Pseudo dice [np.float32(0.986)]\n",
            "2025-12-23 00:42:10.011357: Epoch time: 22.43 s\n",
            "2025-12-23 00:42:11.334223: \n",
            "2025-12-23 00:42:11.334527: Epoch 443\n",
            "2025-12-23 00:42:11.334657: Current learning rate: 0.00591\n",
            "2025-12-23 00:42:33.778966: train_loss -0.967\n",
            "2025-12-23 00:42:33.779161: val_loss -0.9825\n",
            "2025-12-23 00:42:33.779283: Pseudo dice [np.float32(0.9879)]\n",
            "2025-12-23 00:42:33.779391: Epoch time: 22.45 s\n",
            "2025-12-23 00:42:35.791947: \n",
            "2025-12-23 00:42:35.792262: Epoch 444\n",
            "2025-12-23 00:42:35.792408: Current learning rate: 0.0059\n",
            "2025-12-23 00:42:58.276087: train_loss -0.9701\n",
            "2025-12-23 00:42:58.276342: val_loss -0.9841\n",
            "2025-12-23 00:42:58.276443: Pseudo dice [np.float32(0.9887)]\n",
            "2025-12-23 00:42:58.276544: Epoch time: 22.49 s\n",
            "2025-12-23 00:42:59.601773: \n",
            "2025-12-23 00:42:59.602116: Epoch 445\n",
            "2025-12-23 00:42:59.602274: Current learning rate: 0.00589\n",
            "2025-12-23 00:43:22.016160: train_loss -0.9691\n",
            "2025-12-23 00:43:22.016583: val_loss -0.9805\n",
            "2025-12-23 00:43:22.016688: Pseudo dice [np.float32(0.9882)]\n",
            "2025-12-23 00:43:22.016792: Epoch time: 22.42 s\n",
            "2025-12-23 00:43:23.337613: \n",
            "2025-12-23 00:43:23.337941: Epoch 446\n",
            "2025-12-23 00:43:23.338079: Current learning rate: 0.00588\n",
            "2025-12-23 00:43:45.789146: train_loss -0.9673\n",
            "2025-12-23 00:43:45.789383: val_loss -0.9786\n",
            "2025-12-23 00:43:45.789482: Pseudo dice [np.float32(0.9837)]\n",
            "2025-12-23 00:43:45.789582: Epoch time: 22.45 s\n",
            "2025-12-23 00:43:47.105056: \n",
            "2025-12-23 00:43:47.105325: Epoch 447\n",
            "2025-12-23 00:43:47.105547: Current learning rate: 0.00587\n",
            "2025-12-23 00:44:09.525586: train_loss -0.9645\n",
            "2025-12-23 00:44:09.525804: val_loss -0.9797\n",
            "2025-12-23 00:44:09.525924: Pseudo dice [np.float32(0.9843)]\n",
            "2025-12-23 00:44:09.526019: Epoch time: 22.42 s\n",
            "2025-12-23 00:44:10.907001: \n",
            "2025-12-23 00:44:10.907266: Epoch 448\n",
            "2025-12-23 00:44:10.907402: Current learning rate: 0.00586\n",
            "2025-12-23 00:44:33.348838: train_loss -0.9669\n",
            "2025-12-23 00:44:33.349172: val_loss -0.9823\n",
            "2025-12-23 00:44:33.349330: Pseudo dice [np.float32(0.9875)]\n",
            "2025-12-23 00:44:33.349446: Epoch time: 22.44 s\n",
            "2025-12-23 00:44:34.726853: \n",
            "2025-12-23 00:44:34.727201: Epoch 449\n",
            "2025-12-23 00:44:34.727387: Current learning rate: 0.00585\n",
            "2025-12-23 00:44:57.187966: train_loss -0.9689\n",
            "2025-12-23 00:44:57.188171: val_loss -0.983\n",
            "2025-12-23 00:44:57.188302: Pseudo dice [np.float32(0.9875)]\n",
            "2025-12-23 00:44:57.188459: Epoch time: 22.46 s\n",
            "2025-12-23 00:44:59.064762: \n",
            "2025-12-23 00:44:59.065164: Epoch 450\n",
            "2025-12-23 00:44:59.065356: Current learning rate: 0.00584\n",
            "2025-12-23 00:45:21.557706: train_loss -0.9691\n",
            "2025-12-23 00:45:21.557954: val_loss -0.9839\n",
            "2025-12-23 00:45:21.558080: Pseudo dice [np.float32(0.988)]\n",
            "2025-12-23 00:45:21.558256: Epoch time: 22.49 s\n",
            "2025-12-23 00:45:22.936732: \n",
            "2025-12-23 00:45:22.937044: Epoch 451\n",
            "2025-12-23 00:45:22.937250: Current learning rate: 0.00583\n",
            "2025-12-23 00:45:45.374552: train_loss -0.9656\n",
            "2025-12-23 00:45:45.374833: val_loss -0.9748\n",
            "2025-12-23 00:45:45.374974: Pseudo dice [np.float32(0.9789)]\n",
            "2025-12-23 00:45:45.375116: Epoch time: 22.44 s\n",
            "2025-12-23 00:45:46.720703: \n",
            "2025-12-23 00:45:46.721064: Epoch 452\n",
            "2025-12-23 00:45:46.721212: Current learning rate: 0.00582\n",
            "2025-12-23 00:46:09.127722: train_loss -0.9511\n",
            "2025-12-23 00:46:09.128010: val_loss -0.9766\n",
            "2025-12-23 00:46:09.128183: Pseudo dice [np.float32(0.9819)]\n",
            "2025-12-23 00:46:09.128335: Epoch time: 22.41 s\n",
            "2025-12-23 00:46:10.506191: \n",
            "2025-12-23 00:46:10.506456: Epoch 453\n",
            "2025-12-23 00:46:10.506603: Current learning rate: 0.00581\n",
            "2025-12-23 00:46:32.937566: train_loss -0.9608\n",
            "2025-12-23 00:46:32.937923: val_loss -0.9804\n",
            "2025-12-23 00:46:32.938022: Pseudo dice [np.float32(0.9858)]\n",
            "2025-12-23 00:46:32.938114: Epoch time: 22.43 s\n",
            "2025-12-23 00:46:34.268549: \n",
            "2025-12-23 00:46:34.268746: Epoch 454\n",
            "2025-12-23 00:46:34.268948: Current learning rate: 0.0058\n",
            "2025-12-23 00:46:56.677632: train_loss -0.965\n",
            "2025-12-23 00:46:56.677872: val_loss -0.9818\n",
            "2025-12-23 00:46:56.677982: Pseudo dice [np.float32(0.9862)]\n",
            "2025-12-23 00:46:56.678103: Epoch time: 22.41 s\n",
            "2025-12-23 00:46:58.048293: \n",
            "2025-12-23 00:46:58.048606: Epoch 455\n",
            "2025-12-23 00:46:58.048749: Current learning rate: 0.00579\n",
            "2025-12-23 00:47:20.475151: train_loss -0.9677\n",
            "2025-12-23 00:47:20.475401: val_loss -0.9843\n",
            "2025-12-23 00:47:20.475525: Pseudo dice [np.float32(0.9883)]\n",
            "2025-12-23 00:47:20.475621: Epoch time: 22.43 s\n",
            "2025-12-23 00:47:21.826286: \n",
            "2025-12-23 00:47:21.826578: Epoch 456\n",
            "2025-12-23 00:47:21.826715: Current learning rate: 0.00578\n",
            "2025-12-23 00:47:44.253152: train_loss -0.97\n",
            "2025-12-23 00:47:44.253414: val_loss -0.9851\n",
            "2025-12-23 00:47:44.253585: Pseudo dice [np.float32(0.9891)]\n",
            "2025-12-23 00:47:44.253706: Epoch time: 22.43 s\n",
            "2025-12-23 00:47:45.575834: \n",
            "2025-12-23 00:47:45.576125: Epoch 457\n",
            "2025-12-23 00:47:45.576269: Current learning rate: 0.00577\n",
            "2025-12-23 00:48:08.005421: train_loss -0.9698\n",
            "2025-12-23 00:48:08.005633: val_loss -0.9834\n",
            "2025-12-23 00:48:08.005721: Pseudo dice [np.float32(0.9878)]\n",
            "2025-12-23 00:48:08.005822: Epoch time: 22.43 s\n",
            "2025-12-23 00:48:09.345692: \n",
            "2025-12-23 00:48:09.345953: Epoch 458\n",
            "2025-12-23 00:48:09.346091: Current learning rate: 0.00576\n",
            "2025-12-23 00:48:31.762733: train_loss -0.9684\n",
            "2025-12-23 00:48:31.762996: val_loss -0.9832\n",
            "2025-12-23 00:48:31.763098: Pseudo dice [np.float32(0.9877)]\n",
            "2025-12-23 00:48:31.763190: Epoch time: 22.42 s\n",
            "2025-12-23 00:48:33.124072: \n",
            "2025-12-23 00:48:33.124290: Epoch 459\n",
            "2025-12-23 00:48:33.124427: Current learning rate: 0.00575\n",
            "2025-12-23 00:48:55.538063: train_loss -0.9688\n",
            "2025-12-23 00:48:55.538287: val_loss -0.9827\n",
            "2025-12-23 00:48:55.538409: Pseudo dice [np.float32(0.9872)]\n",
            "2025-12-23 00:48:55.538500: Epoch time: 22.42 s\n",
            "2025-12-23 00:48:56.872257: \n",
            "2025-12-23 00:48:56.872429: Epoch 460\n",
            "2025-12-23 00:48:56.872605: Current learning rate: 0.00574\n",
            "2025-12-23 00:49:19.297535: train_loss -0.9685\n",
            "2025-12-23 00:49:19.297837: val_loss -0.9843\n",
            "2025-12-23 00:49:19.297934: Pseudo dice [np.float32(0.9879)]\n",
            "2025-12-23 00:49:19.298026: Epoch time: 22.43 s\n",
            "2025-12-23 00:49:20.639490: \n",
            "2025-12-23 00:49:20.639818: Epoch 461\n",
            "2025-12-23 00:49:20.639985: Current learning rate: 0.00573\n",
            "2025-12-23 00:49:43.033901: train_loss -0.9687\n",
            "2025-12-23 00:49:43.034301: val_loss -0.9826\n",
            "2025-12-23 00:49:43.034417: Pseudo dice [np.float32(0.9868)]\n",
            "2025-12-23 00:49:43.034528: Epoch time: 22.4 s\n",
            "2025-12-23 00:49:44.355257: \n",
            "2025-12-23 00:49:44.355538: Epoch 462\n",
            "2025-12-23 00:49:44.355674: Current learning rate: 0.00572\n",
            "2025-12-23 00:50:06.758600: train_loss -0.9688\n",
            "2025-12-23 00:50:06.758935: val_loss -0.9839\n",
            "2025-12-23 00:50:06.759057: Pseudo dice [np.float32(0.9883)]\n",
            "2025-12-23 00:50:06.759179: Epoch time: 22.4 s\n",
            "2025-12-23 00:50:08.796430: \n",
            "2025-12-23 00:50:08.796792: Epoch 463\n",
            "2025-12-23 00:50:08.796993: Current learning rate: 0.00571\n",
            "2025-12-23 00:50:31.265636: train_loss -0.9633\n",
            "2025-12-23 00:50:31.265852: val_loss -0.9817\n",
            "2025-12-23 00:50:31.265943: Pseudo dice [np.float32(0.9861)]\n",
            "2025-12-23 00:50:31.266031: Epoch time: 22.47 s\n",
            "2025-12-23 00:50:32.580255: \n",
            "2025-12-23 00:50:32.580539: Epoch 464\n",
            "2025-12-23 00:50:32.580674: Current learning rate: 0.0057\n",
            "2025-12-23 00:50:55.088585: train_loss -0.9665\n",
            "2025-12-23 00:50:55.088786: val_loss -0.9828\n",
            "2025-12-23 00:50:55.088892: Pseudo dice [np.float32(0.9883)]\n",
            "2025-12-23 00:50:55.088980: Epoch time: 22.51 s\n",
            "2025-12-23 00:50:56.408244: \n",
            "2025-12-23 00:50:56.408602: Epoch 465\n",
            "2025-12-23 00:50:56.408744: Current learning rate: 0.0057\n",
            "2025-12-23 00:51:18.886927: train_loss -0.969\n",
            "2025-12-23 00:51:18.887183: val_loss -0.9831\n",
            "2025-12-23 00:51:18.887298: Pseudo dice [np.float32(0.9878)]\n",
            "2025-12-23 00:51:18.887391: Epoch time: 22.48 s\n",
            "2025-12-23 00:51:20.215015: \n",
            "2025-12-23 00:51:20.215388: Epoch 466\n",
            "2025-12-23 00:51:20.215557: Current learning rate: 0.00569\n",
            "2025-12-23 00:51:42.647864: train_loss -0.969\n",
            "2025-12-23 00:51:42.648066: val_loss -0.9817\n",
            "2025-12-23 00:51:42.648152: Pseudo dice [np.float32(0.9868)]\n",
            "2025-12-23 00:51:42.648273: Epoch time: 22.43 s\n",
            "2025-12-23 00:51:43.988051: \n",
            "2025-12-23 00:51:43.988311: Epoch 467\n",
            "2025-12-23 00:51:43.988465: Current learning rate: 0.00568\n",
            "2025-12-23 00:52:06.422078: train_loss -0.9706\n",
            "2025-12-23 00:52:06.422347: val_loss -0.9843\n",
            "2025-12-23 00:52:06.422496: Pseudo dice [np.float32(0.9881)]\n",
            "2025-12-23 00:52:06.422672: Epoch time: 22.44 s\n",
            "2025-12-23 00:52:07.785487: \n",
            "2025-12-23 00:52:07.785728: Epoch 468\n",
            "2025-12-23 00:52:07.785860: Current learning rate: 0.00567\n",
            "2025-12-23 00:52:30.350678: train_loss -0.9693\n",
            "2025-12-23 00:52:30.350933: val_loss -0.9837\n",
            "2025-12-23 00:52:30.351051: Pseudo dice [np.float32(0.9883)]\n",
            "2025-12-23 00:52:30.351143: Epoch time: 22.57 s\n",
            "2025-12-23 00:52:31.682276: \n",
            "2025-12-23 00:52:31.682596: Epoch 469\n",
            "2025-12-23 00:52:31.682754: Current learning rate: 0.00566\n",
            "2025-12-23 00:52:54.137137: train_loss -0.9695\n",
            "2025-12-23 00:52:54.137432: val_loss -0.9839\n",
            "2025-12-23 00:52:54.137600: Pseudo dice [np.float32(0.9888)]\n",
            "2025-12-23 00:52:54.137700: Epoch time: 22.46 s\n",
            "2025-12-23 00:52:55.499614: \n",
            "2025-12-23 00:52:55.499876: Epoch 470\n",
            "2025-12-23 00:52:55.500008: Current learning rate: 0.00565\n",
            "2025-12-23 00:53:17.968312: train_loss -0.9712\n",
            "2025-12-23 00:53:17.968537: val_loss -0.9833\n",
            "2025-12-23 00:53:17.968629: Pseudo dice [np.float32(0.9871)]\n",
            "2025-12-23 00:53:17.968721: Epoch time: 22.47 s\n",
            "2025-12-23 00:53:19.311106: \n",
            "2025-12-23 00:53:19.311514: Epoch 471\n",
            "2025-12-23 00:53:19.311678: Current learning rate: 0.00564\n",
            "2025-12-23 00:53:41.781635: train_loss -0.9708\n",
            "2025-12-23 00:53:41.781983: val_loss -0.9849\n",
            "2025-12-23 00:53:41.782082: Pseudo dice [np.float32(0.9886)]\n",
            "2025-12-23 00:53:41.782177: Epoch time: 22.47 s\n",
            "2025-12-23 00:53:43.124813: \n",
            "2025-12-23 00:53:43.125056: Epoch 472\n",
            "2025-12-23 00:53:43.125188: Current learning rate: 0.00563\n",
            "2025-12-23 00:54:05.552437: train_loss -0.9714\n",
            "2025-12-23 00:54:05.552702: val_loss -0.9853\n",
            "2025-12-23 00:54:05.552793: Pseudo dice [np.float32(0.9889)]\n",
            "2025-12-23 00:54:05.552887: Epoch time: 22.43 s\n",
            "2025-12-23 00:54:06.897118: \n",
            "2025-12-23 00:54:06.897299: Epoch 473\n",
            "2025-12-23 00:54:06.897445: Current learning rate: 0.00562\n",
            "2025-12-23 00:54:29.299785: train_loss -0.9716\n",
            "2025-12-23 00:54:29.300028: val_loss -0.9853\n",
            "2025-12-23 00:54:29.300117: Pseudo dice [np.float32(0.9888)]\n",
            "2025-12-23 00:54:29.300205: Epoch time: 22.4 s\n",
            "2025-12-23 00:54:30.645060: \n",
            "2025-12-23 00:54:30.645406: Epoch 474\n",
            "2025-12-23 00:54:30.645575: Current learning rate: 0.00561\n",
            "2025-12-23 00:54:53.083189: train_loss -0.9712\n",
            "2025-12-23 00:54:53.083408: val_loss -0.9829\n",
            "2025-12-23 00:54:53.083515: Pseudo dice [np.float32(0.9857)]\n",
            "2025-12-23 00:54:53.083608: Epoch time: 22.44 s\n",
            "2025-12-23 00:54:54.431812: \n",
            "2025-12-23 00:54:54.432017: Epoch 475\n",
            "2025-12-23 00:54:54.432154: Current learning rate: 0.0056\n",
            "2025-12-23 00:55:16.864159: train_loss -0.9705\n",
            "2025-12-23 00:55:16.864417: val_loss -0.9834\n",
            "2025-12-23 00:55:16.864508: Pseudo dice [np.float32(0.9877)]\n",
            "2025-12-23 00:55:16.864601: Epoch time: 22.43 s\n",
            "2025-12-23 00:55:18.186054: \n",
            "2025-12-23 00:55:18.186392: Epoch 476\n",
            "2025-12-23 00:55:18.186642: Current learning rate: 0.00559\n",
            "2025-12-23 00:55:40.657589: train_loss -0.9725\n",
            "2025-12-23 00:55:40.657808: val_loss -0.9843\n",
            "2025-12-23 00:55:40.657899: Pseudo dice [np.float32(0.9892)]\n",
            "2025-12-23 00:55:40.658033: Epoch time: 22.47 s\n",
            "2025-12-23 00:55:41.994883: \n",
            "2025-12-23 00:55:41.995087: Epoch 477\n",
            "2025-12-23 00:55:41.995269: Current learning rate: 0.00558\n",
            "2025-12-23 00:56:04.434991: train_loss -0.9718\n",
            "2025-12-23 00:56:04.435268: val_loss -0.9845\n",
            "2025-12-23 00:56:04.435366: Pseudo dice [np.float32(0.9882)]\n",
            "2025-12-23 00:56:04.435457: Epoch time: 22.44 s\n",
            "2025-12-23 00:56:05.774757: \n",
            "2025-12-23 00:56:05.774974: Epoch 478\n",
            "2025-12-23 00:56:05.775106: Current learning rate: 0.00557\n",
            "2025-12-23 00:56:28.204405: train_loss -0.9695\n",
            "2025-12-23 00:56:28.204661: val_loss -0.9854\n",
            "2025-12-23 00:56:28.204818: Pseudo dice [np.float32(0.989)]\n",
            "2025-12-23 00:56:28.205032: Epoch time: 22.43 s\n",
            "2025-12-23 00:56:29.559068: \n",
            "2025-12-23 00:56:29.559685: Epoch 479\n",
            "2025-12-23 00:56:29.559841: Current learning rate: 0.00556\n",
            "2025-12-23 00:56:51.959778: train_loss -0.9706\n",
            "2025-12-23 00:56:51.960153: val_loss -0.9838\n",
            "2025-12-23 00:56:51.960336: Pseudo dice [np.float32(0.9878)]\n",
            "2025-12-23 00:56:51.960476: Epoch time: 22.4 s\n",
            "2025-12-23 00:56:53.362152: \n",
            "2025-12-23 00:56:53.362538: Epoch 480\n",
            "2025-12-23 00:56:53.362714: Current learning rate: 0.00555\n",
            "2025-12-23 00:57:15.757725: train_loss -0.9696\n",
            "2025-12-23 00:57:15.757956: val_loss -0.9837\n",
            "2025-12-23 00:57:15.758046: Pseudo dice [np.float32(0.987)]\n",
            "2025-12-23 00:57:15.758135: Epoch time: 22.4 s\n",
            "2025-12-23 00:57:17.131115: \n",
            "2025-12-23 00:57:17.131464: Epoch 481\n",
            "2025-12-23 00:57:17.131615: Current learning rate: 0.00554\n",
            "2025-12-23 00:57:39.500513: train_loss -0.9708\n",
            "2025-12-23 00:57:39.500812: val_loss -0.9842\n",
            "2025-12-23 00:57:39.500934: Pseudo dice [np.float32(0.9885)]\n",
            "2025-12-23 00:57:39.501059: Epoch time: 22.37 s\n",
            "2025-12-23 00:57:41.577875: \n",
            "2025-12-23 00:57:41.578065: Epoch 482\n",
            "2025-12-23 00:57:41.578200: Current learning rate: 0.00553\n",
            "2025-12-23 00:58:04.059952: train_loss -0.9687\n",
            "2025-12-23 00:58:04.060417: val_loss -0.9808\n",
            "2025-12-23 00:58:04.060571: Pseudo dice [np.float32(0.9866)]\n",
            "2025-12-23 00:58:04.060718: Epoch time: 22.48 s\n",
            "2025-12-23 00:58:05.428493: \n",
            "2025-12-23 00:58:05.428812: Epoch 483\n",
            "2025-12-23 00:58:05.428961: Current learning rate: 0.00552\n",
            "2025-12-23 00:58:27.850324: train_loss -0.967\n",
            "2025-12-23 00:58:27.850562: val_loss -0.9831\n",
            "2025-12-23 00:58:27.850677: Pseudo dice [np.float32(0.9872)]\n",
            "2025-12-23 00:58:27.850772: Epoch time: 22.42 s\n",
            "2025-12-23 00:58:29.237363: \n",
            "2025-12-23 00:58:29.237667: Epoch 484\n",
            "2025-12-23 00:58:29.237809: Current learning rate: 0.00551\n",
            "2025-12-23 00:58:51.672262: train_loss -0.9629\n",
            "2025-12-23 00:58:51.672528: val_loss -0.9758\n",
            "2025-12-23 00:58:51.672631: Pseudo dice [np.float32(0.9817)]\n",
            "2025-12-23 00:58:51.672752: Epoch time: 22.44 s\n",
            "2025-12-23 00:58:53.057260: \n",
            "2025-12-23 00:58:53.057599: Epoch 485\n",
            "2025-12-23 00:58:53.057739: Current learning rate: 0.0055\n",
            "2025-12-23 00:59:15.482311: train_loss -0.9588\n",
            "2025-12-23 00:59:15.482568: val_loss -0.9797\n",
            "2025-12-23 00:59:15.482682: Pseudo dice [np.float32(0.9845)]\n",
            "2025-12-23 00:59:15.482833: Epoch time: 22.43 s\n",
            "2025-12-23 00:59:16.849598: \n",
            "2025-12-23 00:59:16.849934: Epoch 486\n",
            "2025-12-23 00:59:16.850085: Current learning rate: 0.00549\n",
            "2025-12-23 00:59:39.306270: train_loss -0.9654\n",
            "2025-12-23 00:59:39.306649: val_loss -0.9815\n",
            "2025-12-23 00:59:39.306750: Pseudo dice [np.float32(0.9857)]\n",
            "2025-12-23 00:59:39.306859: Epoch time: 22.46 s\n",
            "2025-12-23 00:59:40.659879: \n",
            "2025-12-23 00:59:40.660164: Epoch 487\n",
            "2025-12-23 00:59:40.660367: Current learning rate: 0.00548\n",
            "2025-12-23 01:00:03.109382: train_loss -0.9657\n",
            "2025-12-23 01:00:03.109585: val_loss -0.9827\n",
            "2025-12-23 01:00:03.109674: Pseudo dice [np.float32(0.9883)]\n",
            "2025-12-23 01:00:03.109772: Epoch time: 22.45 s\n",
            "2025-12-23 01:00:04.469032: \n",
            "2025-12-23 01:00:04.469303: Epoch 488\n",
            "2025-12-23 01:00:04.469554: Current learning rate: 0.00547\n",
            "2025-12-23 01:00:26.970620: train_loss -0.9676\n",
            "2025-12-23 01:00:26.970840: val_loss -0.983\n",
            "2025-12-23 01:00:26.970935: Pseudo dice [np.float32(0.9859)]\n",
            "2025-12-23 01:00:26.971034: Epoch time: 22.5 s\n",
            "2025-12-23 01:00:28.324519: \n",
            "2025-12-23 01:00:28.324825: Epoch 489\n",
            "2025-12-23 01:00:28.324957: Current learning rate: 0.00546\n",
            "2025-12-23 01:00:50.766477: train_loss -0.969\n",
            "2025-12-23 01:00:50.766747: val_loss -0.9825\n",
            "2025-12-23 01:00:50.767050: Pseudo dice [np.float32(0.9869)]\n",
            "2025-12-23 01:00:50.767146: Epoch time: 22.44 s\n",
            "2025-12-23 01:00:52.134672: \n",
            "2025-12-23 01:00:52.134868: Epoch 490\n",
            "2025-12-23 01:00:52.134996: Current learning rate: 0.00546\n",
            "2025-12-23 01:01:14.640800: train_loss -0.9688\n",
            "2025-12-23 01:01:14.641040: val_loss -0.9846\n",
            "2025-12-23 01:01:14.641138: Pseudo dice [np.float32(0.9882)]\n",
            "2025-12-23 01:01:14.641294: Epoch time: 22.51 s\n",
            "2025-12-23 01:01:15.984931: \n",
            "2025-12-23 01:01:15.985183: Epoch 491\n",
            "2025-12-23 01:01:15.985339: Current learning rate: 0.00545\n",
            "2025-12-23 01:01:38.404905: train_loss -0.9468\n",
            "2025-12-23 01:01:38.405231: val_loss -0.9652\n",
            "2025-12-23 01:01:38.405351: Pseudo dice [np.float32(0.9731)]\n",
            "2025-12-23 01:01:38.405445: Epoch time: 22.42 s\n",
            "2025-12-23 01:01:39.739834: \n",
            "2025-12-23 01:01:39.740109: Epoch 492\n",
            "2025-12-23 01:01:39.740259: Current learning rate: 0.00544\n",
            "2025-12-23 01:02:02.194491: train_loss -0.954\n",
            "2025-12-23 01:02:02.194692: val_loss -0.9752\n",
            "2025-12-23 01:02:02.194779: Pseudo dice [np.float32(0.9798)]\n",
            "2025-12-23 01:02:02.194872: Epoch time: 22.46 s\n",
            "2025-12-23 01:02:03.543255: \n",
            "2025-12-23 01:02:03.543590: Epoch 493\n",
            "2025-12-23 01:02:03.543756: Current learning rate: 0.00543\n",
            "2025-12-23 01:02:25.946849: train_loss -0.9487\n",
            "2025-12-23 01:02:25.947191: val_loss -0.9774\n",
            "2025-12-23 01:02:25.947342: Pseudo dice [np.float32(0.9833)]\n",
            "2025-12-23 01:02:25.947498: Epoch time: 22.4 s\n",
            "2025-12-23 01:02:27.300685: \n",
            "2025-12-23 01:02:27.301023: Epoch 494\n",
            "2025-12-23 01:02:27.301148: Current learning rate: 0.00542\n",
            "2025-12-23 01:02:49.746902: train_loss -0.9621\n",
            "2025-12-23 01:02:49.747263: val_loss -0.9828\n",
            "2025-12-23 01:02:49.747372: Pseudo dice [np.float32(0.9872)]\n",
            "2025-12-23 01:02:49.747464: Epoch time: 22.45 s\n",
            "2025-12-23 01:02:51.094136: \n",
            "2025-12-23 01:02:51.094360: Epoch 495\n",
            "2025-12-23 01:02:51.094495: Current learning rate: 0.00541\n",
            "2025-12-23 01:03:13.493276: train_loss -0.9649\n",
            "2025-12-23 01:03:13.493485: val_loss -0.9697\n",
            "2025-12-23 01:03:13.493603: Pseudo dice [np.float32(0.9766)]\n",
            "2025-12-23 01:03:13.493701: Epoch time: 22.4 s\n",
            "2025-12-23 01:03:14.849133: \n",
            "2025-12-23 01:03:14.849362: Epoch 496\n",
            "2025-12-23 01:03:14.849511: Current learning rate: 0.0054\n",
            "2025-12-23 01:03:37.281542: train_loss -0.9614\n",
            "2025-12-23 01:03:37.281824: val_loss -0.9813\n",
            "2025-12-23 01:03:37.281973: Pseudo dice [np.float32(0.9856)]\n",
            "2025-12-23 01:03:37.282133: Epoch time: 22.43 s\n",
            "2025-12-23 01:03:38.616625: \n",
            "2025-12-23 01:03:38.616839: Epoch 497\n",
            "2025-12-23 01:03:38.616971: Current learning rate: 0.00539\n",
            "2025-12-23 01:04:01.031373: train_loss -0.9667\n",
            "2025-12-23 01:04:01.031744: val_loss -0.9818\n",
            "2025-12-23 01:04:01.031843: Pseudo dice [np.float32(0.9865)]\n",
            "2025-12-23 01:04:01.031936: Epoch time: 22.42 s\n",
            "2025-12-23 01:04:02.357638: \n",
            "2025-12-23 01:04:02.357956: Epoch 498\n",
            "2025-12-23 01:04:02.358204: Current learning rate: 0.00538\n",
            "2025-12-23 01:04:24.770358: train_loss -0.9698\n",
            "2025-12-23 01:04:24.770561: val_loss -0.9832\n",
            "2025-12-23 01:04:24.770645: Pseudo dice [np.float32(0.9887)]\n",
            "2025-12-23 01:04:24.770733: Epoch time: 22.41 s\n",
            "2025-12-23 01:04:26.111977: \n",
            "2025-12-23 01:04:26.112140: Epoch 499\n",
            "2025-12-23 01:04:26.112278: Current learning rate: 0.00537\n",
            "2025-12-23 01:04:48.564356: train_loss -0.9684\n",
            "2025-12-23 01:04:48.564608: val_loss -0.9835\n",
            "2025-12-23 01:04:48.564728: Pseudo dice [np.float32(0.9874)]\n",
            "2025-12-23 01:04:48.564824: Epoch time: 22.45 s\n",
            "2025-12-23 01:04:51.119204: \n",
            "2025-12-23 01:04:51.119555: Epoch 500\n",
            "2025-12-23 01:04:51.119685: Current learning rate: 0.00536\n",
            "2025-12-23 01:05:13.601502: train_loss -0.9684\n",
            "2025-12-23 01:05:13.601898: val_loss -0.9837\n",
            "2025-12-23 01:05:13.602029: Pseudo dice [np.float32(0.9876)]\n",
            "2025-12-23 01:05:13.602134: Epoch time: 22.48 s\n",
            "2025-12-23 01:05:14.945652: \n",
            "2025-12-23 01:05:14.945944: Epoch 501\n",
            "2025-12-23 01:05:14.946164: Current learning rate: 0.00535\n",
            "2025-12-23 01:05:37.456721: train_loss -0.9688\n",
            "2025-12-23 01:05:37.456952: val_loss -0.9845\n",
            "2025-12-23 01:05:37.457041: Pseudo dice [np.float32(0.9891)]\n",
            "2025-12-23 01:05:37.457187: Epoch time: 22.51 s\n",
            "2025-12-23 01:05:38.807540: \n",
            "2025-12-23 01:05:38.807887: Epoch 502\n",
            "2025-12-23 01:05:38.808036: Current learning rate: 0.00534\n",
            "2025-12-23 01:06:01.294986: train_loss -0.9689\n",
            "2025-12-23 01:06:01.295316: val_loss -0.9852\n",
            "2025-12-23 01:06:01.295516: Pseudo dice [np.float32(0.9884)]\n",
            "2025-12-23 01:06:01.295696: Epoch time: 22.49 s\n",
            "2025-12-23 01:06:02.636250: \n",
            "2025-12-23 01:06:02.636489: Epoch 503\n",
            "2025-12-23 01:06:02.636621: Current learning rate: 0.00533\n",
            "2025-12-23 01:06:25.089773: train_loss -0.9681\n",
            "2025-12-23 01:06:25.090018: val_loss -0.9822\n",
            "2025-12-23 01:06:25.090180: Pseudo dice [np.float32(0.9856)]\n",
            "2025-12-23 01:06:25.090343: Epoch time: 22.45 s\n",
            "2025-12-23 01:06:26.449041: \n",
            "2025-12-23 01:06:26.449354: Epoch 504\n",
            "2025-12-23 01:06:26.449492: Current learning rate: 0.00532\n",
            "2025-12-23 01:06:48.991966: train_loss -0.9696\n",
            "2025-12-23 01:06:48.992190: val_loss -0.9846\n",
            "2025-12-23 01:06:48.992338: Pseudo dice [np.float32(0.9879)]\n",
            "2025-12-23 01:06:48.992703: Epoch time: 22.54 s\n",
            "2025-12-23 01:06:50.338733: \n",
            "2025-12-23 01:06:50.338984: Epoch 505\n",
            "2025-12-23 01:06:50.339114: Current learning rate: 0.00531\n",
            "2025-12-23 01:07:12.789943: train_loss -0.9702\n",
            "2025-12-23 01:07:12.790131: val_loss -0.9843\n",
            "2025-12-23 01:07:12.790234: Pseudo dice [np.float32(0.9876)]\n",
            "2025-12-23 01:07:12.790353: Epoch time: 22.45 s\n",
            "2025-12-23 01:07:14.125115: \n",
            "2025-12-23 01:07:14.125336: Epoch 506\n",
            "2025-12-23 01:07:14.125534: Current learning rate: 0.0053\n",
            "2025-12-23 01:07:36.619823: train_loss -0.9698\n",
            "2025-12-23 01:07:36.620084: val_loss -0.9821\n",
            "2025-12-23 01:07:36.620178: Pseudo dice [np.float32(0.987)]\n",
            "2025-12-23 01:07:36.620284: Epoch time: 22.5 s\n",
            "2025-12-23 01:07:37.981938: \n",
            "2025-12-23 01:07:37.982205: Epoch 507\n",
            "2025-12-23 01:07:37.982386: Current learning rate: 0.00529\n",
            "2025-12-23 01:08:00.439619: train_loss -0.97\n",
            "2025-12-23 01:08:00.439917: val_loss -0.9856\n",
            "2025-12-23 01:08:00.440052: Pseudo dice [np.float32(0.9885)]\n",
            "2025-12-23 01:08:00.440149: Epoch time: 22.46 s\n",
            "2025-12-23 01:08:01.776461: \n",
            "2025-12-23 01:08:01.776850: Epoch 508\n",
            "2025-12-23 01:08:01.777022: Current learning rate: 0.00528\n",
            "2025-12-23 01:08:24.296666: train_loss -0.9698\n",
            "2025-12-23 01:08:24.297129: val_loss -0.9843\n",
            "2025-12-23 01:08:24.297340: Pseudo dice [np.float32(0.9878)]\n",
            "2025-12-23 01:08:24.297544: Epoch time: 22.52 s\n",
            "2025-12-23 01:08:25.638304: \n",
            "2025-12-23 01:08:25.638609: Epoch 509\n",
            "2025-12-23 01:08:25.638742: Current learning rate: 0.00527\n",
            "2025-12-23 01:08:48.073827: train_loss -0.9701\n",
            "2025-12-23 01:08:48.074244: val_loss -0.9848\n",
            "2025-12-23 01:08:48.074427: Pseudo dice [np.float32(0.9881)]\n",
            "2025-12-23 01:08:48.074583: Epoch time: 22.44 s\n",
            "2025-12-23 01:08:49.456411: \n",
            "2025-12-23 01:08:49.456666: Epoch 510\n",
            "2025-12-23 01:08:49.456807: Current learning rate: 0.00526\n",
            "2025-12-23 01:09:11.912503: train_loss -0.9703\n",
            "2025-12-23 01:09:11.912756: val_loss -0.9857\n",
            "2025-12-23 01:09:11.912852: Pseudo dice [np.float32(0.9888)]\n",
            "2025-12-23 01:09:11.912941: Epoch time: 22.46 s\n",
            "2025-12-23 01:09:13.265796: \n",
            "2025-12-23 01:09:13.265983: Epoch 511\n",
            "2025-12-23 01:09:13.266111: Current learning rate: 0.00525\n",
            "2025-12-23 01:09:35.733713: train_loss -0.9698\n",
            "2025-12-23 01:09:35.733967: val_loss -0.9847\n",
            "2025-12-23 01:09:35.734082: Pseudo dice [np.float32(0.9884)]\n",
            "2025-12-23 01:09:35.734200: Epoch time: 22.47 s\n",
            "2025-12-23 01:09:37.066929: \n",
            "2025-12-23 01:09:37.067237: Epoch 512\n",
            "2025-12-23 01:09:37.067396: Current learning rate: 0.00524\n",
            "2025-12-23 01:09:59.510898: train_loss -0.9722\n",
            "2025-12-23 01:09:59.511160: val_loss -0.9838\n",
            "2025-12-23 01:09:59.511294: Pseudo dice [np.float32(0.988)]\n",
            "2025-12-23 01:09:59.511419: Epoch time: 22.45 s\n",
            "2025-12-23 01:10:00.864553: \n",
            "2025-12-23 01:10:00.864804: Epoch 513\n",
            "2025-12-23 01:10:00.864940: Current learning rate: 0.00523\n",
            "2025-12-23 01:10:23.334337: train_loss -0.9715\n",
            "2025-12-23 01:10:23.334601: val_loss -0.9837\n",
            "2025-12-23 01:10:23.334783: Pseudo dice [np.float32(0.9878)]\n",
            "2025-12-23 01:10:23.334976: Epoch time: 22.47 s\n",
            "2025-12-23 01:10:24.717470: \n",
            "2025-12-23 01:10:24.717643: Epoch 514\n",
            "2025-12-23 01:10:24.717864: Current learning rate: 0.00522\n",
            "2025-12-23 01:10:47.195660: train_loss -0.9708\n",
            "2025-12-23 01:10:47.195963: val_loss -0.9849\n",
            "2025-12-23 01:10:47.196101: Pseudo dice [np.float32(0.9884)]\n",
            "2025-12-23 01:10:47.196201: Epoch time: 22.48 s\n",
            "2025-12-23 01:10:48.586073: \n",
            "2025-12-23 01:10:48.586464: Epoch 515\n",
            "2025-12-23 01:10:48.586603: Current learning rate: 0.00521\n",
            "2025-12-23 01:11:11.044690: train_loss -0.9716\n",
            "2025-12-23 01:11:11.044896: val_loss -0.9851\n",
            "2025-12-23 01:11:11.044984: Pseudo dice [np.float32(0.9885)]\n",
            "2025-12-23 01:11:11.045075: Epoch time: 22.46 s\n",
            "2025-12-23 01:11:12.416414: \n",
            "2025-12-23 01:11:12.416709: Epoch 516\n",
            "2025-12-23 01:11:12.416849: Current learning rate: 0.0052\n",
            "2025-12-23 01:11:34.823508: train_loss -0.9708\n",
            "2025-12-23 01:11:34.823925: val_loss -0.986\n",
            "2025-12-23 01:11:34.824076: Pseudo dice [np.float32(0.9889)]\n",
            "2025-12-23 01:11:34.824204: Epoch time: 22.41 s\n",
            "2025-12-23 01:11:36.227655: \n",
            "2025-12-23 01:11:36.227981: Epoch 517\n",
            "2025-12-23 01:11:36.228114: Current learning rate: 0.00519\n",
            "2025-12-23 01:11:58.642178: train_loss -0.9725\n",
            "2025-12-23 01:11:58.642391: val_loss -0.9854\n",
            "2025-12-23 01:11:58.642482: Pseudo dice [np.float32(0.9882)]\n",
            "2025-12-23 01:11:58.642642: Epoch time: 22.42 s\n",
            "2025-12-23 01:12:00.707698: \n",
            "2025-12-23 01:12:00.707983: Epoch 518\n",
            "2025-12-23 01:12:00.708129: Current learning rate: 0.00518\n",
            "2025-12-23 01:12:23.233512: train_loss -0.9723\n",
            "2025-12-23 01:12:23.233739: val_loss -0.9859\n",
            "2025-12-23 01:12:23.233845: Pseudo dice [np.float32(0.9888)]\n",
            "2025-12-23 01:12:23.233936: Epoch time: 22.53 s\n",
            "2025-12-23 01:12:24.592927: \n",
            "2025-12-23 01:12:24.593197: Epoch 519\n",
            "2025-12-23 01:12:24.593343: Current learning rate: 0.00518\n",
            "2025-12-23 01:12:47.044093: train_loss -0.9727\n",
            "2025-12-23 01:12:47.044346: val_loss -0.9851\n",
            "2025-12-23 01:12:47.044441: Pseudo dice [np.float32(0.989)]\n",
            "2025-12-23 01:12:47.044532: Epoch time: 22.45 s\n",
            "2025-12-23 01:12:48.382506: \n",
            "2025-12-23 01:12:48.382810: Epoch 520\n",
            "2025-12-23 01:12:48.382944: Current learning rate: 0.00517\n",
            "2025-12-23 01:13:10.825085: train_loss -0.9718\n",
            "2025-12-23 01:13:10.825344: val_loss -0.9868\n",
            "2025-12-23 01:13:10.825445: Pseudo dice [np.float32(0.9898)]\n",
            "2025-12-23 01:13:10.825536: Epoch time: 22.44 s\n",
            "2025-12-23 01:13:12.213518: \n",
            "2025-12-23 01:13:12.213900: Epoch 521\n",
            "2025-12-23 01:13:12.214033: Current learning rate: 0.00516\n",
            "2025-12-23 01:13:34.661128: train_loss -0.9715\n",
            "2025-12-23 01:13:34.661384: val_loss -0.9856\n",
            "2025-12-23 01:13:34.661483: Pseudo dice [np.float32(0.9887)]\n",
            "2025-12-23 01:13:34.661582: Epoch time: 22.45 s\n",
            "2025-12-23 01:13:36.068647: \n",
            "2025-12-23 01:13:36.068861: Epoch 522\n",
            "2025-12-23 01:13:36.068988: Current learning rate: 0.00515\n",
            "2025-12-23 01:13:58.527395: train_loss -0.9709\n",
            "2025-12-23 01:13:58.527861: val_loss -0.9848\n",
            "2025-12-23 01:13:58.527958: Pseudo dice [np.float32(0.9883)]\n",
            "2025-12-23 01:13:58.528080: Epoch time: 22.46 s\n",
            "2025-12-23 01:13:59.895682: \n",
            "2025-12-23 01:13:59.895972: Epoch 523\n",
            "2025-12-23 01:13:59.896128: Current learning rate: 0.00514\n",
            "2025-12-23 01:14:22.321736: train_loss -0.9722\n",
            "2025-12-23 01:14:22.321987: val_loss -0.9846\n",
            "2025-12-23 01:14:22.322074: Pseudo dice [np.float32(0.9886)]\n",
            "2025-12-23 01:14:22.322163: Epoch time: 22.43 s\n",
            "2025-12-23 01:14:23.678195: \n",
            "2025-12-23 01:14:23.678512: Epoch 524\n",
            "2025-12-23 01:14:23.678671: Current learning rate: 0.00513\n",
            "2025-12-23 01:14:46.157630: train_loss -0.9707\n",
            "2025-12-23 01:14:46.157853: val_loss -0.9841\n",
            "2025-12-23 01:14:46.157942: Pseudo dice [np.float32(0.9885)]\n",
            "2025-12-23 01:14:46.158034: Epoch time: 22.48 s\n",
            "2025-12-23 01:14:47.508046: \n",
            "2025-12-23 01:14:47.508212: Epoch 525\n",
            "2025-12-23 01:14:47.508397: Current learning rate: 0.00512\n",
            "2025-12-23 01:15:09.989987: train_loss -0.9716\n",
            "2025-12-23 01:15:09.990315: val_loss -0.9837\n",
            "2025-12-23 01:15:09.990434: Pseudo dice [np.float32(0.9877)]\n",
            "2025-12-23 01:15:09.990563: Epoch time: 22.48 s\n",
            "2025-12-23 01:15:11.338127: \n",
            "2025-12-23 01:15:11.338526: Epoch 526\n",
            "2025-12-23 01:15:11.338668: Current learning rate: 0.00511\n",
            "2025-12-23 01:15:33.796058: train_loss -0.9713\n",
            "2025-12-23 01:15:33.796334: val_loss -0.9854\n",
            "2025-12-23 01:15:33.796433: Pseudo dice [np.float32(0.9885)]\n",
            "2025-12-23 01:15:33.796525: Epoch time: 22.46 s\n",
            "2025-12-23 01:15:35.189986: \n",
            "2025-12-23 01:15:35.190269: Epoch 527\n",
            "2025-12-23 01:15:35.190421: Current learning rate: 0.0051\n",
            "2025-12-23 01:15:57.644974: train_loss -0.9723\n",
            "2025-12-23 01:15:57.645411: val_loss -0.9858\n",
            "2025-12-23 01:15:57.645513: Pseudo dice [np.float32(0.9897)]\n",
            "2025-12-23 01:15:57.645622: Epoch time: 22.46 s\n",
            "2025-12-23 01:15:58.997031: \n",
            "2025-12-23 01:15:58.997260: Epoch 528\n",
            "2025-12-23 01:15:58.997457: Current learning rate: 0.00509\n",
            "2025-12-23 01:16:21.454154: train_loss -0.9712\n",
            "2025-12-23 01:16:21.454374: val_loss -0.9838\n",
            "2025-12-23 01:16:21.454591: Pseudo dice [np.float32(0.9874)]\n",
            "2025-12-23 01:16:21.454701: Epoch time: 22.46 s\n",
            "2025-12-23 01:16:22.795330: \n",
            "2025-12-23 01:16:22.795603: Epoch 529\n",
            "2025-12-23 01:16:22.795738: Current learning rate: 0.00508\n",
            "2025-12-23 01:16:45.228011: train_loss -0.9722\n",
            "2025-12-23 01:16:45.228246: val_loss -0.9849\n",
            "2025-12-23 01:16:45.228365: Pseudo dice [np.float32(0.9882)]\n",
            "2025-12-23 01:16:45.228459: Epoch time: 22.43 s\n",
            "2025-12-23 01:16:46.557250: \n",
            "2025-12-23 01:16:46.557516: Epoch 530\n",
            "2025-12-23 01:16:46.557651: Current learning rate: 0.00507\n",
            "2025-12-23 01:17:08.996561: train_loss -0.9714\n",
            "2025-12-23 01:17:08.996870: val_loss -0.9836\n",
            "2025-12-23 01:17:08.996976: Pseudo dice [np.float32(0.9884)]\n",
            "2025-12-23 01:17:08.997068: Epoch time: 22.44 s\n",
            "2025-12-23 01:17:10.349344: \n",
            "2025-12-23 01:17:10.349635: Epoch 531\n",
            "2025-12-23 01:17:10.349956: Current learning rate: 0.00506\n",
            "2025-12-23 01:17:32.772042: train_loss -0.9723\n",
            "2025-12-23 01:17:32.772324: val_loss -0.985\n",
            "2025-12-23 01:17:32.772447: Pseudo dice [np.float32(0.9887)]\n",
            "2025-12-23 01:17:32.772551: Epoch time: 22.42 s\n",
            "2025-12-23 01:17:34.157109: \n",
            "2025-12-23 01:17:34.157438: Epoch 532\n",
            "2025-12-23 01:17:34.157606: Current learning rate: 0.00505\n",
            "2025-12-23 01:17:56.644451: train_loss -0.9713\n",
            "2025-12-23 01:17:56.644679: val_loss -0.9847\n",
            "2025-12-23 01:17:56.644763: Pseudo dice [np.float32(0.9879)]\n",
            "2025-12-23 01:17:56.644869: Epoch time: 22.49 s\n",
            "2025-12-23 01:17:57.987490: \n",
            "2025-12-23 01:17:57.987747: Epoch 533\n",
            "2025-12-23 01:17:57.987922: Current learning rate: 0.00504\n",
            "2025-12-23 01:18:20.450728: train_loss -0.9709\n",
            "2025-12-23 01:18:20.450940: val_loss -0.9844\n",
            "2025-12-23 01:18:20.451029: Pseudo dice [np.float32(0.9886)]\n",
            "2025-12-23 01:18:20.451118: Epoch time: 22.46 s\n",
            "2025-12-23 01:18:21.817905: \n",
            "2025-12-23 01:18:21.818152: Epoch 534\n",
            "2025-12-23 01:18:21.818295: Current learning rate: 0.00503\n",
            "2025-12-23 01:18:44.230568: train_loss -0.9718\n",
            "2025-12-23 01:18:44.230769: val_loss -0.9845\n",
            "2025-12-23 01:18:44.230860: Pseudo dice [np.float32(0.9885)]\n",
            "2025-12-23 01:18:44.230959: Epoch time: 22.41 s\n",
            "2025-12-23 01:18:45.573699: \n",
            "2025-12-23 01:18:45.574013: Epoch 535\n",
            "2025-12-23 01:18:45.574193: Current learning rate: 0.00502\n",
            "2025-12-23 01:19:07.994127: train_loss -0.9722\n",
            "2025-12-23 01:19:07.994456: val_loss -0.9846\n",
            "2025-12-23 01:19:07.994579: Pseudo dice [np.float32(0.9878)]\n",
            "2025-12-23 01:19:07.994699: Epoch time: 22.42 s\n",
            "2025-12-23 01:19:10.049184: \n",
            "2025-12-23 01:19:10.049470: Epoch 536\n",
            "2025-12-23 01:19:10.049650: Current learning rate: 0.00501\n",
            "2025-12-23 01:19:32.495220: train_loss -0.973\n",
            "2025-12-23 01:19:32.495472: val_loss -0.9833\n",
            "2025-12-23 01:19:32.495568: Pseudo dice [np.float32(0.9871)]\n",
            "2025-12-23 01:19:32.495670: Epoch time: 22.45 s\n",
            "2025-12-23 01:19:33.832427: \n",
            "2025-12-23 01:19:33.832720: Epoch 537\n",
            "2025-12-23 01:19:33.832917: Current learning rate: 0.005\n",
            "2025-12-23 01:19:56.288200: train_loss -0.9715\n",
            "2025-12-23 01:19:56.288511: val_loss -0.9843\n",
            "2025-12-23 01:19:56.288611: Pseudo dice [np.float32(0.9885)]\n",
            "2025-12-23 01:19:56.288702: Epoch time: 22.46 s\n",
            "2025-12-23 01:19:57.604209: \n",
            "2025-12-23 01:19:57.604438: Epoch 538\n",
            "2025-12-23 01:19:57.604583: Current learning rate: 0.00499\n",
            "2025-12-23 01:20:20.054111: train_loss -0.9724\n",
            "2025-12-23 01:20:20.054442: val_loss -0.985\n",
            "2025-12-23 01:20:20.054549: Pseudo dice [np.float32(0.9886)]\n",
            "2025-12-23 01:20:20.054639: Epoch time: 22.45 s\n",
            "2025-12-23 01:20:21.407865: \n",
            "2025-12-23 01:20:21.408177: Epoch 539\n",
            "2025-12-23 01:20:21.408346: Current learning rate: 0.00498\n",
            "2025-12-23 01:20:43.881266: train_loss -0.9712\n",
            "2025-12-23 01:20:43.881665: val_loss -0.9854\n",
            "2025-12-23 01:20:43.881861: Pseudo dice [np.float32(0.9887)]\n",
            "2025-12-23 01:20:43.881958: Epoch time: 22.47 s\n",
            "2025-12-23 01:20:45.246068: \n",
            "2025-12-23 01:20:45.246423: Epoch 540\n",
            "2025-12-23 01:20:45.246570: Current learning rate: 0.00497\n",
            "2025-12-23 01:21:07.692498: train_loss -0.9727\n",
            "2025-12-23 01:21:07.692719: val_loss -0.9861\n",
            "2025-12-23 01:21:07.692812: Pseudo dice [np.float32(0.9886)]\n",
            "2025-12-23 01:21:07.692903: Epoch time: 22.45 s\n",
            "2025-12-23 01:21:09.034160: \n",
            "2025-12-23 01:21:09.034537: Epoch 541\n",
            "2025-12-23 01:21:09.034689: Current learning rate: 0.00496\n",
            "2025-12-23 01:21:31.483964: train_loss -0.9724\n",
            "2025-12-23 01:21:31.484235: val_loss -0.986\n",
            "2025-12-23 01:21:31.484358: Pseudo dice [np.float32(0.9895)]\n",
            "2025-12-23 01:21:31.484473: Epoch time: 22.45 s\n",
            "2025-12-23 01:21:32.817303: \n",
            "2025-12-23 01:21:32.817635: Epoch 542\n",
            "2025-12-23 01:21:32.817794: Current learning rate: 0.00495\n",
            "2025-12-23 01:21:55.276839: train_loss -0.9725\n",
            "2025-12-23 01:21:55.277089: val_loss -0.985\n",
            "2025-12-23 01:21:55.277390: Pseudo dice [np.float32(0.9864)]\n",
            "2025-12-23 01:21:55.277509: Epoch time: 22.46 s\n",
            "2025-12-23 01:21:56.621605: \n",
            "2025-12-23 01:21:56.621836: Epoch 543\n",
            "2025-12-23 01:21:56.622026: Current learning rate: 0.00494\n",
            "2025-12-23 01:22:19.064520: train_loss -0.9722\n",
            "2025-12-23 01:22:19.064762: val_loss -0.9852\n",
            "2025-12-23 01:22:19.064876: Pseudo dice [np.float32(0.9892)]\n",
            "2025-12-23 01:22:19.064991: Epoch time: 22.44 s\n",
            "2025-12-23 01:22:20.410307: \n",
            "2025-12-23 01:22:20.410658: Epoch 544\n",
            "2025-12-23 01:22:20.410791: Current learning rate: 0.00493\n",
            "2025-12-23 01:22:42.829645: train_loss -0.9732\n",
            "2025-12-23 01:22:42.829983: val_loss -0.9841\n",
            "2025-12-23 01:22:42.830208: Pseudo dice [np.float32(0.9884)]\n",
            "2025-12-23 01:22:42.830391: Epoch time: 22.42 s\n",
            "2025-12-23 01:22:44.194117: \n",
            "2025-12-23 01:22:44.194328: Epoch 545\n",
            "2025-12-23 01:22:44.194540: Current learning rate: 0.00492\n",
            "2025-12-23 01:23:06.672985: train_loss -0.9734\n",
            "2025-12-23 01:23:06.673481: val_loss -0.9844\n",
            "2025-12-23 01:23:06.673666: Pseudo dice [np.float32(0.9888)]\n",
            "2025-12-23 01:23:06.673860: Epoch time: 22.48 s\n",
            "2025-12-23 01:23:08.015645: \n",
            "2025-12-23 01:23:08.015954: Epoch 546\n",
            "2025-12-23 01:23:08.016118: Current learning rate: 0.00491\n",
            "2025-12-23 01:23:30.485616: train_loss -0.9725\n",
            "2025-12-23 01:23:30.485832: val_loss -0.9848\n",
            "2025-12-23 01:23:30.485920: Pseudo dice [np.float32(0.9889)]\n",
            "2025-12-23 01:23:30.486011: Epoch time: 22.47 s\n",
            "2025-12-23 01:23:31.811325: \n",
            "2025-12-23 01:23:31.811616: Epoch 547\n",
            "2025-12-23 01:23:31.811743: Current learning rate: 0.0049\n",
            "2025-12-23 01:23:54.235871: train_loss -0.9735\n",
            "2025-12-23 01:23:54.236073: val_loss -0.9865\n",
            "2025-12-23 01:23:54.236161: Pseudo dice [np.float32(0.9897)]\n",
            "2025-12-23 01:23:54.236311: Epoch time: 22.43 s\n",
            "2025-12-23 01:23:54.236440: Yayy! New best EMA pseudo Dice: 0.9886000156402588\n",
            "2025-12-23 01:23:56.192374: \n",
            "2025-12-23 01:23:56.192601: Epoch 548\n",
            "2025-12-23 01:23:56.192788: Current learning rate: 0.00489\n",
            "2025-12-23 01:24:18.711003: train_loss -0.9736\n",
            "2025-12-23 01:24:18.711443: val_loss -0.9857\n",
            "2025-12-23 01:24:18.711599: Pseudo dice [np.float32(0.9887)]\n",
            "2025-12-23 01:24:18.711724: Epoch time: 22.52 s\n",
            "2025-12-23 01:24:18.711822: Yayy! New best EMA pseudo Dice: 0.9886000156402588\n",
            "2025-12-23 01:24:20.626198: \n",
            "2025-12-23 01:24:20.626494: Epoch 549\n",
            "2025-12-23 01:24:20.626664: Current learning rate: 0.00488\n",
            "2025-12-23 01:24:43.125802: train_loss -0.9734\n",
            "2025-12-23 01:24:43.126152: val_loss -0.9856\n",
            "2025-12-23 01:24:43.126316: Pseudo dice [np.float32(0.9881)]\n",
            "2025-12-23 01:24:43.126476: Epoch time: 22.5 s\n",
            "2025-12-23 01:24:44.998057: \n",
            "2025-12-23 01:24:44.998334: Epoch 550\n",
            "2025-12-23 01:24:44.998483: Current learning rate: 0.00487\n",
            "2025-12-23 01:25:07.410541: train_loss -0.9733\n",
            "2025-12-23 01:25:07.410777: val_loss -0.9859\n",
            "2025-12-23 01:25:07.410869: Pseudo dice [np.float32(0.9896)]\n",
            "2025-12-23 01:25:07.410962: Epoch time: 22.41 s\n",
            "2025-12-23 01:25:07.411040: Yayy! New best EMA pseudo Dice: 0.9886000156402588\n",
            "2025-12-23 01:25:09.271322: \n",
            "2025-12-23 01:25:09.271643: Epoch 551\n",
            "2025-12-23 01:25:09.271781: Current learning rate: 0.00486\n",
            "2025-12-23 01:25:31.751373: train_loss -0.9725\n",
            "2025-12-23 01:25:31.751572: val_loss -0.9856\n",
            "2025-12-23 01:25:31.751657: Pseudo dice [np.float32(0.9894)]\n",
            "2025-12-23 01:25:31.751748: Epoch time: 22.48 s\n",
            "2025-12-23 01:25:31.751832: Yayy! New best EMA pseudo Dice: 0.9886999726295471\n",
            "2025-12-23 01:25:33.596843: \n",
            "2025-12-23 01:25:33.597168: Epoch 552\n",
            "2025-12-23 01:25:33.597320: Current learning rate: 0.00485\n",
            "2025-12-23 01:25:56.024298: train_loss -0.9715\n",
            "2025-12-23 01:25:56.024549: val_loss -0.9853\n",
            "2025-12-23 01:25:56.024643: Pseudo dice [np.float32(0.9888)]\n",
            "2025-12-23 01:25:56.024741: Epoch time: 22.43 s\n",
            "2025-12-23 01:25:56.024817: Yayy! New best EMA pseudo Dice: 0.9886999726295471\n",
            "2025-12-23 01:25:58.527613: \n",
            "2025-12-23 01:25:58.528062: Epoch 553\n",
            "2025-12-23 01:25:58.528211: Current learning rate: 0.00484\n",
            "2025-12-23 01:26:21.082675: train_loss -0.9716\n",
            "2025-12-23 01:26:21.082897: val_loss -0.9836\n",
            "2025-12-23 01:26:21.083018: Pseudo dice [np.float32(0.988)]\n",
            "2025-12-23 01:26:21.083128: Epoch time: 22.56 s\n",
            "2025-12-23 01:26:22.443208: \n",
            "2025-12-23 01:26:22.443460: Epoch 554\n",
            "2025-12-23 01:26:22.443593: Current learning rate: 0.00484\n",
            "2025-12-23 01:26:44.891466: train_loss -0.9728\n",
            "2025-12-23 01:26:44.891815: val_loss -0.9851\n",
            "2025-12-23 01:26:44.891937: Pseudo dice [np.float32(0.9892)]\n",
            "2025-12-23 01:26:44.892067: Epoch time: 22.45 s\n",
            "2025-12-23 01:26:46.223631: \n",
            "2025-12-23 01:26:46.223974: Epoch 555\n",
            "2025-12-23 01:26:46.224108: Current learning rate: 0.00483\n",
            "2025-12-23 01:27:08.711367: train_loss -0.9719\n",
            "2025-12-23 01:27:08.711918: val_loss -0.9845\n",
            "2025-12-23 01:27:08.712063: Pseudo dice [np.float32(0.9882)]\n",
            "2025-12-23 01:27:08.712179: Epoch time: 22.49 s\n",
            "2025-12-23 01:27:10.058607: \n",
            "2025-12-23 01:27:10.058835: Epoch 556\n",
            "2025-12-23 01:27:10.059024: Current learning rate: 0.00482\n",
            "2025-12-23 01:27:32.505378: train_loss -0.9713\n",
            "2025-12-23 01:27:32.505586: val_loss -0.985\n",
            "2025-12-23 01:27:32.505677: Pseudo dice [np.float32(0.9884)]\n",
            "2025-12-23 01:27:32.505769: Epoch time: 22.45 s\n",
            "2025-12-23 01:27:33.857615: \n",
            "2025-12-23 01:27:33.857898: Epoch 557\n",
            "2025-12-23 01:27:33.858027: Current learning rate: 0.00481\n",
            "2025-12-23 01:27:56.326885: train_loss -0.9729\n",
            "2025-12-23 01:27:56.327092: val_loss -0.9865\n",
            "2025-12-23 01:27:56.327179: Pseudo dice [np.float32(0.9893)]\n",
            "2025-12-23 01:27:56.327350: Epoch time: 22.47 s\n",
            "2025-12-23 01:27:57.689979: \n",
            "2025-12-23 01:27:57.690185: Epoch 558\n",
            "2025-12-23 01:27:57.690363: Current learning rate: 0.0048\n",
            "2025-12-23 01:28:20.173774: train_loss -0.9734\n",
            "2025-12-23 01:28:20.173974: val_loss -0.9857\n",
            "2025-12-23 01:28:20.174062: Pseudo dice [np.float32(0.9885)]\n",
            "2025-12-23 01:28:20.174150: Epoch time: 22.49 s\n",
            "2025-12-23 01:28:21.542526: \n",
            "2025-12-23 01:28:21.542855: Epoch 559\n",
            "2025-12-23 01:28:21.543118: Current learning rate: 0.00479\n",
            "2025-12-23 01:28:44.020139: train_loss -0.9719\n",
            "2025-12-23 01:28:44.020487: val_loss -0.9849\n",
            "2025-12-23 01:28:44.020615: Pseudo dice [np.float32(0.9886)]\n",
            "2025-12-23 01:28:44.020730: Epoch time: 22.48 s\n",
            "2025-12-23 01:28:45.404071: \n",
            "2025-12-23 01:28:45.404392: Epoch 560\n",
            "2025-12-23 01:28:45.404563: Current learning rate: 0.00478\n",
            "2025-12-23 01:29:07.855426: train_loss -0.9733\n",
            "2025-12-23 01:29:07.855716: val_loss -0.986\n",
            "2025-12-23 01:29:07.855809: Pseudo dice [np.float32(0.9895)]\n",
            "2025-12-23 01:29:07.855967: Epoch time: 22.45 s\n",
            "2025-12-23 01:29:07.856066: Yayy! New best EMA pseudo Dice: 0.9887999892234802\n",
            "2025-12-23 01:29:09.811279: \n",
            "2025-12-23 01:29:09.811573: Epoch 561\n",
            "2025-12-23 01:29:09.811728: Current learning rate: 0.00477\n",
            "2025-12-23 01:29:32.274451: train_loss -0.9731\n",
            "2025-12-23 01:29:32.274761: val_loss -0.9858\n",
            "2025-12-23 01:29:32.274989: Pseudo dice [np.float32(0.9894)]\n",
            "2025-12-23 01:29:32.275201: Epoch time: 22.46 s\n",
            "2025-12-23 01:29:32.275425: Yayy! New best EMA pseudo Dice: 0.9887999892234802\n",
            "2025-12-23 01:29:34.186185: \n",
            "2025-12-23 01:29:34.186523: Epoch 562\n",
            "2025-12-23 01:29:34.186667: Current learning rate: 0.00476\n",
            "2025-12-23 01:29:56.701823: train_loss -0.9727\n",
            "2025-12-23 01:29:56.702170: val_loss -0.9836\n",
            "2025-12-23 01:29:56.702312: Pseudo dice [np.float32(0.9874)]\n",
            "2025-12-23 01:29:56.702410: Epoch time: 22.52 s\n",
            "2025-12-23 01:29:58.073109: \n",
            "2025-12-23 01:29:58.073310: Epoch 563\n",
            "2025-12-23 01:29:58.073440: Current learning rate: 0.00475\n",
            "2025-12-23 01:30:20.538405: train_loss -0.9723\n",
            "2025-12-23 01:30:20.538751: val_loss -0.9851\n",
            "2025-12-23 01:30:20.538962: Pseudo dice [np.float32(0.9883)]\n",
            "2025-12-23 01:30:20.539131: Epoch time: 22.47 s\n",
            "2025-12-23 01:30:21.896507: \n",
            "2025-12-23 01:30:21.896922: Epoch 564\n",
            "2025-12-23 01:30:21.897084: Current learning rate: 0.00474\n",
            "2025-12-23 01:30:44.342896: train_loss -0.9722\n",
            "2025-12-23 01:30:44.343331: val_loss -0.9862\n",
            "2025-12-23 01:30:44.343481: Pseudo dice [np.float32(0.9899)]\n",
            "2025-12-23 01:30:44.343595: Epoch time: 22.45 s\n",
            "2025-12-23 01:30:45.706968: \n",
            "2025-12-23 01:30:45.707295: Epoch 565\n",
            "2025-12-23 01:30:45.707441: Current learning rate: 0.00473\n",
            "2025-12-23 01:31:08.134174: train_loss -0.9735\n",
            "2025-12-23 01:31:08.134460: val_loss -0.9837\n",
            "2025-12-23 01:31:08.134708: Pseudo dice [np.float32(0.988)]\n",
            "2025-12-23 01:31:08.134869: Epoch time: 22.43 s\n",
            "2025-12-23 01:31:09.503260: \n",
            "2025-12-23 01:31:09.503451: Epoch 566\n",
            "2025-12-23 01:31:09.503580: Current learning rate: 0.00472\n",
            "2025-12-23 01:31:31.954363: train_loss -0.9733\n",
            "2025-12-23 01:31:31.954578: val_loss -0.9856\n",
            "2025-12-23 01:31:31.954699: Pseudo dice [np.float32(0.9895)]\n",
            "2025-12-23 01:31:31.954851: Epoch time: 22.45 s\n",
            "2025-12-23 01:31:33.323703: \n",
            "2025-12-23 01:31:33.324033: Epoch 567\n",
            "2025-12-23 01:31:33.324169: Current learning rate: 0.00471\n",
            "2025-12-23 01:31:55.755838: train_loss -0.9726\n",
            "2025-12-23 01:31:55.756153: val_loss -0.9849\n",
            "2025-12-23 01:31:55.756419: Pseudo dice [np.float32(0.9892)]\n",
            "2025-12-23 01:31:55.756612: Epoch time: 22.43 s\n",
            "2025-12-23 01:31:57.102417: \n",
            "2025-12-23 01:31:57.102578: Epoch 568\n",
            "2025-12-23 01:31:57.102707: Current learning rate: 0.0047\n",
            "2025-12-23 01:32:19.538370: train_loss -0.973\n",
            "2025-12-23 01:32:19.538567: val_loss -0.985\n",
            "2025-12-23 01:32:19.538668: Pseudo dice [np.float32(0.9882)]\n",
            "2025-12-23 01:32:19.538778: Epoch time: 22.44 s\n",
            "2025-12-23 01:32:20.876622: \n",
            "2025-12-23 01:32:20.876828: Epoch 569\n",
            "2025-12-23 01:32:20.876958: Current learning rate: 0.00469\n",
            "2025-12-23 01:32:43.302154: train_loss -0.9732\n",
            "2025-12-23 01:32:43.302448: val_loss -0.9845\n",
            "2025-12-23 01:32:43.302586: Pseudo dice [np.float32(0.9889)]\n",
            "2025-12-23 01:32:43.302696: Epoch time: 22.43 s\n",
            "2025-12-23 01:32:44.669119: \n",
            "2025-12-23 01:32:44.669459: Epoch 570\n",
            "2025-12-23 01:32:44.669596: Current learning rate: 0.00468\n",
            "2025-12-23 01:33:07.087422: train_loss -0.9732\n",
            "2025-12-23 01:33:07.087647: val_loss -0.986\n",
            "2025-12-23 01:33:07.087769: Pseudo dice [np.float32(0.9888)]\n",
            "2025-12-23 01:33:07.087981: Epoch time: 22.42 s\n",
            "2025-12-23 01:33:09.101310: \n",
            "2025-12-23 01:33:09.101565: Epoch 571\n",
            "2025-12-23 01:33:09.101699: Current learning rate: 0.00467\n",
            "2025-12-23 01:33:31.618868: train_loss -0.9732\n",
            "2025-12-23 01:33:31.619103: val_loss -0.9842\n",
            "2025-12-23 01:33:31.619210: Pseudo dice [np.float32(0.9878)]\n",
            "2025-12-23 01:33:31.619318: Epoch time: 22.52 s\n",
            "2025-12-23 01:33:32.945068: \n",
            "2025-12-23 01:33:32.945289: Epoch 572\n",
            "2025-12-23 01:33:32.945458: Current learning rate: 0.00466\n",
            "2025-12-23 01:33:55.404499: train_loss -0.972\n",
            "2025-12-23 01:33:55.404720: val_loss -0.9853\n",
            "2025-12-23 01:33:55.404838: Pseudo dice [np.float32(0.9895)]\n",
            "2025-12-23 01:33:55.404960: Epoch time: 22.46 s\n",
            "2025-12-23 01:33:56.720366: \n",
            "2025-12-23 01:33:56.720611: Epoch 573\n",
            "2025-12-23 01:33:56.720746: Current learning rate: 0.00465\n",
            "2025-12-23 01:34:19.185208: train_loss -0.973\n",
            "2025-12-23 01:34:19.185440: val_loss -0.9849\n",
            "2025-12-23 01:34:19.185543: Pseudo dice [np.float32(0.9882)]\n",
            "2025-12-23 01:34:19.185634: Epoch time: 22.47 s\n",
            "2025-12-23 01:34:20.541614: \n",
            "2025-12-23 01:34:20.541952: Epoch 574\n",
            "2025-12-23 01:34:20.542089: Current learning rate: 0.00464\n",
            "2025-12-23 01:34:43.076132: train_loss -0.9728\n",
            "2025-12-23 01:34:43.076445: val_loss -0.9845\n",
            "2025-12-23 01:34:43.076595: Pseudo dice [np.float32(0.9891)]\n",
            "2025-12-23 01:34:43.076693: Epoch time: 22.54 s\n",
            "2025-12-23 01:34:44.443535: \n",
            "2025-12-23 01:34:44.443859: Epoch 575\n",
            "2025-12-23 01:34:44.444020: Current learning rate: 0.00463\n",
            "2025-12-23 01:35:06.909554: train_loss -0.9737\n",
            "2025-12-23 01:35:06.909833: val_loss -0.9856\n",
            "2025-12-23 01:35:06.909989: Pseudo dice [np.float32(0.9896)]\n",
            "2025-12-23 01:35:06.910115: Epoch time: 22.47 s\n",
            "2025-12-23 01:35:08.288907: \n",
            "2025-12-23 01:35:08.289095: Epoch 576\n",
            "2025-12-23 01:35:08.289269: Current learning rate: 0.00462\n",
            "2025-12-23 01:35:30.757664: train_loss -0.9728\n",
            "2025-12-23 01:35:30.757881: val_loss -0.9841\n",
            "2025-12-23 01:35:30.757998: Pseudo dice [np.float32(0.9876)]\n",
            "2025-12-23 01:35:30.758115: Epoch time: 22.47 s\n",
            "2025-12-23 01:35:32.113962: \n",
            "2025-12-23 01:35:32.114264: Epoch 577\n",
            "2025-12-23 01:35:32.114411: Current learning rate: 0.00461\n",
            "2025-12-23 01:35:54.600782: train_loss -0.9733\n",
            "2025-12-23 01:35:54.601091: val_loss -0.9855\n",
            "2025-12-23 01:35:54.601255: Pseudo dice [np.float32(0.989)]\n",
            "2025-12-23 01:35:54.601387: Epoch time: 22.49 s\n",
            "2025-12-23 01:35:55.975526: \n",
            "2025-12-23 01:35:55.975831: Epoch 578\n",
            "2025-12-23 01:35:55.975974: Current learning rate: 0.0046\n",
            "2025-12-23 01:36:18.434426: train_loss -0.9712\n",
            "2025-12-23 01:36:18.434871: val_loss -0.9842\n",
            "2025-12-23 01:36:18.435026: Pseudo dice [np.float32(0.9866)]\n",
            "2025-12-23 01:36:18.435147: Epoch time: 22.46 s\n",
            "2025-12-23 01:36:19.796500: \n",
            "2025-12-23 01:36:19.796692: Epoch 579\n",
            "2025-12-23 01:36:19.796821: Current learning rate: 0.00459\n",
            "2025-12-23 01:36:42.245394: train_loss -0.9642\n",
            "2025-12-23 01:36:42.245665: val_loss -0.9826\n",
            "2025-12-23 01:36:42.245778: Pseudo dice [np.float32(0.9864)]\n",
            "2025-12-23 01:36:42.245900: Epoch time: 22.45 s\n",
            "2025-12-23 01:36:43.610529: \n",
            "2025-12-23 01:36:43.610855: Epoch 580\n",
            "2025-12-23 01:36:43.610990: Current learning rate: 0.00458\n",
            "2025-12-23 01:37:06.070707: train_loss -0.9682\n",
            "2025-12-23 01:37:06.070920: val_loss -0.9838\n",
            "2025-12-23 01:37:06.071031: Pseudo dice [np.float32(0.9882)]\n",
            "2025-12-23 01:37:06.071146: Epoch time: 22.46 s\n",
            "2025-12-23 01:37:07.408529: \n",
            "2025-12-23 01:37:07.408816: Epoch 581\n",
            "2025-12-23 01:37:07.408973: Current learning rate: 0.00457\n",
            "2025-12-23 01:37:29.842877: train_loss -0.9699\n",
            "2025-12-23 01:37:29.843086: val_loss -0.9848\n",
            "2025-12-23 01:37:29.843250: Pseudo dice [np.float32(0.9877)]\n",
            "2025-12-23 01:37:29.843367: Epoch time: 22.44 s\n",
            "2025-12-23 01:37:31.232413: \n",
            "2025-12-23 01:37:31.232719: Epoch 582\n",
            "2025-12-23 01:37:31.232851: Current learning rate: 0.00456\n",
            "2025-12-23 01:37:53.694032: train_loss -0.9664\n",
            "2025-12-23 01:37:53.694343: val_loss -0.9819\n",
            "2025-12-23 01:37:53.694474: Pseudo dice [np.float32(0.9866)]\n",
            "2025-12-23 01:37:53.694633: Epoch time: 22.46 s\n",
            "2025-12-23 01:37:55.061062: \n",
            "2025-12-23 01:37:55.061270: Epoch 583\n",
            "2025-12-23 01:37:55.061439: Current learning rate: 0.00455\n",
            "2025-12-23 01:38:17.517154: train_loss -0.9679\n",
            "2025-12-23 01:38:17.517396: val_loss -0.9838\n",
            "2025-12-23 01:38:17.517512: Pseudo dice [np.float32(0.9876)]\n",
            "2025-12-23 01:38:17.517627: Epoch time: 22.46 s\n",
            "2025-12-23 01:38:18.908175: \n",
            "2025-12-23 01:38:18.908536: Epoch 584\n",
            "2025-12-23 01:38:18.908695: Current learning rate: 0.00454\n",
            "2025-12-23 01:38:41.340731: train_loss -0.9712\n",
            "2025-12-23 01:38:41.341034: val_loss -0.984\n",
            "2025-12-23 01:38:41.341143: Pseudo dice [np.float32(0.9884)]\n",
            "2025-12-23 01:38:41.341341: Epoch time: 22.43 s\n",
            "2025-12-23 01:38:42.703848: \n",
            "2025-12-23 01:38:42.704152: Epoch 585\n",
            "2025-12-23 01:38:42.704296: Current learning rate: 0.00453\n",
            "2025-12-23 01:39:05.126720: train_loss -0.9729\n",
            "2025-12-23 01:39:05.126951: val_loss -0.9846\n",
            "2025-12-23 01:39:05.127064: Pseudo dice [np.float32(0.9881)]\n",
            "2025-12-23 01:39:05.127199: Epoch time: 22.42 s\n",
            "2025-12-23 01:39:06.488406: \n",
            "2025-12-23 01:39:06.488749: Epoch 586\n",
            "2025-12-23 01:39:06.488894: Current learning rate: 0.00452\n",
            "2025-12-23 01:39:28.940507: train_loss -0.9722\n",
            "2025-12-23 01:39:28.940746: val_loss -0.9856\n",
            "2025-12-23 01:39:28.940887: Pseudo dice [np.float32(0.9889)]\n",
            "2025-12-23 01:39:28.940994: Epoch time: 22.45 s\n",
            "2025-12-23 01:39:30.286072: \n",
            "2025-12-23 01:39:30.286291: Epoch 587\n",
            "2025-12-23 01:39:30.286449: Current learning rate: 0.00451\n",
            "2025-12-23 01:39:52.757181: train_loss -0.973\n",
            "2025-12-23 01:39:52.757501: val_loss -0.9855\n",
            "2025-12-23 01:39:52.757609: Pseudo dice [np.float32(0.9888)]\n",
            "2025-12-23 01:39:52.757702: Epoch time: 22.47 s\n",
            "2025-12-23 01:39:54.139519: \n",
            "2025-12-23 01:39:54.139820: Epoch 588\n",
            "2025-12-23 01:39:54.139950: Current learning rate: 0.0045\n",
            "2025-12-23 01:40:16.557927: train_loss -0.9719\n",
            "2025-12-23 01:40:16.558153: val_loss -0.9853\n",
            "2025-12-23 01:40:16.558281: Pseudo dice [np.float32(0.9889)]\n",
            "2025-12-23 01:40:16.558601: Epoch time: 22.42 s\n",
            "2025-12-23 01:40:18.614629: \n",
            "2025-12-23 01:40:18.614978: Epoch 589\n",
            "2025-12-23 01:40:18.615125: Current learning rate: 0.00449\n",
            "2025-12-23 01:40:41.074935: train_loss -0.9727\n",
            "2025-12-23 01:40:41.075178: val_loss -0.9865\n",
            "2025-12-23 01:40:41.075311: Pseudo dice [np.float32(0.9894)]\n",
            "2025-12-23 01:40:41.075414: Epoch time: 22.46 s\n",
            "2025-12-23 01:40:42.390512: \n",
            "2025-12-23 01:40:42.390822: Epoch 590\n",
            "2025-12-23 01:40:42.390972: Current learning rate: 0.00448\n",
            "2025-12-23 01:41:04.880034: train_loss -0.9718\n",
            "2025-12-23 01:41:04.880341: val_loss -0.9847\n",
            "2025-12-23 01:41:04.880489: Pseudo dice [np.float32(0.9879)]\n",
            "2025-12-23 01:41:04.880648: Epoch time: 22.49 s\n",
            "2025-12-23 01:41:06.203393: \n",
            "2025-12-23 01:41:06.203756: Epoch 591\n",
            "2025-12-23 01:41:06.203892: Current learning rate: 0.00447\n",
            "2025-12-23 01:41:28.693022: train_loss -0.9727\n",
            "2025-12-23 01:41:28.693300: val_loss -0.9854\n",
            "2025-12-23 01:41:28.693404: Pseudo dice [np.float32(0.9893)]\n",
            "2025-12-23 01:41:28.693497: Epoch time: 22.49 s\n",
            "2025-12-23 01:41:30.043779: \n",
            "2025-12-23 01:41:30.044082: Epoch 592\n",
            "2025-12-23 01:41:30.044209: Current learning rate: 0.00446\n",
            "2025-12-23 01:41:52.521087: train_loss -0.9721\n",
            "2025-12-23 01:41:52.521475: val_loss -0.986\n",
            "2025-12-23 01:41:52.521577: Pseudo dice [np.float32(0.9893)]\n",
            "2025-12-23 01:41:52.521678: Epoch time: 22.48 s\n",
            "2025-12-23 01:41:53.876133: \n",
            "2025-12-23 01:41:53.876464: Epoch 593\n",
            "2025-12-23 01:41:53.876602: Current learning rate: 0.00445\n",
            "2025-12-23 01:42:16.349586: train_loss -0.9721\n",
            "2025-12-23 01:42:16.349888: val_loss -0.9844\n",
            "2025-12-23 01:42:16.350025: Pseudo dice [np.float32(0.9874)]\n",
            "2025-12-23 01:42:16.350179: Epoch time: 22.47 s\n",
            "2025-12-23 01:42:17.726490: \n",
            "2025-12-23 01:42:17.726892: Epoch 594\n",
            "2025-12-23 01:42:17.727026: Current learning rate: 0.00444\n",
            "2025-12-23 01:42:40.249960: train_loss -0.9728\n",
            "2025-12-23 01:42:40.250182: val_loss -0.9849\n",
            "2025-12-23 01:42:40.250331: Pseudo dice [np.float32(0.9891)]\n",
            "2025-12-23 01:42:40.250445: Epoch time: 22.52 s\n",
            "2025-12-23 01:42:41.634212: \n",
            "2025-12-23 01:42:41.634562: Epoch 595\n",
            "2025-12-23 01:42:41.634721: Current learning rate: 0.00443\n",
            "2025-12-23 01:43:04.140003: train_loss -0.9728\n",
            "2025-12-23 01:43:04.140327: val_loss -0.9855\n",
            "2025-12-23 01:43:04.140532: Pseudo dice [np.float32(0.9887)]\n",
            "2025-12-23 01:43:04.140709: Epoch time: 22.51 s\n",
            "2025-12-23 01:43:05.544549: \n",
            "2025-12-23 01:43:05.544833: Epoch 596\n",
            "2025-12-23 01:43:05.544970: Current learning rate: 0.00442\n",
            "2025-12-23 01:43:28.018785: train_loss -0.9708\n",
            "2025-12-23 01:43:28.019237: val_loss -0.9838\n",
            "2025-12-23 01:43:28.019404: Pseudo dice [np.float32(0.9878)]\n",
            "2025-12-23 01:43:28.019500: Epoch time: 22.48 s\n",
            "2025-12-23 01:43:29.406582: \n",
            "2025-12-23 01:43:29.406931: Epoch 597\n",
            "2025-12-23 01:43:29.407072: Current learning rate: 0.00441\n",
            "2025-12-23 01:43:51.828846: train_loss -0.964\n",
            "2025-12-23 01:43:51.829298: val_loss -0.9833\n",
            "2025-12-23 01:43:51.829410: Pseudo dice [np.float32(0.987)]\n",
            "2025-12-23 01:43:51.829512: Epoch time: 22.42 s\n",
            "2025-12-23 01:43:53.234855: \n",
            "2025-12-23 01:43:53.235146: Epoch 598\n",
            "2025-12-23 01:43:53.235295: Current learning rate: 0.0044\n",
            "2025-12-23 01:44:15.675902: train_loss -0.9683\n",
            "2025-12-23 01:44:15.676272: val_loss -0.982\n",
            "2025-12-23 01:44:15.676512: Pseudo dice [np.float32(0.9869)]\n",
            "2025-12-23 01:44:15.676740: Epoch time: 22.44 s\n",
            "2025-12-23 01:44:17.041240: \n",
            "2025-12-23 01:44:17.041589: Epoch 599\n",
            "2025-12-23 01:44:17.041770: Current learning rate: 0.00439\n",
            "2025-12-23 01:44:39.525596: train_loss -0.9679\n",
            "2025-12-23 01:44:39.525817: val_loss -0.9818\n",
            "2025-12-23 01:44:39.525908: Pseudo dice [np.float32(0.9871)]\n",
            "2025-12-23 01:44:39.526005: Epoch time: 22.49 s\n",
            "2025-12-23 01:44:41.497549: \n",
            "2025-12-23 01:44:41.497764: Epoch 600\n",
            "2025-12-23 01:44:41.497901: Current learning rate: 0.00438\n",
            "2025-12-23 01:45:03.967175: train_loss -0.9705\n",
            "2025-12-23 01:45:03.967541: val_loss -0.9853\n",
            "2025-12-23 01:45:03.967767: Pseudo dice [np.float32(0.9886)]\n",
            "2025-12-23 01:45:03.967883: Epoch time: 22.47 s\n",
            "2025-12-23 01:45:05.337092: \n",
            "2025-12-23 01:45:05.337358: Epoch 601\n",
            "2025-12-23 01:45:05.337497: Current learning rate: 0.00437\n",
            "2025-12-23 01:45:27.846764: train_loss -0.9714\n",
            "2025-12-23 01:45:27.846972: val_loss -0.9824\n",
            "2025-12-23 01:45:27.847087: Pseudo dice [np.float32(0.9869)]\n",
            "2025-12-23 01:45:27.847276: Epoch time: 22.51 s\n",
            "2025-12-23 01:45:29.236196: \n",
            "2025-12-23 01:45:29.236491: Epoch 602\n",
            "2025-12-23 01:45:29.236630: Current learning rate: 0.00436\n",
            "2025-12-23 01:45:51.628627: train_loss -0.9716\n",
            "2025-12-23 01:45:51.628837: val_loss -0.9833\n",
            "2025-12-23 01:45:51.629027: Pseudo dice [np.float32(0.9878)]\n",
            "2025-12-23 01:45:51.629125: Epoch time: 22.39 s\n",
            "2025-12-23 01:45:53.027986: \n",
            "2025-12-23 01:45:53.028350: Epoch 603\n",
            "2025-12-23 01:45:53.028487: Current learning rate: 0.00435\n",
            "2025-12-23 01:46:15.426068: train_loss -0.9709\n",
            "2025-12-23 01:46:15.426352: val_loss -0.9837\n",
            "2025-12-23 01:46:15.426467: Pseudo dice [np.float32(0.9882)]\n",
            "2025-12-23 01:46:15.426588: Epoch time: 22.4 s\n",
            "2025-12-23 01:46:16.818299: \n",
            "2025-12-23 01:46:16.818682: Epoch 604\n",
            "2025-12-23 01:46:16.818825: Current learning rate: 0.00434\n",
            "2025-12-23 01:46:39.238896: train_loss -0.9712\n",
            "2025-12-23 01:46:39.239112: val_loss -0.9851\n",
            "2025-12-23 01:46:39.239201: Pseudo dice [np.float32(0.9885)]\n",
            "2025-12-23 01:46:39.239335: Epoch time: 22.42 s\n",
            "2025-12-23 01:46:40.623096: \n",
            "2025-12-23 01:46:40.623405: Epoch 605\n",
            "2025-12-23 01:46:40.623569: Current learning rate: 0.00433\n",
            "2025-12-23 01:47:03.047276: train_loss -0.9719\n",
            "2025-12-23 01:47:03.047549: val_loss -0.9849\n",
            "2025-12-23 01:47:03.047715: Pseudo dice [np.float32(0.9889)]\n",
            "2025-12-23 01:47:03.047945: Epoch time: 22.43 s\n",
            "2025-12-23 01:47:04.422242: \n",
            "2025-12-23 01:47:04.422580: Epoch 606\n",
            "2025-12-23 01:47:04.422740: Current learning rate: 0.00432\n",
            "2025-12-23 01:47:26.831529: train_loss -0.9713\n",
            "2025-12-23 01:47:26.831742: val_loss -0.9856\n",
            "2025-12-23 01:47:26.831836: Pseudo dice [np.float32(0.9894)]\n",
            "2025-12-23 01:47:26.831935: Epoch time: 22.41 s\n",
            "2025-12-23 01:47:28.888546: \n",
            "2025-12-23 01:47:28.888836: Epoch 607\n",
            "2025-12-23 01:47:28.889071: Current learning rate: 0.00431\n",
            "2025-12-23 01:47:51.424799: train_loss -0.9733\n",
            "2025-12-23 01:47:51.425156: val_loss -0.9852\n",
            "2025-12-23 01:47:51.425330: Pseudo dice [np.float32(0.9886)]\n",
            "2025-12-23 01:47:51.425435: Epoch time: 22.54 s\n",
            "2025-12-23 01:47:52.779579: \n",
            "2025-12-23 01:47:52.779912: Epoch 608\n",
            "2025-12-23 01:47:52.780075: Current learning rate: 0.0043\n",
            "2025-12-23 01:48:15.269945: train_loss -0.9696\n",
            "2025-12-23 01:48:15.270255: val_loss -0.9832\n",
            "2025-12-23 01:48:15.270377: Pseudo dice [np.float32(0.9871)]\n",
            "2025-12-23 01:48:15.270528: Epoch time: 22.49 s\n",
            "2025-12-23 01:48:16.618528: \n",
            "2025-12-23 01:48:16.618885: Epoch 609\n",
            "2025-12-23 01:48:16.619021: Current learning rate: 0.00429\n",
            "2025-12-23 01:48:39.064136: train_loss -0.9664\n",
            "2025-12-23 01:48:39.064465: val_loss -0.9829\n",
            "2025-12-23 01:48:39.064593: Pseudo dice [np.float32(0.9866)]\n",
            "2025-12-23 01:48:39.064697: Epoch time: 22.45 s\n",
            "2025-12-23 01:48:40.423760: \n",
            "2025-12-23 01:48:40.424190: Epoch 610\n",
            "2025-12-23 01:48:40.424372: Current learning rate: 0.00429\n",
            "2025-12-23 01:49:02.907962: train_loss -0.9675\n",
            "2025-12-23 01:49:02.908189: val_loss -0.9824\n",
            "2025-12-23 01:49:02.908334: Pseudo dice [np.float32(0.9868)]\n",
            "2025-12-23 01:49:02.908433: Epoch time: 22.49 s\n",
            "2025-12-23 01:49:04.251503: \n",
            "2025-12-23 01:49:04.251806: Epoch 611\n",
            "2025-12-23 01:49:04.251948: Current learning rate: 0.00428\n",
            "2025-12-23 01:49:26.742698: train_loss -0.971\n",
            "2025-12-23 01:49:26.742988: val_loss -0.9842\n",
            "2025-12-23 01:49:26.743138: Pseudo dice [np.float32(0.9887)]\n",
            "2025-12-23 01:49:26.743312: Epoch time: 22.49 s\n",
            "2025-12-23 01:49:28.106301: \n",
            "2025-12-23 01:49:28.106518: Epoch 612\n",
            "2025-12-23 01:49:28.106644: Current learning rate: 0.00427\n",
            "2025-12-23 01:49:50.535520: train_loss -0.9727\n",
            "2025-12-23 01:49:50.535893: val_loss -0.9857\n",
            "2025-12-23 01:49:50.536052: Pseudo dice [np.float32(0.9887)]\n",
            "2025-12-23 01:49:50.536198: Epoch time: 22.43 s\n",
            "2025-12-23 01:49:51.907109: \n",
            "2025-12-23 01:49:51.907419: Epoch 613\n",
            "2025-12-23 01:49:51.907578: Current learning rate: 0.00426\n",
            "2025-12-23 01:50:14.356940: train_loss -0.9719\n",
            "2025-12-23 01:50:14.357242: val_loss -0.9829\n",
            "2025-12-23 01:50:14.357429: Pseudo dice [np.float32(0.9872)]\n",
            "2025-12-23 01:50:14.357543: Epoch time: 22.45 s\n",
            "2025-12-23 01:50:15.745936: \n",
            "2025-12-23 01:50:15.746224: Epoch 614\n",
            "2025-12-23 01:50:15.746360: Current learning rate: 0.00425\n",
            "2025-12-23 01:50:38.297410: train_loss -0.9709\n",
            "2025-12-23 01:50:38.297678: val_loss -0.9848\n",
            "2025-12-23 01:50:38.297871: Pseudo dice [np.float32(0.9885)]\n",
            "2025-12-23 01:50:38.298045: Epoch time: 22.55 s\n",
            "2025-12-23 01:50:39.676837: \n",
            "2025-12-23 01:50:39.677043: Epoch 615\n",
            "2025-12-23 01:50:39.677172: Current learning rate: 0.00424\n",
            "2025-12-23 01:51:02.152291: train_loss -0.9681\n",
            "2025-12-23 01:51:02.152484: val_loss -0.9834\n",
            "2025-12-23 01:51:02.152580: Pseudo dice [np.float32(0.9876)]\n",
            "2025-12-23 01:51:02.152701: Epoch time: 22.48 s\n",
            "2025-12-23 01:51:03.530153: \n",
            "2025-12-23 01:51:03.530477: Epoch 616\n",
            "2025-12-23 01:51:03.530617: Current learning rate: 0.00423\n",
            "2025-12-23 01:51:25.988307: train_loss -0.9692\n",
            "2025-12-23 01:51:25.988634: val_loss -0.9846\n",
            "2025-12-23 01:51:25.988782: Pseudo dice [np.float32(0.9887)]\n",
            "2025-12-23 01:51:25.988902: Epoch time: 22.46 s\n",
            "2025-12-23 01:51:27.362635: \n",
            "2025-12-23 01:51:27.362947: Epoch 617\n",
            "2025-12-23 01:51:27.363080: Current learning rate: 0.00422\n",
            "2025-12-23 01:51:49.825190: train_loss -0.9701\n",
            "2025-12-23 01:51:49.825524: val_loss -0.9843\n",
            "2025-12-23 01:51:49.825663: Pseudo dice [np.float32(0.9882)]\n",
            "2025-12-23 01:51:49.825772: Epoch time: 22.46 s\n",
            "2025-12-23 01:51:51.197952: \n",
            "2025-12-23 01:51:51.198253: Epoch 618\n",
            "2025-12-23 01:51:51.198405: Current learning rate: 0.00421\n",
            "2025-12-23 01:52:13.719856: train_loss -0.9716\n",
            "2025-12-23 01:52:13.720140: val_loss -0.9854\n",
            "2025-12-23 01:52:13.720403: Pseudo dice [np.float32(0.9884)]\n",
            "2025-12-23 01:52:13.720654: Epoch time: 22.52 s\n",
            "2025-12-23 01:52:15.074941: \n",
            "2025-12-23 01:52:15.075133: Epoch 619\n",
            "2025-12-23 01:52:15.075278: Current learning rate: 0.0042\n",
            "2025-12-23 01:52:37.476943: train_loss -0.9715\n",
            "2025-12-23 01:52:37.477175: val_loss -0.9841\n",
            "2025-12-23 01:52:37.477308: Pseudo dice [np.float32(0.9867)]\n",
            "2025-12-23 01:52:37.477410: Epoch time: 22.4 s\n",
            "2025-12-23 01:52:38.811364: \n",
            "2025-12-23 01:52:38.811623: Epoch 620\n",
            "2025-12-23 01:52:38.811772: Current learning rate: 0.00419\n",
            "2025-12-23 01:53:01.238130: train_loss -0.9725\n",
            "2025-12-23 01:53:01.238383: val_loss -0.9861\n",
            "2025-12-23 01:53:01.238561: Pseudo dice [np.float32(0.9895)]\n",
            "2025-12-23 01:53:01.238704: Epoch time: 22.43 s\n",
            "2025-12-23 01:53:02.584250: \n",
            "2025-12-23 01:53:02.584523: Epoch 621\n",
            "2025-12-23 01:53:02.584654: Current learning rate: 0.00418\n",
            "2025-12-23 01:53:25.008799: train_loss -0.9722\n",
            "2025-12-23 01:53:25.009139: val_loss -0.9852\n",
            "2025-12-23 01:53:25.009262: Pseudo dice [np.float32(0.989)]\n",
            "2025-12-23 01:53:25.009391: Epoch time: 22.43 s\n",
            "2025-12-23 01:53:26.379031: \n",
            "2025-12-23 01:53:26.379308: Epoch 622\n",
            "2025-12-23 01:53:26.379462: Current learning rate: 0.00417\n",
            "2025-12-23 01:53:48.817427: train_loss -0.972\n",
            "2025-12-23 01:53:48.817643: val_loss -0.9838\n",
            "2025-12-23 01:53:48.817735: Pseudo dice [np.float32(0.988)]\n",
            "2025-12-23 01:53:48.817828: Epoch time: 22.44 s\n",
            "2025-12-23 01:53:50.181437: \n",
            "2025-12-23 01:53:50.181766: Epoch 623\n",
            "2025-12-23 01:53:50.181900: Current learning rate: 0.00416\n",
            "2025-12-23 01:54:12.575852: train_loss -0.9712\n",
            "2025-12-23 01:54:12.576098: val_loss -0.9859\n",
            "2025-12-23 01:54:12.576188: Pseudo dice [np.float32(0.9891)]\n",
            "2025-12-23 01:54:12.576317: Epoch time: 22.4 s\n",
            "2025-12-23 01:54:13.939174: \n",
            "2025-12-23 01:54:13.939405: Epoch 624\n",
            "2025-12-23 01:54:13.939538: Current learning rate: 0.00415\n",
            "2025-12-23 01:54:36.375305: train_loss -0.9718\n",
            "2025-12-23 01:54:36.375536: val_loss -0.9856\n",
            "2025-12-23 01:54:36.375637: Pseudo dice [np.float32(0.9893)]\n",
            "2025-12-23 01:54:36.375725: Epoch time: 22.44 s\n",
            "2025-12-23 01:54:38.511847: \n",
            "2025-12-23 01:54:38.512067: Epoch 625\n",
            "2025-12-23 01:54:38.512253: Current learning rate: 0.00414\n",
            "2025-12-23 01:55:00.966482: train_loss -0.9708\n",
            "2025-12-23 01:55:00.966685: val_loss -0.9854\n",
            "2025-12-23 01:55:00.966775: Pseudo dice [np.float32(0.9884)]\n",
            "2025-12-23 01:55:00.966863: Epoch time: 22.46 s\n",
            "2025-12-23 01:55:02.308492: \n",
            "2025-12-23 01:55:02.308843: Epoch 626\n",
            "2025-12-23 01:55:02.308982: Current learning rate: 0.00413\n",
            "2025-12-23 01:55:24.813638: train_loss -0.9723\n",
            "2025-12-23 01:55:24.813962: val_loss -0.9855\n",
            "2025-12-23 01:55:24.814068: Pseudo dice [np.float32(0.9893)]\n",
            "2025-12-23 01:55:24.814159: Epoch time: 22.51 s\n",
            "2025-12-23 01:55:26.183863: \n",
            "2025-12-23 01:55:26.184152: Epoch 627\n",
            "2025-12-23 01:55:26.184306: Current learning rate: 0.00412\n",
            "2025-12-23 01:55:48.653742: train_loss -0.9745\n",
            "2025-12-23 01:55:48.653960: val_loss -0.986\n",
            "2025-12-23 01:55:48.654048: Pseudo dice [np.float32(0.9897)]\n",
            "2025-12-23 01:55:48.654138: Epoch time: 22.47 s\n",
            "2025-12-23 01:55:50.005687: \n",
            "2025-12-23 01:55:50.006026: Epoch 628\n",
            "2025-12-23 01:55:50.006159: Current learning rate: 0.00411\n",
            "2025-12-23 01:56:12.495409: train_loss -0.9732\n",
            "2025-12-23 01:56:12.495695: val_loss -0.9851\n",
            "2025-12-23 01:56:12.495789: Pseudo dice [np.float32(0.989)]\n",
            "2025-12-23 01:56:12.495883: Epoch time: 22.49 s\n",
            "2025-12-23 01:56:13.829520: \n",
            "2025-12-23 01:56:13.829891: Epoch 629\n",
            "2025-12-23 01:56:13.830034: Current learning rate: 0.0041\n",
            "2025-12-23 01:56:36.333244: train_loss -0.9734\n",
            "2025-12-23 01:56:36.333461: val_loss -0.9852\n",
            "2025-12-23 01:56:36.333559: Pseudo dice [np.float32(0.9895)]\n",
            "2025-12-23 01:56:36.333653: Epoch time: 22.5 s\n",
            "2025-12-23 01:56:37.700494: \n",
            "2025-12-23 01:56:37.700893: Epoch 630\n",
            "2025-12-23 01:56:37.701021: Current learning rate: 0.00409\n",
            "2025-12-23 01:57:00.205517: train_loss -0.974\n",
            "2025-12-23 01:57:00.205746: val_loss -0.9829\n",
            "2025-12-23 01:57:00.205868: Pseudo dice [np.float32(0.9875)]\n",
            "2025-12-23 01:57:00.206040: Epoch time: 22.51 s\n",
            "2025-12-23 01:57:01.586202: \n",
            "2025-12-23 01:57:01.586458: Epoch 631\n",
            "2025-12-23 01:57:01.586592: Current learning rate: 0.00408\n",
            "2025-12-23 01:57:24.048990: train_loss -0.9729\n",
            "2025-12-23 01:57:24.049228: val_loss -0.9848\n",
            "2025-12-23 01:57:24.049352: Pseudo dice [np.float32(0.9888)]\n",
            "2025-12-23 01:57:24.049448: Epoch time: 22.46 s\n",
            "2025-12-23 01:57:25.413767: \n",
            "2025-12-23 01:57:25.414028: Epoch 632\n",
            "2025-12-23 01:57:25.414160: Current learning rate: 0.00407\n",
            "2025-12-23 01:57:47.872342: train_loss -0.9725\n",
            "2025-12-23 01:57:47.872617: val_loss -0.9853\n",
            "2025-12-23 01:57:47.872715: Pseudo dice [np.float32(0.9894)]\n",
            "2025-12-23 01:57:47.872889: Epoch time: 22.46 s\n",
            "2025-12-23 01:57:49.278250: \n",
            "2025-12-23 01:57:49.278576: Epoch 633\n",
            "2025-12-23 01:57:49.278726: Current learning rate: 0.00406\n",
            "2025-12-23 01:58:11.764573: train_loss -0.9735\n",
            "2025-12-23 01:58:11.764795: val_loss -0.9844\n",
            "2025-12-23 01:58:11.764929: Pseudo dice [np.float32(0.9886)]\n",
            "2025-12-23 01:58:11.765056: Epoch time: 22.49 s\n",
            "2025-12-23 01:58:13.137657: \n",
            "2025-12-23 01:58:13.138047: Epoch 634\n",
            "2025-12-23 01:58:13.138185: Current learning rate: 0.00405\n",
            "2025-12-23 01:58:35.572368: train_loss -0.9733\n",
            "2025-12-23 01:58:35.572782: val_loss -0.9842\n",
            "2025-12-23 01:58:35.572876: Pseudo dice [np.float32(0.9882)]\n",
            "2025-12-23 01:58:35.572971: Epoch time: 22.44 s\n",
            "2025-12-23 01:58:36.954328: \n",
            "2025-12-23 01:58:36.954504: Epoch 635\n",
            "2025-12-23 01:58:36.954629: Current learning rate: 0.00404\n",
            "2025-12-23 01:58:59.371776: train_loss -0.9732\n",
            "2025-12-23 01:58:59.372046: val_loss -0.9848\n",
            "2025-12-23 01:58:59.372195: Pseudo dice [np.float32(0.9885)]\n",
            "2025-12-23 01:58:59.372324: Epoch time: 22.42 s\n",
            "2025-12-23 01:59:00.763590: \n",
            "2025-12-23 01:59:00.763971: Epoch 636\n",
            "2025-12-23 01:59:00.764128: Current learning rate: 0.00403\n",
            "2025-12-23 01:59:23.216726: train_loss -0.973\n",
            "2025-12-23 01:59:23.217138: val_loss -0.9856\n",
            "2025-12-23 01:59:23.217271: Pseudo dice [np.float32(0.9895)]\n",
            "2025-12-23 01:59:23.217387: Epoch time: 22.45 s\n",
            "2025-12-23 01:59:24.600272: \n",
            "2025-12-23 01:59:24.600654: Epoch 637\n",
            "2025-12-23 01:59:24.600793: Current learning rate: 0.00402\n",
            "2025-12-23 01:59:47.019590: train_loss -0.9743\n",
            "2025-12-23 01:59:47.019909: val_loss -0.9863\n",
            "2025-12-23 01:59:47.020007: Pseudo dice [np.float32(0.9896)]\n",
            "2025-12-23 01:59:47.020098: Epoch time: 22.42 s\n",
            "2025-12-23 01:59:48.383255: \n",
            "2025-12-23 01:59:48.383578: Epoch 638\n",
            "2025-12-23 01:59:48.383790: Current learning rate: 0.00401\n",
            "2025-12-23 02:00:10.811555: train_loss -0.974\n",
            "2025-12-23 02:00:10.811833: val_loss -0.9856\n",
            "2025-12-23 02:00:10.811960: Pseudo dice [np.float32(0.9881)]\n",
            "2025-12-23 02:00:10.812085: Epoch time: 22.43 s\n",
            "2025-12-23 02:00:12.205936: \n",
            "2025-12-23 02:00:12.206200: Epoch 639\n",
            "2025-12-23 02:00:12.206436: Current learning rate: 0.004\n",
            "2025-12-23 02:00:34.693147: train_loss -0.9736\n",
            "2025-12-23 02:00:34.693424: val_loss -0.9862\n",
            "2025-12-23 02:00:34.693520: Pseudo dice [np.float32(0.9888)]\n",
            "2025-12-23 02:00:34.693612: Epoch time: 22.49 s\n",
            "2025-12-23 02:00:36.059637: \n",
            "2025-12-23 02:00:36.060026: Epoch 640\n",
            "2025-12-23 02:00:36.060260: Current learning rate: 0.00399\n",
            "2025-12-23 02:00:58.451933: train_loss -0.9739\n",
            "2025-12-23 02:00:58.452176: val_loss -0.9847\n",
            "2025-12-23 02:00:58.452325: Pseudo dice [np.float32(0.9887)]\n",
            "2025-12-23 02:00:58.452444: Epoch time: 22.39 s\n",
            "2025-12-23 02:00:59.826745: \n",
            "2025-12-23 02:00:59.827081: Epoch 641\n",
            "2025-12-23 02:00:59.827313: Current learning rate: 0.00398\n",
            "2025-12-23 02:01:22.217991: train_loss -0.9734\n",
            "2025-12-23 02:01:22.218236: val_loss -0.9852\n",
            "2025-12-23 02:01:22.218345: Pseudo dice [np.float32(0.9888)]\n",
            "2025-12-23 02:01:22.218439: Epoch time: 22.39 s\n",
            "2025-12-23 02:01:23.594374: \n",
            "2025-12-23 02:01:23.594642: Epoch 642\n",
            "2025-12-23 02:01:23.594779: Current learning rate: 0.00397\n",
            "2025-12-23 02:01:45.975037: train_loss -0.9731\n",
            "2025-12-23 02:01:45.975278: val_loss -0.9852\n",
            "2025-12-23 02:01:45.975372: Pseudo dice [np.float32(0.9891)]\n",
            "2025-12-23 02:01:45.975461: Epoch time: 22.38 s\n",
            "2025-12-23 02:01:47.999341: \n",
            "2025-12-23 02:01:47.999573: Epoch 643\n",
            "2025-12-23 02:01:47.999732: Current learning rate: 0.00396\n",
            "2025-12-23 02:02:10.525452: train_loss -0.9726\n",
            "2025-12-23 02:02:10.525836: val_loss -0.9839\n",
            "2025-12-23 02:02:10.526063: Pseudo dice [np.float32(0.9886)]\n",
            "2025-12-23 02:02:10.526272: Epoch time: 22.53 s\n",
            "2025-12-23 02:02:11.872152: \n",
            "2025-12-23 02:02:11.872441: Epoch 644\n",
            "2025-12-23 02:02:11.872606: Current learning rate: 0.00395\n",
            "2025-12-23 02:02:34.326790: train_loss -0.9733\n",
            "2025-12-23 02:02:34.327045: val_loss -0.9847\n",
            "2025-12-23 02:02:34.327137: Pseudo dice [np.float32(0.9883)]\n",
            "2025-12-23 02:02:34.327251: Epoch time: 22.46 s\n",
            "2025-12-23 02:02:35.667689: \n",
            "2025-12-23 02:02:35.668025: Epoch 645\n",
            "2025-12-23 02:02:35.668183: Current learning rate: 0.00394\n",
            "2025-12-23 02:02:58.149846: train_loss -0.9733\n",
            "2025-12-23 02:02:58.150061: val_loss -0.9844\n",
            "2025-12-23 02:02:58.150150: Pseudo dice [np.float32(0.988)]\n",
            "2025-12-23 02:02:58.150260: Epoch time: 22.48 s\n",
            "2025-12-23 02:02:59.484617: \n",
            "2025-12-23 02:02:59.484900: Epoch 646\n",
            "2025-12-23 02:02:59.485044: Current learning rate: 0.00393\n",
            "2025-12-23 02:03:21.958022: train_loss -0.9732\n",
            "2025-12-23 02:03:21.958398: val_loss -0.9854\n",
            "2025-12-23 02:03:21.958608: Pseudo dice [np.float32(0.9888)]\n",
            "2025-12-23 02:03:21.958753: Epoch time: 22.47 s\n",
            "2025-12-23 02:03:23.346910: \n",
            "2025-12-23 02:03:23.347144: Epoch 647\n",
            "2025-12-23 02:03:23.347410: Current learning rate: 0.00392\n",
            "2025-12-23 02:03:45.808619: train_loss -0.9742\n",
            "2025-12-23 02:03:45.808831: val_loss -0.985\n",
            "2025-12-23 02:03:45.808927: Pseudo dice [np.float32(0.9891)]\n",
            "2025-12-23 02:03:45.809021: Epoch time: 22.46 s\n",
            "2025-12-23 02:03:47.156277: \n",
            "2025-12-23 02:03:47.156557: Epoch 648\n",
            "2025-12-23 02:03:47.156682: Current learning rate: 0.00391\n",
            "2025-12-23 02:04:09.632730: train_loss -0.9734\n",
            "2025-12-23 02:04:09.632943: val_loss -0.9858\n",
            "2025-12-23 02:04:09.633033: Pseudo dice [np.float32(0.9893)]\n",
            "2025-12-23 02:04:09.633122: Epoch time: 22.48 s\n",
            "2025-12-23 02:04:11.012964: \n",
            "2025-12-23 02:04:11.013156: Epoch 649\n",
            "2025-12-23 02:04:11.013298: Current learning rate: 0.0039\n",
            "2025-12-23 02:04:33.527843: train_loss -0.9742\n",
            "2025-12-23 02:04:33.528072: val_loss -0.9861\n",
            "2025-12-23 02:04:33.528169: Pseudo dice [np.float32(0.9896)]\n",
            "2025-12-23 02:04:33.528318: Epoch time: 22.52 s\n",
            "2025-12-23 02:04:34.138052: Yayy! New best EMA pseudo Dice: 0.9889000058174133\n",
            "2025-12-23 02:04:36.077783: \n",
            "2025-12-23 02:04:36.078094: Epoch 650\n",
            "2025-12-23 02:04:36.078249: Current learning rate: 0.00389\n",
            "2025-12-23 02:04:58.570446: train_loss -0.9737\n",
            "2025-12-23 02:04:58.570724: val_loss -0.9845\n",
            "2025-12-23 02:04:58.570898: Pseudo dice [np.float32(0.9879)]\n",
            "2025-12-23 02:04:58.571032: Epoch time: 22.49 s\n",
            "2025-12-23 02:04:59.947553: \n",
            "2025-12-23 02:04:59.947739: Epoch 651\n",
            "2025-12-23 02:04:59.947863: Current learning rate: 0.00388\n",
            "2025-12-23 02:05:22.636445: train_loss -0.9732\n",
            "2025-12-23 02:05:22.636694: val_loss -0.987\n",
            "2025-12-23 02:05:22.636786: Pseudo dice [np.float32(0.9902)]\n",
            "2025-12-23 02:05:22.636904: Epoch time: 22.69 s\n",
            "2025-12-23 02:05:22.637078: Yayy! New best EMA pseudo Dice: 0.9889000058174133\n",
            "2025-12-23 02:05:24.537808: \n",
            "2025-12-23 02:05:24.538084: Epoch 652\n",
            "2025-12-23 02:05:24.538230: Current learning rate: 0.00387\n",
            "2025-12-23 02:05:47.021600: train_loss -0.9731\n",
            "2025-12-23 02:05:47.021832: val_loss -0.9846\n",
            "2025-12-23 02:05:47.021928: Pseudo dice [np.float32(0.9881)]\n",
            "2025-12-23 02:05:47.022018: Epoch time: 22.49 s\n",
            "2025-12-23 02:05:48.405005: \n",
            "2025-12-23 02:05:48.405385: Epoch 653\n",
            "2025-12-23 02:05:48.405542: Current learning rate: 0.00386\n",
            "2025-12-23 02:06:10.826935: train_loss -0.9563\n",
            "2025-12-23 02:06:10.827150: val_loss -0.972\n",
            "2025-12-23 02:06:10.827264: Pseudo dice [np.float32(0.9806)]\n",
            "2025-12-23 02:06:10.827373: Epoch time: 22.42 s\n",
            "2025-12-23 02:06:12.195734: \n",
            "2025-12-23 02:06:12.196160: Epoch 654\n",
            "2025-12-23 02:06:12.196341: Current learning rate: 0.00385\n",
            "2025-12-23 02:06:34.627424: train_loss -0.9561\n",
            "2025-12-23 02:06:34.627644: val_loss -0.9782\n",
            "2025-12-23 02:06:34.627823: Pseudo dice [np.float32(0.9833)]\n",
            "2025-12-23 02:06:34.627937: Epoch time: 22.43 s\n",
            "2025-12-23 02:06:35.986980: \n",
            "2025-12-23 02:06:35.987247: Epoch 655\n",
            "2025-12-23 02:06:35.987382: Current learning rate: 0.00384\n",
            "2025-12-23 02:06:58.437638: train_loss -0.9655\n",
            "2025-12-23 02:06:58.437891: val_loss -0.9817\n",
            "2025-12-23 02:06:58.438002: Pseudo dice [np.float32(0.9867)]\n",
            "2025-12-23 02:06:58.438132: Epoch time: 22.45 s\n",
            "2025-12-23 02:06:59.784949: \n",
            "2025-12-23 02:06:59.785236: Epoch 656\n",
            "2025-12-23 02:06:59.785390: Current learning rate: 0.00383\n",
            "2025-12-23 02:07:22.239441: train_loss -0.9663\n",
            "2025-12-23 02:07:22.239730: val_loss -0.9842\n",
            "2025-12-23 02:07:22.239835: Pseudo dice [np.float32(0.9883)]\n",
            "2025-12-23 02:07:22.239996: Epoch time: 22.46 s\n",
            "2025-12-23 02:07:23.588073: \n",
            "2025-12-23 02:07:23.588307: Epoch 657\n",
            "2025-12-23 02:07:23.588475: Current learning rate: 0.00382\n",
            "2025-12-23 02:07:46.015789: train_loss -0.969\n",
            "2025-12-23 02:07:46.016098: val_loss -0.9808\n",
            "2025-12-23 02:07:46.016203: Pseudo dice [np.float32(0.9851)]\n",
            "2025-12-23 02:07:46.016330: Epoch time: 22.43 s\n",
            "2025-12-23 02:07:47.352426: \n",
            "2025-12-23 02:07:47.352737: Epoch 658\n",
            "2025-12-23 02:07:47.352873: Current learning rate: 0.00381\n",
            "2025-12-23 02:08:09.801172: train_loss -0.9703\n",
            "2025-12-23 02:08:09.801510: val_loss -0.9841\n",
            "2025-12-23 02:08:09.801609: Pseudo dice [np.float32(0.9883)]\n",
            "2025-12-23 02:08:09.801701: Epoch time: 22.45 s\n",
            "2025-12-23 02:08:11.170684: \n",
            "2025-12-23 02:08:11.170971: Epoch 659\n",
            "2025-12-23 02:08:11.171107: Current learning rate: 0.0038\n",
            "2025-12-23 02:08:33.611794: train_loss -0.9713\n",
            "2025-12-23 02:08:33.611999: val_loss -0.9838\n",
            "2025-12-23 02:08:33.612087: Pseudo dice [np.float32(0.9874)]\n",
            "2025-12-23 02:08:33.612176: Epoch time: 22.44 s\n",
            "2025-12-23 02:08:35.669577: \n",
            "2025-12-23 02:08:35.669796: Epoch 660\n",
            "2025-12-23 02:08:35.669987: Current learning rate: 0.00379\n",
            "2025-12-23 02:08:58.229110: train_loss -0.9723\n",
            "2025-12-23 02:08:58.229372: val_loss -0.985\n",
            "2025-12-23 02:08:58.229488: Pseudo dice [np.float32(0.9882)]\n",
            "2025-12-23 02:08:58.229666: Epoch time: 22.56 s\n",
            "2025-12-23 02:08:59.568300: \n",
            "2025-12-23 02:08:59.568662: Epoch 661\n",
            "2025-12-23 02:08:59.568797: Current learning rate: 0.00378\n",
            "2025-12-23 02:09:22.040657: train_loss -0.9732\n",
            "2025-12-23 02:09:22.041042: val_loss -0.9867\n",
            "2025-12-23 02:09:22.041197: Pseudo dice [np.float32(0.9897)]\n",
            "2025-12-23 02:09:22.041367: Epoch time: 22.47 s\n",
            "2025-12-23 02:09:23.397077: \n",
            "2025-12-23 02:09:23.397392: Epoch 662\n",
            "2025-12-23 02:09:23.397531: Current learning rate: 0.00377\n",
            "2025-12-23 02:09:45.902778: train_loss -0.973\n",
            "2025-12-23 02:09:45.903029: val_loss -0.9856\n",
            "2025-12-23 02:09:45.903117: Pseudo dice [np.float32(0.9891)]\n",
            "2025-12-23 02:09:45.903270: Epoch time: 22.51 s\n",
            "2025-12-23 02:09:47.256960: \n",
            "2025-12-23 02:09:47.257394: Epoch 663\n",
            "2025-12-23 02:09:47.257550: Current learning rate: 0.00376\n",
            "2025-12-23 02:10:09.759356: train_loss -0.971\n",
            "2025-12-23 02:10:09.759613: val_loss -0.9847\n",
            "2025-12-23 02:10:09.759718: Pseudo dice [np.float32(0.9885)]\n",
            "2025-12-23 02:10:09.759814: Epoch time: 22.5 s\n",
            "2025-12-23 02:10:11.124164: \n",
            "2025-12-23 02:10:11.124527: Epoch 664\n",
            "2025-12-23 02:10:11.124660: Current learning rate: 0.00375\n",
            "2025-12-23 02:10:33.644887: train_loss -0.9698\n",
            "2025-12-23 02:10:33.645211: val_loss -0.9845\n",
            "2025-12-23 02:10:33.645353: Pseudo dice [np.float32(0.9884)]\n",
            "2025-12-23 02:10:33.645450: Epoch time: 22.52 s\n",
            "2025-12-23 02:10:34.992016: \n",
            "2025-12-23 02:10:34.992234: Epoch 665\n",
            "2025-12-23 02:10:34.992409: Current learning rate: 0.00374\n",
            "2025-12-23 02:10:57.721824: train_loss -0.9726\n",
            "2025-12-23 02:10:57.722135: val_loss -0.9854\n",
            "2025-12-23 02:10:57.722307: Pseudo dice [np.float32(0.989)]\n",
            "2025-12-23 02:10:57.722468: Epoch time: 22.73 s\n",
            "2025-12-23 02:10:59.105418: \n",
            "2025-12-23 02:10:59.105711: Epoch 666\n",
            "2025-12-23 02:10:59.105850: Current learning rate: 0.00373\n",
            "2025-12-23 02:11:21.635409: train_loss -0.9732\n",
            "2025-12-23 02:11:21.635663: val_loss -0.9844\n",
            "2025-12-23 02:11:21.635754: Pseudo dice [np.float32(0.989)]\n",
            "2025-12-23 02:11:21.635853: Epoch time: 22.53 s\n",
            "2025-12-23 02:11:22.994527: \n",
            "2025-12-23 02:11:22.994817: Epoch 667\n",
            "2025-12-23 02:11:22.994953: Current learning rate: 0.00372\n",
            "2025-12-23 02:11:45.431002: train_loss -0.972\n",
            "2025-12-23 02:11:45.431375: val_loss -0.9858\n",
            "2025-12-23 02:11:45.431541: Pseudo dice [np.float32(0.9892)]\n",
            "2025-12-23 02:11:45.431658: Epoch time: 22.44 s\n",
            "2025-12-23 02:11:46.788013: \n",
            "2025-12-23 02:11:46.788181: Epoch 668\n",
            "2025-12-23 02:11:46.788321: Current learning rate: 0.00371\n",
            "2025-12-23 02:12:09.535026: train_loss -0.9726\n",
            "2025-12-23 02:12:09.535281: val_loss -0.9858\n",
            "2025-12-23 02:12:09.535410: Pseudo dice [np.float32(0.989)]\n",
            "2025-12-23 02:12:09.535540: Epoch time: 22.75 s\n",
            "2025-12-23 02:12:10.925240: \n",
            "2025-12-23 02:12:10.925423: Epoch 669\n",
            "2025-12-23 02:12:10.925583: Current learning rate: 0.0037\n",
            "2025-12-23 02:12:33.387971: train_loss -0.9728\n",
            "2025-12-23 02:12:33.388178: val_loss -0.9857\n",
            "2025-12-23 02:12:33.388323: Pseudo dice [np.float32(0.9888)]\n",
            "2025-12-23 02:12:33.388520: Epoch time: 22.46 s\n",
            "2025-12-23 02:12:34.784344: \n",
            "2025-12-23 02:12:34.784659: Epoch 670\n",
            "2025-12-23 02:12:34.784801: Current learning rate: 0.00369\n",
            "2025-12-23 02:12:57.483661: train_loss -0.9703\n",
            "2025-12-23 02:12:57.483898: val_loss -0.9859\n",
            "2025-12-23 02:12:57.484049: Pseudo dice [np.float32(0.9888)]\n",
            "2025-12-23 02:12:57.484207: Epoch time: 22.7 s\n",
            "2025-12-23 02:12:58.864940: \n",
            "2025-12-23 02:12:58.865177: Epoch 671\n",
            "2025-12-23 02:12:58.865323: Current learning rate: 0.00368\n",
            "2025-12-23 02:13:21.580599: train_loss -0.9683\n",
            "2025-12-23 02:13:21.581015: val_loss -0.9829\n",
            "2025-12-23 02:13:21.581162: Pseudo dice [np.float32(0.987)]\n",
            "2025-12-23 02:13:21.581303: Epoch time: 22.72 s\n",
            "2025-12-23 02:13:22.977541: \n",
            "2025-12-23 02:13:22.977763: Epoch 672\n",
            "2025-12-23 02:13:22.977928: Current learning rate: 0.00367\n",
            "2025-12-23 02:13:45.383338: train_loss -0.9671\n",
            "2025-12-23 02:13:45.383530: val_loss -0.9839\n",
            "2025-12-23 02:13:45.383620: Pseudo dice [np.float32(0.9879)]\n",
            "2025-12-23 02:13:45.383840: Epoch time: 22.41 s\n",
            "2025-12-23 02:13:46.747835: \n",
            "2025-12-23 02:13:46.748090: Epoch 673\n",
            "2025-12-23 02:13:46.748252: Current learning rate: 0.00366\n",
            "2025-12-23 02:14:09.184574: train_loss -0.9699\n",
            "2025-12-23 02:14:09.184783: val_loss -0.9846\n",
            "2025-12-23 02:14:09.184874: Pseudo dice [np.float32(0.9877)]\n",
            "2025-12-23 02:14:09.184963: Epoch time: 22.44 s\n",
            "2025-12-23 02:14:10.558472: \n",
            "2025-12-23 02:14:10.558849: Epoch 674\n",
            "2025-12-23 02:14:10.558995: Current learning rate: 0.00365\n",
            "2025-12-23 02:14:32.970033: train_loss -0.9715\n",
            "2025-12-23 02:14:32.970270: val_loss -0.9854\n",
            "2025-12-23 02:14:32.970395: Pseudo dice [np.float32(0.9891)]\n",
            "2025-12-23 02:14:32.970494: Epoch time: 22.41 s\n",
            "2025-12-23 02:14:34.377099: \n",
            "2025-12-23 02:14:34.377358: Epoch 675\n",
            "2025-12-23 02:14:34.377499: Current learning rate: 0.00364\n",
            "2025-12-23 02:14:56.767303: train_loss -0.9714\n",
            "2025-12-23 02:14:56.767509: val_loss -0.9859\n",
            "2025-12-23 02:14:56.767630: Pseudo dice [np.float32(0.989)]\n",
            "2025-12-23 02:14:56.767868: Epoch time: 22.39 s\n",
            "2025-12-23 02:14:58.201530: \n",
            "2025-12-23 02:14:58.201815: Epoch 676\n",
            "2025-12-23 02:14:58.201966: Current learning rate: 0.00363\n",
            "2025-12-23 02:15:20.625733: train_loss -0.9723\n",
            "2025-12-23 02:15:20.626029: val_loss -0.9843\n",
            "2025-12-23 02:15:20.626134: Pseudo dice [np.float32(0.9875)]\n",
            "2025-12-23 02:15:20.626256: Epoch time: 22.43 s\n",
            "2025-12-23 02:15:22.716028: \n",
            "2025-12-23 02:15:22.716356: Epoch 677\n",
            "2025-12-23 02:15:22.716537: Current learning rate: 0.00362\n",
            "2025-12-23 02:15:45.270871: train_loss -0.9725\n",
            "2025-12-23 02:15:45.271174: val_loss -0.9843\n",
            "2025-12-23 02:15:45.271326: Pseudo dice [np.float32(0.9873)]\n",
            "2025-12-23 02:15:45.271483: Epoch time: 22.56 s\n",
            "2025-12-23 02:15:46.651882: \n",
            "2025-12-23 02:15:46.652229: Epoch 678\n",
            "2025-12-23 02:15:46.652531: Current learning rate: 0.00361\n",
            "2025-12-23 02:16:09.119641: train_loss -0.9733\n",
            "2025-12-23 02:16:09.119893: val_loss -0.9837\n",
            "2025-12-23 02:16:09.120011: Pseudo dice [np.float32(0.988)]\n",
            "2025-12-23 02:16:09.120126: Epoch time: 22.47 s\n",
            "2025-12-23 02:16:10.512660: \n",
            "2025-12-23 02:16:10.512995: Epoch 679\n",
            "2025-12-23 02:16:10.513174: Current learning rate: 0.0036\n",
            "2025-12-23 02:16:32.981925: train_loss -0.9731\n",
            "2025-12-23 02:16:32.982272: val_loss -0.9859\n",
            "2025-12-23 02:16:32.982487: Pseudo dice [np.float32(0.9893)]\n",
            "2025-12-23 02:16:32.982707: Epoch time: 22.47 s\n",
            "2025-12-23 02:16:34.352830: \n",
            "2025-12-23 02:16:34.353030: Epoch 680\n",
            "2025-12-23 02:16:34.353153: Current learning rate: 0.00359\n",
            "2025-12-23 02:16:56.878435: train_loss -0.9733\n",
            "2025-12-23 02:16:56.878670: val_loss -0.9851\n",
            "2025-12-23 02:16:56.878783: Pseudo dice [np.float32(0.9886)]\n",
            "2025-12-23 02:16:56.878875: Epoch time: 22.53 s\n",
            "2025-12-23 02:16:58.283318: \n",
            "2025-12-23 02:16:58.283512: Epoch 681\n",
            "2025-12-23 02:16:58.283758: Current learning rate: 0.00358\n",
            "2025-12-23 02:17:20.748276: train_loss -0.9726\n",
            "2025-12-23 02:17:20.748709: val_loss -0.9856\n",
            "2025-12-23 02:17:20.748860: Pseudo dice [np.float32(0.9882)]\n",
            "2025-12-23 02:17:20.748994: Epoch time: 22.47 s\n",
            "2025-12-23 02:17:22.146167: \n",
            "2025-12-23 02:17:22.146559: Epoch 682\n",
            "2025-12-23 02:17:22.146704: Current learning rate: 0.00357\n",
            "2025-12-23 02:17:44.625372: train_loss -0.9732\n",
            "2025-12-23 02:17:44.625599: val_loss -0.9855\n",
            "2025-12-23 02:17:44.625783: Pseudo dice [np.float32(0.9889)]\n",
            "2025-12-23 02:17:44.625900: Epoch time: 22.48 s\n",
            "2025-12-23 02:17:46.037745: \n",
            "2025-12-23 02:17:46.038000: Epoch 683\n",
            "2025-12-23 02:17:46.038164: Current learning rate: 0.00356\n",
            "2025-12-23 02:18:08.492931: train_loss -0.9722\n",
            "2025-12-23 02:18:08.493206: val_loss -0.984\n",
            "2025-12-23 02:18:08.493327: Pseudo dice [np.float32(0.9892)]\n",
            "2025-12-23 02:18:08.493420: Epoch time: 22.46 s\n",
            "2025-12-23 02:18:09.916678: \n",
            "2025-12-23 02:18:09.917008: Epoch 684\n",
            "2025-12-23 02:18:09.917154: Current learning rate: 0.00355\n",
            "2025-12-23 02:18:32.364745: train_loss -0.9729\n",
            "2025-12-23 02:18:32.365254: val_loss -0.9864\n",
            "2025-12-23 02:18:32.365384: Pseudo dice [np.float32(0.9897)]\n",
            "2025-12-23 02:18:32.365494: Epoch time: 22.45 s\n",
            "2025-12-23 02:18:33.756005: \n",
            "2025-12-23 02:18:33.756284: Epoch 685\n",
            "2025-12-23 02:18:33.756437: Current learning rate: 0.00354\n",
            "2025-12-23 02:18:56.195945: train_loss -0.9741\n",
            "2025-12-23 02:18:56.196338: val_loss -0.9852\n",
            "2025-12-23 02:18:56.196473: Pseudo dice [np.float32(0.9885)]\n",
            "2025-12-23 02:18:56.196613: Epoch time: 22.44 s\n",
            "2025-12-23 02:18:57.551935: \n",
            "2025-12-23 02:18:57.552311: Epoch 686\n",
            "2025-12-23 02:18:57.552462: Current learning rate: 0.00353\n",
            "2025-12-23 02:19:20.014096: train_loss -0.9739\n",
            "2025-12-23 02:19:20.014321: val_loss -0.985\n",
            "2025-12-23 02:19:20.014499: Pseudo dice [np.float32(0.9897)]\n",
            "2025-12-23 02:19:20.014686: Epoch time: 22.46 s\n",
            "2025-12-23 02:19:21.416380: \n",
            "2025-12-23 02:19:21.416769: Epoch 687\n",
            "2025-12-23 02:19:21.416908: Current learning rate: 0.00352\n",
            "2025-12-23 02:19:43.922748: train_loss -0.9734\n",
            "2025-12-23 02:19:43.923006: val_loss -0.985\n",
            "2025-12-23 02:19:43.923179: Pseudo dice [np.float32(0.9888)]\n",
            "2025-12-23 02:19:43.923389: Epoch time: 22.51 s\n",
            "2025-12-23 02:19:45.292482: \n",
            "2025-12-23 02:19:45.292673: Epoch 688\n",
            "2025-12-23 02:19:45.292798: Current learning rate: 0.00351\n",
            "2025-12-23 02:20:07.772779: train_loss -0.9735\n",
            "2025-12-23 02:20:07.773027: val_loss -0.9864\n",
            "2025-12-23 02:20:07.773197: Pseudo dice [np.float32(0.9902)]\n",
            "2025-12-23 02:20:07.773332: Epoch time: 22.48 s\n",
            "2025-12-23 02:20:09.135950: \n",
            "2025-12-23 02:20:09.136161: Epoch 689\n",
            "2025-12-23 02:20:09.136303: Current learning rate: 0.0035\n",
            "2025-12-23 02:20:31.574251: train_loss -0.9734\n",
            "2025-12-23 02:20:31.574478: val_loss -0.986\n",
            "2025-12-23 02:20:31.574570: Pseudo dice [np.float32(0.9889)]\n",
            "2025-12-23 02:20:31.574660: Epoch time: 22.44 s\n",
            "2025-12-23 02:20:32.961575: \n",
            "2025-12-23 02:20:32.961912: Epoch 690\n",
            "2025-12-23 02:20:32.962053: Current learning rate: 0.00349\n",
            "2025-12-23 02:20:55.415920: train_loss -0.973\n",
            "2025-12-23 02:20:55.416197: val_loss -0.9844\n",
            "2025-12-23 02:20:55.416372: Pseudo dice [np.float32(0.9879)]\n",
            "2025-12-23 02:20:55.416475: Epoch time: 22.46 s\n",
            "2025-12-23 02:20:56.808847: \n",
            "2025-12-23 02:20:56.809154: Epoch 691\n",
            "2025-12-23 02:20:56.809308: Current learning rate: 0.00348\n",
            "2025-12-23 02:21:19.290622: train_loss -0.9731\n",
            "2025-12-23 02:21:19.290846: val_loss -0.9849\n",
            "2025-12-23 02:21:19.290935: Pseudo dice [np.float32(0.9891)]\n",
            "2025-12-23 02:21:19.291023: Epoch time: 22.48 s\n",
            "2025-12-23 02:21:20.664489: \n",
            "2025-12-23 02:21:20.664916: Epoch 692\n",
            "2025-12-23 02:21:20.665133: Current learning rate: 0.00346\n",
            "2025-12-23 02:21:43.102038: train_loss -0.9727\n",
            "2025-12-23 02:21:43.102322: val_loss -0.9863\n",
            "2025-12-23 02:21:43.102443: Pseudo dice [np.float32(0.9894)]\n",
            "2025-12-23 02:21:43.102565: Epoch time: 22.44 s\n",
            "2025-12-23 02:21:44.469693: \n",
            "2025-12-23 02:21:44.469983: Epoch 693\n",
            "2025-12-23 02:21:44.470114: Current learning rate: 0.00345\n",
            "2025-12-23 02:22:07.078656: train_loss -0.9727\n",
            "2025-12-23 02:22:07.078954: val_loss -0.9848\n",
            "2025-12-23 02:22:07.079074: Pseudo dice [np.float32(0.9876)]\n",
            "2025-12-23 02:22:07.079169: Epoch time: 22.61 s\n",
            "2025-12-23 02:22:08.452696: \n",
            "2025-12-23 02:22:08.453009: Epoch 694\n",
            "2025-12-23 02:22:08.453153: Current learning rate: 0.00344\n",
            "2025-12-23 02:22:30.851117: train_loss -0.9747\n",
            "2025-12-23 02:22:30.851344: val_loss -0.9865\n",
            "2025-12-23 02:22:30.851496: Pseudo dice [np.float32(0.9897)]\n",
            "2025-12-23 02:22:30.851616: Epoch time: 22.4 s\n",
            "2025-12-23 02:22:32.879957: \n",
            "2025-12-23 02:22:32.880387: Epoch 695\n",
            "2025-12-23 02:22:32.880544: Current learning rate: 0.00343\n",
            "2025-12-23 02:22:55.376091: train_loss -0.9728\n",
            "2025-12-23 02:22:55.376544: val_loss -0.9855\n",
            "2025-12-23 02:22:55.376725: Pseudo dice [np.float32(0.9892)]\n",
            "2025-12-23 02:22:55.376828: Epoch time: 22.5 s\n",
            "2025-12-23 02:22:56.751469: \n",
            "2025-12-23 02:22:56.751917: Epoch 696\n",
            "2025-12-23 02:22:56.752064: Current learning rate: 0.00342\n",
            "2025-12-23 02:23:19.292505: train_loss -0.9731\n",
            "2025-12-23 02:23:19.292710: val_loss -0.9868\n",
            "2025-12-23 02:23:19.292799: Pseudo dice [np.float32(0.9898)]\n",
            "2025-12-23 02:23:19.292894: Epoch time: 22.54 s\n",
            "2025-12-23 02:23:19.293140: Yayy! New best EMA pseudo Dice: 0.9890000224113464\n",
            "2025-12-23 02:23:21.192764: \n",
            "2025-12-23 02:23:21.192964: Epoch 697\n",
            "2025-12-23 02:23:21.193089: Current learning rate: 0.00341\n",
            "2025-12-23 02:23:43.692093: train_loss -0.9738\n",
            "2025-12-23 02:23:43.692352: val_loss -0.9857\n",
            "2025-12-23 02:23:43.692473: Pseudo dice [np.float32(0.9882)]\n",
            "2025-12-23 02:23:43.692640: Epoch time: 22.5 s\n",
            "2025-12-23 02:23:45.071997: \n",
            "2025-12-23 02:23:45.072330: Epoch 698\n",
            "2025-12-23 02:23:45.072469: Current learning rate: 0.0034\n",
            "2025-12-23 02:24:07.551586: train_loss -0.9736\n",
            "2025-12-23 02:24:07.551851: val_loss -0.9863\n",
            "2025-12-23 02:24:07.552017: Pseudo dice [np.float32(0.9893)]\n",
            "2025-12-23 02:24:07.552154: Epoch time: 22.48 s\n",
            "2025-12-23 02:24:08.943552: \n",
            "2025-12-23 02:24:08.943902: Epoch 699\n",
            "2025-12-23 02:24:08.944049: Current learning rate: 0.00339\n",
            "2025-12-23 02:24:31.394184: train_loss -0.9736\n",
            "2025-12-23 02:24:31.394461: val_loss -0.9851\n",
            "2025-12-23 02:24:31.394589: Pseudo dice [np.float32(0.9888)]\n",
            "2025-12-23 02:24:31.394693: Epoch time: 22.45 s\n",
            "2025-12-23 02:24:33.336956: \n",
            "2025-12-23 02:24:33.337250: Epoch 700\n",
            "2025-12-23 02:24:33.337401: Current learning rate: 0.00338\n",
            "2025-12-23 02:24:55.839954: train_loss -0.9742\n",
            "2025-12-23 02:24:55.840339: val_loss -0.9849\n",
            "2025-12-23 02:24:55.840464: Pseudo dice [np.float32(0.9883)]\n",
            "2025-12-23 02:24:55.840596: Epoch time: 22.5 s\n",
            "2025-12-23 02:24:57.221099: \n",
            "2025-12-23 02:24:57.221357: Epoch 701\n",
            "2025-12-23 02:24:57.221516: Current learning rate: 0.00337\n",
            "2025-12-23 02:25:19.703413: train_loss -0.9753\n",
            "2025-12-23 02:25:19.703628: val_loss -0.9867\n",
            "2025-12-23 02:25:19.703718: Pseudo dice [np.float32(0.9895)]\n",
            "2025-12-23 02:25:19.703813: Epoch time: 22.48 s\n",
            "2025-12-23 02:25:21.071540: \n",
            "2025-12-23 02:25:21.071839: Epoch 702\n",
            "2025-12-23 02:25:21.071970: Current learning rate: 0.00336\n",
            "2025-12-23 02:25:43.542063: train_loss -0.9739\n",
            "2025-12-23 02:25:43.542520: val_loss -0.9866\n",
            "2025-12-23 02:25:43.542681: Pseudo dice [np.float32(0.9889)]\n",
            "2025-12-23 02:25:43.542843: Epoch time: 22.47 s\n",
            "2025-12-23 02:25:44.944366: \n",
            "2025-12-23 02:25:44.944755: Epoch 703\n",
            "2025-12-23 02:25:44.944902: Current learning rate: 0.00335\n",
            "2025-12-23 02:26:07.468270: train_loss -0.9748\n",
            "2025-12-23 02:26:07.468475: val_loss -0.9858\n",
            "2025-12-23 02:26:07.468591: Pseudo dice [np.float32(0.9889)]\n",
            "2025-12-23 02:26:07.468696: Epoch time: 22.53 s\n",
            "2025-12-23 02:26:08.885359: \n",
            "2025-12-23 02:26:08.885554: Epoch 704\n",
            "2025-12-23 02:26:08.885686: Current learning rate: 0.00334\n",
            "2025-12-23 02:26:31.295826: train_loss -0.9739\n",
            "2025-12-23 02:26:31.296026: val_loss -0.9857\n",
            "2025-12-23 02:26:31.296117: Pseudo dice [np.float32(0.9895)]\n",
            "2025-12-23 02:26:31.296207: Epoch time: 22.41 s\n",
            "2025-12-23 02:26:31.296300: Yayy! New best EMA pseudo Dice: 0.9890000224113464\n",
            "2025-12-23 02:26:33.206552: \n",
            "2025-12-23 02:26:33.206865: Epoch 705\n",
            "2025-12-23 02:26:33.207000: Current learning rate: 0.00333\n",
            "2025-12-23 02:26:55.659399: train_loss -0.9751\n",
            "2025-12-23 02:26:55.659812: val_loss -0.9851\n",
            "2025-12-23 02:26:55.659911: Pseudo dice [np.float32(0.9883)]\n",
            "2025-12-23 02:26:55.660009: Epoch time: 22.45 s\n",
            "2025-12-23 02:26:57.047622: \n",
            "2025-12-23 02:26:57.047800: Epoch 706\n",
            "2025-12-23 02:26:57.047962: Current learning rate: 0.00332\n",
            "2025-12-23 02:27:19.508208: train_loss -0.974\n",
            "2025-12-23 02:27:19.508662: val_loss -0.9853\n",
            "2025-12-23 02:27:19.508864: Pseudo dice [np.float32(0.9889)]\n",
            "2025-12-23 02:27:19.509079: Epoch time: 22.46 s\n",
            "2025-12-23 02:27:20.896061: \n",
            "2025-12-23 02:27:20.896370: Epoch 707\n",
            "2025-12-23 02:27:20.896515: Current learning rate: 0.00331\n",
            "2025-12-23 02:27:43.400009: train_loss -0.9734\n",
            "2025-12-23 02:27:43.400277: val_loss -0.9862\n",
            "2025-12-23 02:27:43.400393: Pseudo dice [np.float32(0.9892)]\n",
            "2025-12-23 02:27:43.400509: Epoch time: 22.51 s\n",
            "2025-12-23 02:27:44.781092: \n",
            "2025-12-23 02:27:44.781441: Epoch 708\n",
            "2025-12-23 02:27:44.781595: Current learning rate: 0.0033\n",
            "2025-12-23 02:28:07.186582: train_loss -0.9738\n",
            "2025-12-23 02:28:07.186885: val_loss -0.9864\n",
            "2025-12-23 02:28:07.186983: Pseudo dice [np.float32(0.9891)]\n",
            "2025-12-23 02:28:07.187073: Epoch time: 22.41 s\n",
            "2025-12-23 02:28:08.533086: \n",
            "2025-12-23 02:28:08.533400: Epoch 709\n",
            "2025-12-23 02:28:08.533539: Current learning rate: 0.00329\n",
            "2025-12-23 02:28:30.902100: train_loss -0.9736\n",
            "2025-12-23 02:28:30.902323: val_loss -0.9868\n",
            "2025-12-23 02:28:30.902415: Pseudo dice [np.float32(0.9897)]\n",
            "2025-12-23 02:28:30.902515: Epoch time: 22.37 s\n",
            "2025-12-23 02:28:30.902592: Yayy! New best EMA pseudo Dice: 0.9890000224113464\n",
            "2025-12-23 02:28:32.784093: \n",
            "2025-12-23 02:28:32.784481: Epoch 710\n",
            "2025-12-23 02:28:32.784628: Current learning rate: 0.00328\n",
            "2025-12-23 02:28:55.223414: train_loss -0.9735\n",
            "2025-12-23 02:28:55.223709: val_loss -0.9866\n",
            "2025-12-23 02:28:55.223803: Pseudo dice [np.float32(0.9897)]\n",
            "2025-12-23 02:28:55.223895: Epoch time: 22.44 s\n",
            "2025-12-23 02:28:55.223969: Yayy! New best EMA pseudo Dice: 0.9890999794006348\n",
            "2025-12-23 02:28:57.176520: \n",
            "2025-12-23 02:28:57.176809: Epoch 711\n",
            "2025-12-23 02:28:57.176945: Current learning rate: 0.00327\n",
            "2025-12-23 02:29:19.635614: train_loss -0.9738\n",
            "2025-12-23 02:29:19.635841: val_loss -0.9857\n",
            "2025-12-23 02:29:19.635940: Pseudo dice [np.float32(0.9888)]\n",
            "2025-12-23 02:29:19.636075: Epoch time: 22.46 s\n",
            "2025-12-23 02:29:21.691163: \n",
            "2025-12-23 02:29:21.691485: Epoch 712\n",
            "2025-12-23 02:29:21.691643: Current learning rate: 0.00326\n",
            "2025-12-23 02:29:44.215155: train_loss -0.9739\n",
            "2025-12-23 02:29:44.215388: val_loss -0.9858\n",
            "2025-12-23 02:29:44.215486: Pseudo dice [np.float32(0.9884)]\n",
            "2025-12-23 02:29:44.215582: Epoch time: 22.53 s\n",
            "2025-12-23 02:29:45.586366: \n",
            "2025-12-23 02:29:45.586598: Epoch 713\n",
            "2025-12-23 02:29:45.586735: Current learning rate: 0.00325\n",
            "2025-12-23 02:30:08.054114: train_loss -0.974\n",
            "2025-12-23 02:30:08.054335: val_loss -0.9863\n",
            "2025-12-23 02:30:08.054429: Pseudo dice [np.float32(0.9894)]\n",
            "2025-12-23 02:30:08.054519: Epoch time: 22.47 s\n",
            "2025-12-23 02:30:09.384585: \n",
            "2025-12-23 02:30:09.384945: Epoch 714\n",
            "2025-12-23 02:30:09.385087: Current learning rate: 0.00324\n",
            "2025-12-23 02:30:31.851589: train_loss -0.9746\n",
            "2025-12-23 02:30:31.852032: val_loss -0.9843\n",
            "2025-12-23 02:30:31.852188: Pseudo dice [np.float32(0.9882)]\n",
            "2025-12-23 02:30:31.852353: Epoch time: 22.47 s\n",
            "2025-12-23 02:30:33.229850: \n",
            "2025-12-23 02:30:33.230130: Epoch 715\n",
            "2025-12-23 02:30:33.230301: Current learning rate: 0.00323\n",
            "2025-12-23 02:30:55.733570: train_loss -0.9737\n",
            "2025-12-23 02:30:55.733764: val_loss -0.9856\n",
            "2025-12-23 02:30:55.733853: Pseudo dice [np.float32(0.9889)]\n",
            "2025-12-23 02:30:55.733940: Epoch time: 22.51 s\n",
            "2025-12-23 02:30:57.131591: \n",
            "2025-12-23 02:30:57.131913: Epoch 716\n",
            "2025-12-23 02:30:57.132064: Current learning rate: 0.00322\n",
            "2025-12-23 02:31:19.551210: train_loss -0.9741\n",
            "2025-12-23 02:31:19.551512: val_loss -0.9856\n",
            "2025-12-23 02:31:19.551613: Pseudo dice [np.float32(0.9896)]\n",
            "2025-12-23 02:31:19.551702: Epoch time: 22.42 s\n",
            "2025-12-23 02:31:20.913821: \n",
            "2025-12-23 02:31:20.914012: Epoch 717\n",
            "2025-12-23 02:31:20.914135: Current learning rate: 0.00321\n",
            "2025-12-23 02:31:43.326869: train_loss -0.9743\n",
            "2025-12-23 02:31:43.327258: val_loss -0.9865\n",
            "2025-12-23 02:31:43.327443: Pseudo dice [np.float32(0.9894)]\n",
            "2025-12-23 02:31:43.327617: Epoch time: 22.41 s\n",
            "2025-12-23 02:31:44.733852: \n",
            "2025-12-23 02:31:44.734137: Epoch 718\n",
            "2025-12-23 02:31:44.734283: Current learning rate: 0.0032\n",
            "2025-12-23 02:32:07.188580: train_loss -0.975\n",
            "2025-12-23 02:32:07.188797: val_loss -0.9861\n",
            "2025-12-23 02:32:07.188888: Pseudo dice [np.float32(0.9889)]\n",
            "2025-12-23 02:32:07.188977: Epoch time: 22.46 s\n",
            "2025-12-23 02:32:08.571276: \n",
            "2025-12-23 02:32:08.571616: Epoch 719\n",
            "2025-12-23 02:32:08.571762: Current learning rate: 0.00319\n",
            "2025-12-23 02:32:31.060092: train_loss -0.9747\n",
            "2025-12-23 02:32:31.060516: val_loss -0.9863\n",
            "2025-12-23 02:32:31.060623: Pseudo dice [np.float32(0.9895)]\n",
            "2025-12-23 02:32:31.060723: Epoch time: 22.49 s\n",
            "2025-12-23 02:32:32.470869: \n",
            "2025-12-23 02:32:32.471183: Epoch 720\n",
            "2025-12-23 02:32:32.471349: Current learning rate: 0.00318\n",
            "2025-12-23 02:32:54.866464: train_loss -0.9738\n",
            "2025-12-23 02:32:54.866678: val_loss -0.986\n",
            "2025-12-23 02:32:54.866766: Pseudo dice [np.float32(0.9893)]\n",
            "2025-12-23 02:32:54.866859: Epoch time: 22.4 s\n",
            "2025-12-23 02:32:54.866938: Yayy! New best EMA pseudo Dice: 0.9890999794006348\n",
            "2025-12-23 02:32:56.824209: \n",
            "2025-12-23 02:32:56.824408: Epoch 721\n",
            "2025-12-23 02:32:56.824541: Current learning rate: 0.00317\n",
            "2025-12-23 02:33:19.304793: train_loss -0.9734\n",
            "2025-12-23 02:33:19.305097: val_loss -0.985\n",
            "2025-12-23 02:33:19.305201: Pseudo dice [np.float32(0.9881)]\n",
            "2025-12-23 02:33:19.305342: Epoch time: 22.48 s\n",
            "2025-12-23 02:33:20.727488: \n",
            "2025-12-23 02:33:20.727659: Epoch 722\n",
            "2025-12-23 02:33:20.727847: Current learning rate: 0.00316\n",
            "2025-12-23 02:33:43.186287: train_loss -0.9718\n",
            "2025-12-23 02:33:43.186653: val_loss -0.9844\n",
            "2025-12-23 02:33:43.186786: Pseudo dice [np.float32(0.9878)]\n",
            "2025-12-23 02:33:43.186921: Epoch time: 22.46 s\n",
            "2025-12-23 02:33:44.569571: \n",
            "2025-12-23 02:33:44.569872: Epoch 723\n",
            "2025-12-23 02:33:44.570013: Current learning rate: 0.00315\n",
            "2025-12-23 02:34:06.982756: train_loss -0.9694\n",
            "2025-12-23 02:34:06.983016: val_loss -0.9839\n",
            "2025-12-23 02:34:06.983165: Pseudo dice [np.float32(0.9878)]\n",
            "2025-12-23 02:34:06.983320: Epoch time: 22.41 s\n",
            "2025-12-23 02:34:08.391160: \n",
            "2025-12-23 02:34:08.391494: Epoch 724\n",
            "2025-12-23 02:34:08.391630: Current learning rate: 0.00314\n",
            "2025-12-23 02:34:30.852582: train_loss -0.972\n",
            "2025-12-23 02:34:30.852862: val_loss -0.9837\n",
            "2025-12-23 02:34:30.852989: Pseudo dice [np.float32(0.9884)]\n",
            "2025-12-23 02:34:30.853117: Epoch time: 22.46 s\n",
            "2025-12-23 02:34:32.267738: \n",
            "2025-12-23 02:34:32.268052: Epoch 725\n",
            "2025-12-23 02:34:32.268195: Current learning rate: 0.00313\n",
            "2025-12-23 02:34:54.674829: train_loss -0.9742\n",
            "2025-12-23 02:34:54.675203: val_loss -0.985\n",
            "2025-12-23 02:34:54.675352: Pseudo dice [np.float32(0.9888)]\n",
            "2025-12-23 02:34:54.675481: Epoch time: 22.41 s\n",
            "2025-12-23 02:34:56.056972: \n",
            "2025-12-23 02:34:56.057256: Epoch 726\n",
            "2025-12-23 02:34:56.057429: Current learning rate: 0.00312\n",
            "2025-12-23 02:35:18.491703: train_loss -0.9741\n",
            "2025-12-23 02:35:18.492005: val_loss -0.9858\n",
            "2025-12-23 02:35:18.492108: Pseudo dice [np.float32(0.9893)]\n",
            "2025-12-23 02:35:18.492200: Epoch time: 22.44 s\n",
            "2025-12-23 02:35:19.868763: \n",
            "2025-12-23 02:35:19.869121: Epoch 727\n",
            "2025-12-23 02:35:19.869274: Current learning rate: 0.00311\n",
            "2025-12-23 02:35:42.282711: train_loss -0.9739\n",
            "2025-12-23 02:35:42.282925: val_loss -0.9862\n",
            "2025-12-23 02:35:42.283014: Pseudo dice [np.float32(0.9897)]\n",
            "2025-12-23 02:35:42.283103: Epoch time: 22.42 s\n",
            "2025-12-23 02:35:43.670738: \n",
            "2025-12-23 02:35:43.671013: Epoch 728\n",
            "2025-12-23 02:35:43.671154: Current learning rate: 0.0031\n",
            "2025-12-23 02:36:06.100051: train_loss -0.973\n",
            "2025-12-23 02:36:06.100270: val_loss -0.985\n",
            "2025-12-23 02:36:06.100374: Pseudo dice [np.float32(0.9884)]\n",
            "2025-12-23 02:36:06.100498: Epoch time: 22.43 s\n",
            "2025-12-23 02:36:08.176026: \n",
            "2025-12-23 02:36:08.176470: Epoch 729\n",
            "2025-12-23 02:36:08.176673: Current learning rate: 0.00309\n",
            "2025-12-23 02:36:30.673605: train_loss -0.9719\n",
            "2025-12-23 02:36:30.673804: val_loss -0.9836\n",
            "2025-12-23 02:36:30.673892: Pseudo dice [np.float32(0.9876)]\n",
            "2025-12-23 02:36:30.673982: Epoch time: 22.5 s\n",
            "2025-12-23 02:36:32.032583: \n",
            "2025-12-23 02:36:32.032979: Epoch 730\n",
            "2025-12-23 02:36:32.033123: Current learning rate: 0.00308\n",
            "2025-12-23 02:36:54.539710: train_loss -0.9728\n",
            "2025-12-23 02:36:54.539962: val_loss -0.9861\n",
            "2025-12-23 02:36:54.540129: Pseudo dice [np.float32(0.9893)]\n",
            "2025-12-23 02:36:54.540259: Epoch time: 22.51 s\n",
            "2025-12-23 02:36:55.920429: \n",
            "2025-12-23 02:36:55.920672: Epoch 731\n",
            "2025-12-23 02:36:55.920837: Current learning rate: 0.00307\n",
            "2025-12-23 02:37:18.403502: train_loss -0.972\n",
            "2025-12-23 02:37:18.403857: val_loss -0.9854\n",
            "2025-12-23 02:37:18.404026: Pseudo dice [np.float32(0.989)]\n",
            "2025-12-23 02:37:18.404171: Epoch time: 22.48 s\n",
            "2025-12-23 02:37:19.756356: \n",
            "2025-12-23 02:37:19.756692: Epoch 732\n",
            "2025-12-23 02:37:19.756841: Current learning rate: 0.00306\n",
            "2025-12-23 02:37:42.226845: train_loss -0.9721\n",
            "2025-12-23 02:37:42.227198: val_loss -0.9856\n",
            "2025-12-23 02:37:42.227343: Pseudo dice [np.float32(0.9891)]\n",
            "2025-12-23 02:37:42.227458: Epoch time: 22.47 s\n",
            "2025-12-23 02:37:43.589172: \n",
            "2025-12-23 02:37:43.589365: Epoch 733\n",
            "2025-12-23 02:37:43.589514: Current learning rate: 0.00305\n",
            "2025-12-23 02:38:06.098497: train_loss -0.9727\n",
            "2025-12-23 02:38:06.098763: val_loss -0.9849\n",
            "2025-12-23 02:38:06.098858: Pseudo dice [np.float32(0.9881)]\n",
            "2025-12-23 02:38:06.098949: Epoch time: 22.51 s\n",
            "2025-12-23 02:38:07.482182: \n",
            "2025-12-23 02:38:07.482367: Epoch 734\n",
            "2025-12-23 02:38:07.482504: Current learning rate: 0.00304\n",
            "2025-12-23 02:38:29.952569: train_loss -0.9723\n",
            "2025-12-23 02:38:29.952822: val_loss -0.9852\n",
            "2025-12-23 02:38:29.952914: Pseudo dice [np.float32(0.9883)]\n",
            "2025-12-23 02:38:29.953001: Epoch time: 22.47 s\n",
            "2025-12-23 02:38:31.313418: \n",
            "2025-12-23 02:38:31.313664: Epoch 735\n",
            "2025-12-23 02:38:31.313792: Current learning rate: 0.00303\n",
            "2025-12-23 02:38:53.758671: train_loss -0.9732\n",
            "2025-12-23 02:38:53.758879: val_loss -0.9863\n",
            "2025-12-23 02:38:53.758991: Pseudo dice [np.float32(0.989)]\n",
            "2025-12-23 02:38:53.759083: Epoch time: 22.45 s\n",
            "2025-12-23 02:38:55.166239: \n",
            "2025-12-23 02:38:55.166449: Epoch 736\n",
            "2025-12-23 02:38:55.166609: Current learning rate: 0.00302\n",
            "2025-12-23 02:39:17.597386: train_loss -0.9726\n",
            "2025-12-23 02:39:17.597592: val_loss -0.9852\n",
            "2025-12-23 02:39:17.597710: Pseudo dice [np.float32(0.989)]\n",
            "2025-12-23 02:39:17.597816: Epoch time: 22.43 s\n",
            "2025-12-23 02:39:18.968168: \n",
            "2025-12-23 02:39:18.968489: Epoch 737\n",
            "2025-12-23 02:39:18.968623: Current learning rate: 0.00301\n",
            "2025-12-23 02:39:41.441457: train_loss -0.9737\n",
            "2025-12-23 02:39:41.441666: val_loss -0.9856\n",
            "2025-12-23 02:39:41.441752: Pseudo dice [np.float32(0.9889)]\n",
            "2025-12-23 02:39:41.441847: Epoch time: 22.47 s\n",
            "2025-12-23 02:39:42.817350: \n",
            "2025-12-23 02:39:42.817653: Epoch 738\n",
            "2025-12-23 02:39:42.817811: Current learning rate: 0.003\n",
            "2025-12-23 02:40:05.268890: train_loss -0.9741\n",
            "2025-12-23 02:40:05.269124: val_loss -0.9853\n",
            "2025-12-23 02:40:05.269246: Pseudo dice [np.float32(0.9889)]\n",
            "2025-12-23 02:40:05.269378: Epoch time: 22.45 s\n",
            "2025-12-23 02:40:06.638602: \n",
            "2025-12-23 02:40:06.638758: Epoch 739\n",
            "2025-12-23 02:40:06.638888: Current learning rate: 0.00299\n",
            "2025-12-23 02:40:29.096709: train_loss -0.9742\n",
            "2025-12-23 02:40:29.097047: val_loss -0.9865\n",
            "2025-12-23 02:40:29.097157: Pseudo dice [np.float32(0.99)]\n",
            "2025-12-23 02:40:29.097274: Epoch time: 22.46 s\n",
            "2025-12-23 02:40:30.447802: \n",
            "2025-12-23 02:40:30.448161: Epoch 740\n",
            "2025-12-23 02:40:30.448308: Current learning rate: 0.00297\n",
            "2025-12-23 02:40:52.888923: train_loss -0.9732\n",
            "2025-12-23 02:40:52.889127: val_loss -0.987\n",
            "2025-12-23 02:40:52.889232: Pseudo dice [np.float32(0.99)]\n",
            "2025-12-23 02:40:52.889348: Epoch time: 22.44 s\n",
            "2025-12-23 02:40:54.276321: \n",
            "2025-12-23 02:40:54.276618: Epoch 741\n",
            "2025-12-23 02:40:54.276747: Current learning rate: 0.00296\n",
            "2025-12-23 02:41:16.742930: train_loss -0.9725\n",
            "2025-12-23 02:41:16.743136: val_loss -0.9847\n",
            "2025-12-23 02:41:16.743255: Pseudo dice [np.float32(0.9884)]\n",
            "2025-12-23 02:41:16.743426: Epoch time: 22.47 s\n",
            "2025-12-23 02:41:18.148439: \n",
            "2025-12-23 02:41:18.148724: Epoch 742\n",
            "2025-12-23 02:41:18.148863: Current learning rate: 0.00295\n",
            "2025-12-23 02:41:40.598558: train_loss -0.9716\n",
            "2025-12-23 02:41:40.598757: val_loss -0.9776\n",
            "2025-12-23 02:41:40.598853: Pseudo dice [np.float32(0.9819)]\n",
            "2025-12-23 02:41:40.598943: Epoch time: 22.45 s\n",
            "2025-12-23 02:41:41.954384: \n",
            "2025-12-23 02:41:41.954617: Epoch 743\n",
            "2025-12-23 02:41:41.954743: Current learning rate: 0.00294\n",
            "2025-12-23 02:42:04.400698: train_loss -0.9729\n",
            "2025-12-23 02:42:04.400942: val_loss -0.986\n",
            "2025-12-23 02:42:04.401186: Pseudo dice [np.float32(0.9895)]\n",
            "2025-12-23 02:42:04.401397: Epoch time: 22.45 s\n",
            "2025-12-23 02:42:05.762949: \n",
            "2025-12-23 02:42:05.763250: Epoch 744\n",
            "2025-12-23 02:42:05.763381: Current learning rate: 0.00293\n",
            "2025-12-23 02:42:28.180260: train_loss -0.9742\n",
            "2025-12-23 02:42:28.180509: val_loss -0.9856\n",
            "2025-12-23 02:42:28.180625: Pseudo dice [np.float32(0.9884)]\n",
            "2025-12-23 02:42:28.180725: Epoch time: 22.42 s\n",
            "2025-12-23 02:42:29.540244: \n",
            "2025-12-23 02:42:29.540429: Epoch 745\n",
            "2025-12-23 02:42:29.540554: Current learning rate: 0.00292\n",
            "2025-12-23 02:42:51.954386: train_loss -0.9739\n",
            "2025-12-23 02:42:51.954691: val_loss -0.9862\n",
            "2025-12-23 02:42:51.954813: Pseudo dice [np.float32(0.9896)]\n",
            "2025-12-23 02:42:51.955035: Epoch time: 22.42 s\n",
            "2025-12-23 02:42:53.991610: \n",
            "2025-12-23 02:42:53.991977: Epoch 746\n",
            "2025-12-23 02:42:53.992108: Current learning rate: 0.00291\n",
            "2025-12-23 02:43:16.479761: train_loss -0.9743\n",
            "2025-12-23 02:43:16.480157: val_loss -0.9865\n",
            "2025-12-23 02:43:16.480331: Pseudo dice [np.float32(0.989)]\n",
            "2025-12-23 02:43:16.480463: Epoch time: 22.49 s\n",
            "2025-12-23 02:43:17.826907: \n",
            "2025-12-23 02:43:17.827251: Epoch 747\n",
            "2025-12-23 02:43:17.827384: Current learning rate: 0.0029\n",
            "2025-12-23 02:43:40.327651: train_loss -0.9737\n",
            "2025-12-23 02:43:40.327873: val_loss -0.9857\n",
            "2025-12-23 02:43:40.327963: Pseudo dice [np.float32(0.9888)]\n",
            "2025-12-23 02:43:40.328053: Epoch time: 22.5 s\n",
            "2025-12-23 02:43:41.714383: \n",
            "2025-12-23 02:43:41.714609: Epoch 748\n",
            "2025-12-23 02:43:41.714767: Current learning rate: 0.00289\n",
            "2025-12-23 02:44:04.191863: train_loss -0.9742\n",
            "2025-12-23 02:44:04.192131: val_loss -0.9853\n",
            "2025-12-23 02:44:04.192323: Pseudo dice [np.float32(0.9886)]\n",
            "2025-12-23 02:44:04.192421: Epoch time: 22.48 s\n",
            "2025-12-23 02:44:05.517534: \n",
            "2025-12-23 02:44:05.517900: Epoch 749\n",
            "2025-12-23 02:44:05.518037: Current learning rate: 0.00288\n",
            "2025-12-23 02:44:28.008198: train_loss -0.974\n",
            "2025-12-23 02:44:28.008418: val_loss -0.9866\n",
            "2025-12-23 02:44:28.008506: Pseudo dice [np.float32(0.9899)]\n",
            "2025-12-23 02:44:28.008606: Epoch time: 22.49 s\n",
            "2025-12-23 02:44:29.868034: \n",
            "2025-12-23 02:44:29.868250: Epoch 750\n",
            "2025-12-23 02:44:29.868378: Current learning rate: 0.00287\n",
            "2025-12-23 02:44:52.356638: train_loss -0.9744\n",
            "2025-12-23 02:44:52.357287: val_loss -0.9859\n",
            "2025-12-23 02:44:52.357394: Pseudo dice [np.float32(0.9898)]\n",
            "2025-12-23 02:44:52.357501: Epoch time: 22.49 s\n",
            "2025-12-23 02:44:53.717524: \n",
            "2025-12-23 02:44:53.717872: Epoch 751\n",
            "2025-12-23 02:44:53.718007: Current learning rate: 0.00286\n",
            "2025-12-23 02:45:16.172766: train_loss -0.9752\n",
            "2025-12-23 02:45:16.172976: val_loss -0.9868\n",
            "2025-12-23 02:45:16.173088: Pseudo dice [np.float32(0.9896)]\n",
            "2025-12-23 02:45:16.173204: Epoch time: 22.46 s\n",
            "2025-12-23 02:45:17.539737: \n",
            "2025-12-23 02:45:17.539932: Epoch 752\n",
            "2025-12-23 02:45:17.540110: Current learning rate: 0.00285\n",
            "2025-12-23 02:45:39.954954: train_loss -0.9748\n",
            "2025-12-23 02:45:39.955189: val_loss -0.987\n",
            "2025-12-23 02:45:39.955332: Pseudo dice [np.float32(0.9895)]\n",
            "2025-12-23 02:45:39.955427: Epoch time: 22.42 s\n",
            "2025-12-23 02:45:41.351202: \n",
            "2025-12-23 02:45:41.351511: Epoch 753\n",
            "2025-12-23 02:45:41.351649: Current learning rate: 0.00284\n",
            "2025-12-23 02:46:03.802692: train_loss -0.9743\n",
            "2025-12-23 02:46:03.802921: val_loss -0.987\n",
            "2025-12-23 02:46:03.803019: Pseudo dice [np.float32(0.9902)]\n",
            "2025-12-23 02:46:03.803111: Epoch time: 22.45 s\n",
            "2025-12-23 02:46:05.189018: \n",
            "2025-12-23 02:46:05.189422: Epoch 754\n",
            "2025-12-23 02:46:05.189590: Current learning rate: 0.00283\n",
            "2025-12-23 02:46:27.622834: train_loss -0.9747\n",
            "2025-12-23 02:46:27.623037: val_loss -0.9859\n",
            "2025-12-23 02:46:27.623123: Pseudo dice [np.float32(0.9886)]\n",
            "2025-12-23 02:46:27.623210: Epoch time: 22.44 s\n",
            "2025-12-23 02:46:28.991358: \n",
            "2025-12-23 02:46:28.991546: Epoch 755\n",
            "2025-12-23 02:46:28.991670: Current learning rate: 0.00282\n",
            "2025-12-23 02:46:51.401121: train_loss -0.9743\n",
            "2025-12-23 02:46:51.401660: val_loss -0.9861\n",
            "2025-12-23 02:46:51.401811: Pseudo dice [np.float32(0.9893)]\n",
            "2025-12-23 02:46:51.401944: Epoch time: 22.41 s\n",
            "2025-12-23 02:46:52.800395: \n",
            "2025-12-23 02:46:52.800656: Epoch 756\n",
            "2025-12-23 02:46:52.800812: Current learning rate: 0.00281\n",
            "2025-12-23 02:47:15.186445: train_loss -0.9754\n",
            "2025-12-23 02:47:15.186716: val_loss -0.9862\n",
            "2025-12-23 02:47:15.186821: Pseudo dice [np.float32(0.9892)]\n",
            "2025-12-23 02:47:15.186913: Epoch time: 22.39 s\n",
            "2025-12-23 02:47:16.608754: \n",
            "2025-12-23 02:47:16.609071: Epoch 757\n",
            "2025-12-23 02:47:16.609201: Current learning rate: 0.0028\n",
            "2025-12-23 02:47:38.993098: train_loss -0.9733\n",
            "2025-12-23 02:47:38.993479: val_loss -0.9862\n",
            "2025-12-23 02:47:38.993665: Pseudo dice [np.float32(0.9895)]\n",
            "2025-12-23 02:47:38.993818: Epoch time: 22.39 s\n",
            "2025-12-23 02:47:38.993930: Yayy! New best EMA pseudo Dice: 0.9890999794006348\n",
            "2025-12-23 02:47:40.933752: \n",
            "2025-12-23 02:47:40.934098: Epoch 758\n",
            "2025-12-23 02:47:40.934288: Current learning rate: 0.00279\n",
            "2025-12-23 02:48:03.374812: train_loss -0.9743\n",
            "2025-12-23 02:48:03.375039: val_loss -0.9858\n",
            "2025-12-23 02:48:03.375139: Pseudo dice [np.float32(0.9887)]\n",
            "2025-12-23 02:48:03.375256: Epoch time: 22.44 s\n",
            "2025-12-23 02:48:04.799470: \n",
            "2025-12-23 02:48:04.799801: Epoch 759\n",
            "2025-12-23 02:48:04.799941: Current learning rate: 0.00278\n",
            "2025-12-23 02:48:27.221855: train_loss -0.9728\n",
            "2025-12-23 02:48:27.222103: val_loss -0.9862\n",
            "2025-12-23 02:48:27.222314: Pseudo dice [np.float32(0.9888)]\n",
            "2025-12-23 02:48:27.222481: Epoch time: 22.42 s\n",
            "2025-12-23 02:48:28.617734: \n",
            "2025-12-23 02:48:28.618081: Epoch 760\n",
            "2025-12-23 02:48:28.618228: Current learning rate: 0.00277\n",
            "2025-12-23 02:48:51.040021: train_loss -0.9748\n",
            "2025-12-23 02:48:51.040272: val_loss -0.9855\n",
            "2025-12-23 02:48:51.040589: Pseudo dice [np.float32(0.9884)]\n",
            "2025-12-23 02:48:51.040733: Epoch time: 22.42 s\n",
            "2025-12-23 02:48:52.430970: \n",
            "2025-12-23 02:48:52.431277: Epoch 761\n",
            "2025-12-23 02:48:52.431483: Current learning rate: 0.00276\n",
            "2025-12-23 02:49:14.832597: train_loss -0.9753\n",
            "2025-12-23 02:49:14.832848: val_loss -0.9874\n",
            "2025-12-23 02:49:14.833048: Pseudo dice [np.float32(0.9901)]\n",
            "2025-12-23 02:49:14.833147: Epoch time: 22.4 s\n",
            "2025-12-23 02:49:16.237865: \n",
            "2025-12-23 02:49:16.238137: Epoch 762\n",
            "2025-12-23 02:49:16.238288: Current learning rate: 0.00275\n",
            "2025-12-23 02:49:38.676997: train_loss -0.9756\n",
            "2025-12-23 02:49:38.677252: val_loss -0.9854\n",
            "2025-12-23 02:49:38.677366: Pseudo dice [np.float32(0.9893)]\n",
            "2025-12-23 02:49:38.677459: Epoch time: 22.44 s\n",
            "2025-12-23 02:49:40.744610: \n",
            "2025-12-23 02:49:40.744821: Epoch 763\n",
            "2025-12-23 02:49:40.744948: Current learning rate: 0.00274\n",
            "2025-12-23 02:50:03.269498: train_loss -0.975\n",
            "2025-12-23 02:50:03.269740: val_loss -0.9864\n",
            "2025-12-23 02:50:03.269850: Pseudo dice [np.float32(0.9894)]\n",
            "2025-12-23 02:50:03.269943: Epoch time: 22.53 s\n",
            "2025-12-23 02:50:03.270016: Yayy! New best EMA pseudo Dice: 0.9891999959945679\n",
            "2025-12-23 02:50:05.179837: \n",
            "2025-12-23 02:50:05.180119: Epoch 764\n",
            "2025-12-23 02:50:05.180269: Current learning rate: 0.00273\n",
            "2025-12-23 02:50:27.673756: train_loss -0.9751\n",
            "2025-12-23 02:50:27.673957: val_loss -0.9853\n",
            "2025-12-23 02:50:27.674045: Pseudo dice [np.float32(0.9888)]\n",
            "2025-12-23 02:50:27.674131: Epoch time: 22.5 s\n",
            "2025-12-23 02:50:29.027643: \n",
            "2025-12-23 02:50:29.027970: Epoch 765\n",
            "2025-12-23 02:50:29.028109: Current learning rate: 0.00272\n",
            "2025-12-23 02:50:51.427208: train_loss -0.9748\n",
            "2025-12-23 02:50:51.427440: val_loss -0.9874\n",
            "2025-12-23 02:50:51.427545: Pseudo dice [np.float32(0.9902)]\n",
            "2025-12-23 02:50:51.427635: Epoch time: 22.4 s\n",
            "2025-12-23 02:50:51.427712: Yayy! New best EMA pseudo Dice: 0.9891999959945679\n",
            "2025-12-23 02:50:53.290545: \n",
            "2025-12-23 02:50:53.290849: Epoch 766\n",
            "2025-12-23 02:50:53.290978: Current learning rate: 0.00271\n",
            "2025-12-23 02:51:15.758913: train_loss -0.9744\n",
            "2025-12-23 02:51:15.759171: val_loss -0.9852\n",
            "2025-12-23 02:51:15.759298: Pseudo dice [np.float32(0.9891)]\n",
            "2025-12-23 02:51:15.759399: Epoch time: 22.47 s\n",
            "2025-12-23 02:51:17.190755: \n",
            "2025-12-23 02:51:17.190965: Epoch 767\n",
            "2025-12-23 02:51:17.191124: Current learning rate: 0.0027\n",
            "2025-12-23 02:51:39.628690: train_loss -0.9743\n",
            "2025-12-23 02:51:39.628993: val_loss -0.9872\n",
            "2025-12-23 02:51:39.629169: Pseudo dice [np.float32(0.9903)]\n",
            "2025-12-23 02:51:39.629381: Epoch time: 22.44 s\n",
            "2025-12-23 02:51:39.629610: Yayy! New best EMA pseudo Dice: 0.989300012588501\n",
            "2025-12-23 02:51:41.570027: \n",
            "2025-12-23 02:51:41.570435: Epoch 768\n",
            "2025-12-23 02:51:41.570596: Current learning rate: 0.00268\n",
            "2025-12-23 02:52:04.030349: train_loss -0.975\n",
            "2025-12-23 02:52:04.030717: val_loss -0.9859\n",
            "2025-12-23 02:52:04.030911: Pseudo dice [np.float32(0.9891)]\n",
            "2025-12-23 02:52:04.031132: Epoch time: 22.46 s\n",
            "2025-12-23 02:52:05.470390: \n",
            "2025-12-23 02:52:05.470666: Epoch 769\n",
            "2025-12-23 02:52:05.470827: Current learning rate: 0.00267\n",
            "2025-12-23 02:52:27.882565: train_loss -0.9758\n",
            "2025-12-23 02:52:27.882761: val_loss -0.9867\n",
            "2025-12-23 02:52:27.882862: Pseudo dice [np.float32(0.9898)]\n",
            "2025-12-23 02:52:27.882953: Epoch time: 22.41 s\n",
            "2025-12-23 02:52:27.883032: Yayy! New best EMA pseudo Dice: 0.9894000291824341\n",
            "2025-12-23 02:52:29.819110: \n",
            "2025-12-23 02:52:29.819520: Epoch 770\n",
            "2025-12-23 02:52:29.819684: Current learning rate: 0.00266\n",
            "2025-12-23 02:52:52.278589: train_loss -0.9747\n",
            "2025-12-23 02:52:52.278864: val_loss -0.9853\n",
            "2025-12-23 02:52:52.278963: Pseudo dice [np.float32(0.9895)]\n",
            "2025-12-23 02:52:52.279053: Epoch time: 22.46 s\n",
            "2025-12-23 02:52:52.279127: Yayy! New best EMA pseudo Dice: 0.9894000291824341\n",
            "2025-12-23 02:52:54.208122: \n",
            "2025-12-23 02:52:54.208310: Epoch 771\n",
            "2025-12-23 02:52:54.208435: Current learning rate: 0.00265\n",
            "2025-12-23 02:53:16.696202: train_loss -0.9745\n",
            "2025-12-23 02:53:16.696560: val_loss -0.9857\n",
            "2025-12-23 02:53:16.696784: Pseudo dice [np.float32(0.9889)]\n",
            "2025-12-23 02:53:16.696929: Epoch time: 22.49 s\n",
            "2025-12-23 02:53:18.104336: \n",
            "2025-12-23 02:53:18.104700: Epoch 772\n",
            "2025-12-23 02:53:18.104854: Current learning rate: 0.00264\n",
            "2025-12-23 02:53:40.511417: train_loss -0.9754\n",
            "2025-12-23 02:53:40.511783: val_loss -0.9861\n",
            "2025-12-23 02:53:40.511890: Pseudo dice [np.float32(0.9895)]\n",
            "2025-12-23 02:53:40.511982: Epoch time: 22.41 s\n",
            "2025-12-23 02:53:41.938605: \n",
            "2025-12-23 02:53:41.938877: Epoch 773\n",
            "2025-12-23 02:53:41.939037: Current learning rate: 0.00263\n",
            "2025-12-23 02:54:04.378117: train_loss -0.9744\n",
            "2025-12-23 02:54:04.378472: val_loss -0.9872\n",
            "2025-12-23 02:54:04.378602: Pseudo dice [np.float32(0.9901)]\n",
            "2025-12-23 02:54:04.378736: Epoch time: 22.44 s\n",
            "2025-12-23 02:54:04.378830: Yayy! New best EMA pseudo Dice: 0.9894000291824341\n",
            "2025-12-23 02:54:06.384733: \n",
            "2025-12-23 02:54:06.384898: Epoch 774\n",
            "2025-12-23 02:54:06.385030: Current learning rate: 0.00262\n",
            "2025-12-23 02:54:28.811643: train_loss -0.9746\n",
            "2025-12-23 02:54:28.811990: val_loss -0.9858\n",
            "2025-12-23 02:54:28.812105: Pseudo dice [np.float32(0.989)]\n",
            "2025-12-23 02:54:28.812245: Epoch time: 22.43 s\n",
            "2025-12-23 02:54:30.222335: \n",
            "2025-12-23 02:54:30.222606: Epoch 775\n",
            "2025-12-23 02:54:30.222749: Current learning rate: 0.00261\n",
            "2025-12-23 02:54:52.628653: train_loss -0.975\n",
            "2025-12-23 02:54:52.628884: val_loss -0.9866\n",
            "2025-12-23 02:54:52.628983: Pseudo dice [np.float32(0.9895)]\n",
            "2025-12-23 02:54:52.629080: Epoch time: 22.41 s\n",
            "2025-12-23 02:54:54.057827: \n",
            "2025-12-23 02:54:54.058128: Epoch 776\n",
            "2025-12-23 02:54:54.058286: Current learning rate: 0.0026\n",
            "2025-12-23 02:55:16.432595: train_loss -0.9739\n",
            "2025-12-23 02:55:16.432881: val_loss -0.9863\n",
            "2025-12-23 02:55:16.432973: Pseudo dice [np.float32(0.9893)]\n",
            "2025-12-23 02:55:16.433064: Epoch time: 22.38 s\n",
            "2025-12-23 02:55:17.843143: \n",
            "2025-12-23 02:55:17.843408: Epoch 777\n",
            "2025-12-23 02:55:17.843544: Current learning rate: 0.00259\n",
            "2025-12-23 02:55:40.262317: train_loss -0.9752\n",
            "2025-12-23 02:55:40.262529: val_loss -0.9844\n",
            "2025-12-23 02:55:40.262732: Pseudo dice [np.float32(0.989)]\n",
            "2025-12-23 02:55:40.262929: Epoch time: 22.42 s\n",
            "2025-12-23 02:55:41.664897: \n",
            "2025-12-23 02:55:41.665209: Epoch 778\n",
            "2025-12-23 02:55:41.665381: Current learning rate: 0.00258\n",
            "2025-12-23 02:56:04.038917: train_loss -0.9746\n",
            "2025-12-23 02:56:04.039248: val_loss -0.9855\n",
            "2025-12-23 02:56:04.039361: Pseudo dice [np.float32(0.989)]\n",
            "2025-12-23 02:56:04.039453: Epoch time: 22.38 s\n",
            "2025-12-23 02:56:06.136951: \n",
            "2025-12-23 02:56:06.137266: Epoch 779\n",
            "2025-12-23 02:56:06.137418: Current learning rate: 0.00257\n",
            "2025-12-23 02:56:28.630620: train_loss -0.9751\n",
            "2025-12-23 02:56:28.630823: val_loss -0.9864\n",
            "2025-12-23 02:56:28.630914: Pseudo dice [np.float32(0.9892)]\n",
            "2025-12-23 02:56:28.631002: Epoch time: 22.49 s\n",
            "2025-12-23 02:56:30.019554: \n",
            "2025-12-23 02:56:30.019951: Epoch 780\n",
            "2025-12-23 02:56:30.020088: Current learning rate: 0.00256\n",
            "2025-12-23 02:56:52.465117: train_loss -0.9755\n",
            "2025-12-23 02:56:52.465554: val_loss -0.9858\n",
            "2025-12-23 02:56:52.465737: Pseudo dice [np.float32(0.9894)]\n",
            "2025-12-23 02:56:52.465899: Epoch time: 22.45 s\n",
            "2025-12-23 02:56:53.857498: \n",
            "2025-12-23 02:56:53.857991: Epoch 781\n",
            "2025-12-23 02:56:53.858146: Current learning rate: 0.00255\n",
            "2025-12-23 02:57:16.337826: train_loss -0.9754\n",
            "2025-12-23 02:57:16.338116: val_loss -0.9864\n",
            "2025-12-23 02:57:16.338212: Pseudo dice [np.float32(0.9894)]\n",
            "2025-12-23 02:57:16.338372: Epoch time: 22.48 s\n",
            "2025-12-23 02:57:17.709503: \n",
            "2025-12-23 02:57:17.709766: Epoch 782\n",
            "2025-12-23 02:57:17.709948: Current learning rate: 0.00254\n",
            "2025-12-23 02:57:40.156113: train_loss -0.9754\n",
            "2025-12-23 02:57:40.156453: val_loss -0.986\n",
            "2025-12-23 02:57:40.156567: Pseudo dice [np.float32(0.9902)]\n",
            "2025-12-23 02:57:40.156662: Epoch time: 22.45 s\n",
            "2025-12-23 02:57:41.541626: \n",
            "2025-12-23 02:57:41.542032: Epoch 783\n",
            "2025-12-23 02:57:41.542181: Current learning rate: 0.00253\n",
            "2025-12-23 02:58:03.977603: train_loss -0.9756\n",
            "2025-12-23 02:58:03.977873: val_loss -0.9854\n",
            "2025-12-23 02:58:03.977966: Pseudo dice [np.float32(0.989)]\n",
            "2025-12-23 02:58:03.978055: Epoch time: 22.44 s\n",
            "2025-12-23 02:58:05.444475: \n",
            "2025-12-23 02:58:05.444736: Epoch 784\n",
            "2025-12-23 02:58:05.444881: Current learning rate: 0.00252\n",
            "2025-12-23 02:58:27.865215: train_loss -0.9744\n",
            "2025-12-23 02:58:27.865490: val_loss -0.9871\n",
            "2025-12-23 02:58:27.865585: Pseudo dice [np.float32(0.9903)]\n",
            "2025-12-23 02:58:27.865674: Epoch time: 22.42 s\n",
            "2025-12-23 02:58:27.865759: Yayy! New best EMA pseudo Dice: 0.9894999861717224\n",
            "2025-12-23 02:58:29.800914: \n",
            "2025-12-23 02:58:29.801250: Epoch 785\n",
            "2025-12-23 02:58:29.801388: Current learning rate: 0.00251\n",
            "2025-12-23 02:58:52.301774: train_loss -0.975\n",
            "2025-12-23 02:58:52.302027: val_loss -0.9847\n",
            "2025-12-23 02:58:52.302119: Pseudo dice [np.float32(0.9881)]\n",
            "2025-12-23 02:58:52.302211: Epoch time: 22.5 s\n",
            "2025-12-23 02:58:53.691624: \n",
            "2025-12-23 02:58:53.692054: Epoch 786\n",
            "2025-12-23 02:58:53.692211: Current learning rate: 0.0025\n",
            "2025-12-23 02:59:16.146399: train_loss -0.9735\n",
            "2025-12-23 02:59:16.146653: val_loss -0.9853\n",
            "2025-12-23 02:59:16.146772: Pseudo dice [np.float32(0.9895)]\n",
            "2025-12-23 02:59:16.146947: Epoch time: 22.46 s\n",
            "2025-12-23 02:59:17.536092: \n",
            "2025-12-23 02:59:17.536438: Epoch 787\n",
            "2025-12-23 02:59:17.536603: Current learning rate: 0.00249\n",
            "2025-12-23 02:59:40.068039: train_loss -0.9757\n",
            "2025-12-23 02:59:40.068289: val_loss -0.9857\n",
            "2025-12-23 02:59:40.068397: Pseudo dice [np.float32(0.99)]\n",
            "2025-12-23 02:59:40.068486: Epoch time: 22.53 s\n",
            "2025-12-23 02:59:41.448391: \n",
            "2025-12-23 02:59:41.448690: Epoch 788\n",
            "2025-12-23 02:59:41.448821: Current learning rate: 0.00248\n",
            "2025-12-23 03:00:03.867147: train_loss -0.9757\n",
            "2025-12-23 03:00:03.867380: val_loss -0.9859\n",
            "2025-12-23 03:00:03.867487: Pseudo dice [np.float32(0.9892)]\n",
            "2025-12-23 03:00:03.867579: Epoch time: 22.42 s\n",
            "2025-12-23 03:00:05.267064: \n",
            "2025-12-23 03:00:05.267400: Epoch 789\n",
            "2025-12-23 03:00:05.267541: Current learning rate: 0.00247\n",
            "2025-12-23 03:00:27.683439: train_loss -0.9758\n",
            "2025-12-23 03:00:27.683661: val_loss -0.9863\n",
            "2025-12-23 03:00:27.683749: Pseudo dice [np.float32(0.9895)]\n",
            "2025-12-23 03:00:27.683851: Epoch time: 22.42 s\n",
            "2025-12-23 03:00:29.061798: \n",
            "2025-12-23 03:00:29.062108: Epoch 790\n",
            "2025-12-23 03:00:29.062258: Current learning rate: 0.00245\n",
            "2025-12-23 03:00:51.521813: train_loss -0.9753\n",
            "2025-12-23 03:00:51.522012: val_loss -0.9859\n",
            "2025-12-23 03:00:51.522100: Pseudo dice [np.float32(0.9899)]\n",
            "2025-12-23 03:00:51.522404: Epoch time: 22.46 s\n",
            "2025-12-23 03:00:52.876337: \n",
            "2025-12-23 03:00:52.876592: Epoch 791\n",
            "2025-12-23 03:00:52.876721: Current learning rate: 0.00244\n",
            "2025-12-23 03:01:15.288598: train_loss -0.9764\n",
            "2025-12-23 03:01:15.289077: val_loss -0.9857\n",
            "2025-12-23 03:01:15.289255: Pseudo dice [np.float32(0.9886)]\n",
            "2025-12-23 03:01:15.289403: Epoch time: 22.41 s\n",
            "2025-12-23 03:01:16.707919: \n",
            "2025-12-23 03:01:16.708145: Epoch 792\n",
            "2025-12-23 03:01:16.708358: Current learning rate: 0.00243\n",
            "2025-12-23 03:01:39.129477: train_loss -0.975\n",
            "2025-12-23 03:01:39.129746: val_loss -0.9864\n",
            "2025-12-23 03:01:39.129896: Pseudo dice [np.float32(0.9892)]\n",
            "2025-12-23 03:01:39.129991: Epoch time: 22.42 s\n",
            "2025-12-23 03:01:40.534239: \n",
            "2025-12-23 03:01:40.534464: Epoch 793\n",
            "2025-12-23 03:01:40.534633: Current learning rate: 0.00242\n",
            "2025-12-23 03:02:02.990040: train_loss -0.9755\n",
            "2025-12-23 03:02:02.990275: val_loss -0.9855\n",
            "2025-12-23 03:02:02.990388: Pseudo dice [np.float32(0.9891)]\n",
            "2025-12-23 03:02:02.990502: Epoch time: 22.46 s\n",
            "2025-12-23 03:02:04.408195: \n",
            "2025-12-23 03:02:04.408387: Epoch 794\n",
            "2025-12-23 03:02:04.408514: Current learning rate: 0.00241\n",
            "2025-12-23 03:02:26.836315: train_loss -0.9757\n",
            "2025-12-23 03:02:26.836593: val_loss -0.9865\n",
            "2025-12-23 03:02:26.836808: Pseudo dice [np.float32(0.9895)]\n",
            "2025-12-23 03:02:26.836954: Epoch time: 22.43 s\n",
            "2025-12-23 03:02:28.239053: \n",
            "2025-12-23 03:02:28.239469: Epoch 795\n",
            "2025-12-23 03:02:28.239609: Current learning rate: 0.0024\n",
            "2025-12-23 03:02:50.625712: train_loss -0.9733\n",
            "2025-12-23 03:02:50.625931: val_loss -0.9845\n",
            "2025-12-23 03:02:50.626054: Pseudo dice [np.float32(0.9879)]\n",
            "2025-12-23 03:02:50.626165: Epoch time: 22.39 s\n",
            "2025-12-23 03:02:52.705607: \n",
            "2025-12-23 03:02:52.705809: Epoch 796\n",
            "2025-12-23 03:02:52.705952: Current learning rate: 0.00239\n",
            "2025-12-23 03:03:15.262695: train_loss -0.9732\n",
            "2025-12-23 03:03:15.262901: val_loss -0.9845\n",
            "2025-12-23 03:03:15.262992: Pseudo dice [np.float32(0.9867)]\n",
            "2025-12-23 03:03:15.263105: Epoch time: 22.56 s\n",
            "2025-12-23 03:03:16.644208: \n",
            "2025-12-23 03:03:16.644563: Epoch 797\n",
            "2025-12-23 03:03:16.644722: Current learning rate: 0.00238\n",
            "2025-12-23 03:03:39.141664: train_loss -0.974\n",
            "2025-12-23 03:03:39.142195: val_loss -0.9852\n",
            "2025-12-23 03:03:39.142353: Pseudo dice [np.float32(0.9889)]\n",
            "2025-12-23 03:03:39.142447: Epoch time: 22.5 s\n",
            "2025-12-23 03:03:40.511355: \n",
            "2025-12-23 03:03:40.511596: Epoch 798\n",
            "2025-12-23 03:03:40.511779: Current learning rate: 0.00237\n",
            "2025-12-23 03:04:02.997642: train_loss -0.9754\n",
            "2025-12-23 03:04:02.997935: val_loss -0.9852\n",
            "2025-12-23 03:04:02.998040: Pseudo dice [np.float32(0.9887)]\n",
            "2025-12-23 03:04:02.998133: Epoch time: 22.49 s\n",
            "2025-12-23 03:04:04.370447: \n",
            "2025-12-23 03:04:04.370736: Epoch 799\n",
            "2025-12-23 03:04:04.370873: Current learning rate: 0.00236\n",
            "2025-12-23 03:04:26.862057: train_loss -0.9746\n",
            "2025-12-23 03:04:26.862282: val_loss -0.9865\n",
            "2025-12-23 03:04:26.862399: Pseudo dice [np.float32(0.9902)]\n",
            "2025-12-23 03:04:26.862535: Epoch time: 22.49 s\n",
            "2025-12-23 03:04:28.735677: \n",
            "2025-12-23 03:04:28.735976: Epoch 800\n",
            "2025-12-23 03:04:28.736120: Current learning rate: 0.00235\n",
            "2025-12-23 03:04:51.207877: train_loss -0.9762\n",
            "2025-12-23 03:04:51.208071: val_loss -0.9861\n",
            "2025-12-23 03:04:51.208154: Pseudo dice [np.float32(0.9896)]\n",
            "2025-12-23 03:04:51.208269: Epoch time: 22.47 s\n",
            "2025-12-23 03:04:52.572234: \n",
            "2025-12-23 03:04:52.572444: Epoch 801\n",
            "2025-12-23 03:04:52.572599: Current learning rate: 0.00234\n",
            "2025-12-23 03:05:15.029118: train_loss -0.9749\n",
            "2025-12-23 03:05:15.029339: val_loss -0.9858\n",
            "2025-12-23 03:05:15.029480: Pseudo dice [np.float32(0.9893)]\n",
            "2025-12-23 03:05:15.029619: Epoch time: 22.46 s\n",
            "2025-12-23 03:05:16.409405: \n",
            "2025-12-23 03:05:16.409621: Epoch 802\n",
            "2025-12-23 03:05:16.409817: Current learning rate: 0.00233\n",
            "2025-12-23 03:05:38.871611: train_loss -0.9761\n",
            "2025-12-23 03:05:38.871895: val_loss -0.9855\n",
            "2025-12-23 03:05:38.871989: Pseudo dice [np.float32(0.9898)]\n",
            "2025-12-23 03:05:38.872079: Epoch time: 22.46 s\n",
            "2025-12-23 03:05:40.258335: \n",
            "2025-12-23 03:05:40.258610: Epoch 803\n",
            "2025-12-23 03:05:40.258752: Current learning rate: 0.00232\n",
            "2025-12-23 03:06:02.723114: train_loss -0.9752\n",
            "2025-12-23 03:06:02.723540: val_loss -0.9861\n",
            "2025-12-23 03:06:02.723709: Pseudo dice [np.float32(0.9898)]\n",
            "2025-12-23 03:06:02.723854: Epoch time: 22.47 s\n",
            "2025-12-23 03:06:04.132888: \n",
            "2025-12-23 03:06:04.133113: Epoch 804\n",
            "2025-12-23 03:06:04.133279: Current learning rate: 0.00231\n",
            "2025-12-23 03:06:26.596115: train_loss -0.9753\n",
            "2025-12-23 03:06:26.596349: val_loss -0.9849\n",
            "2025-12-23 03:06:26.596444: Pseudo dice [np.float32(0.9885)]\n",
            "2025-12-23 03:06:26.596691: Epoch time: 22.46 s\n",
            "2025-12-23 03:06:28.003549: \n",
            "2025-12-23 03:06:28.003789: Epoch 805\n",
            "2025-12-23 03:06:28.003931: Current learning rate: 0.0023\n",
            "2025-12-23 03:06:50.399643: train_loss -0.9755\n",
            "2025-12-23 03:06:50.399845: val_loss -0.9861\n",
            "2025-12-23 03:06:50.399959: Pseudo dice [np.float32(0.9894)]\n",
            "2025-12-23 03:06:50.400115: Epoch time: 22.4 s\n",
            "2025-12-23 03:06:51.770486: \n",
            "2025-12-23 03:06:51.770662: Epoch 806\n",
            "2025-12-23 03:06:51.770785: Current learning rate: 0.00229\n",
            "2025-12-23 03:07:14.166586: train_loss -0.9747\n",
            "2025-12-23 03:07:14.166854: val_loss -0.9865\n",
            "2025-12-23 03:07:14.166957: Pseudo dice [np.float32(0.9891)]\n",
            "2025-12-23 03:07:14.167049: Epoch time: 22.4 s\n",
            "2025-12-23 03:07:15.556787: \n",
            "2025-12-23 03:07:15.557064: Epoch 807\n",
            "2025-12-23 03:07:15.557201: Current learning rate: 0.00228\n",
            "2025-12-23 03:07:38.017871: train_loss -0.9759\n",
            "2025-12-23 03:07:38.018085: val_loss -0.9861\n",
            "2025-12-23 03:07:38.018171: Pseudo dice [np.float32(0.9894)]\n",
            "2025-12-23 03:07:38.018274: Epoch time: 22.46 s\n",
            "2025-12-23 03:07:39.403698: \n",
            "2025-12-23 03:07:39.403982: Epoch 808\n",
            "2025-12-23 03:07:39.404133: Current learning rate: 0.00226\n",
            "2025-12-23 03:08:01.828974: train_loss -0.975\n",
            "2025-12-23 03:08:01.829199: val_loss -0.9856\n",
            "2025-12-23 03:08:01.829350: Pseudo dice [np.float32(0.9893)]\n",
            "2025-12-23 03:08:01.829464: Epoch time: 22.43 s\n",
            "2025-12-23 03:08:03.250936: \n",
            "2025-12-23 03:08:03.251143: Epoch 809\n",
            "2025-12-23 03:08:03.251324: Current learning rate: 0.00225\n",
            "2025-12-23 03:08:25.701694: train_loss -0.9751\n",
            "2025-12-23 03:08:25.701917: val_loss -0.9852\n",
            "2025-12-23 03:08:25.702007: Pseudo dice [np.float32(0.9882)]\n",
            "2025-12-23 03:08:25.702096: Epoch time: 22.45 s\n",
            "2025-12-23 03:08:27.095562: \n",
            "2025-12-23 03:08:27.095822: Epoch 810\n",
            "2025-12-23 03:08:27.095961: Current learning rate: 0.00224\n",
            "2025-12-23 03:08:49.519995: train_loss -0.9752\n",
            "2025-12-23 03:08:49.520273: val_loss -0.9868\n",
            "2025-12-23 03:08:49.520440: Pseudo dice [np.float32(0.9897)]\n",
            "2025-12-23 03:08:49.520539: Epoch time: 22.43 s\n",
            "2025-12-23 03:08:50.907452: \n",
            "2025-12-23 03:08:50.907659: Epoch 811\n",
            "2025-12-23 03:08:50.907792: Current learning rate: 0.00223\n",
            "2025-12-23 03:09:13.356110: train_loss -0.9758\n",
            "2025-12-23 03:09:13.356380: val_loss -0.9871\n",
            "2025-12-23 03:09:13.356487: Pseudo dice [np.float32(0.9907)]\n",
            "2025-12-23 03:09:13.356577: Epoch time: 22.45 s\n",
            "2025-12-23 03:09:14.756770: \n",
            "2025-12-23 03:09:14.756924: Epoch 812\n",
            "2025-12-23 03:09:14.757053: Current learning rate: 0.00222\n",
            "2025-12-23 03:09:37.162788: train_loss -0.9744\n",
            "2025-12-23 03:09:37.163028: val_loss -0.9854\n",
            "2025-12-23 03:09:37.163185: Pseudo dice [np.float32(0.9894)]\n",
            "2025-12-23 03:09:37.163386: Epoch time: 22.41 s\n",
            "2025-12-23 03:09:39.285626: \n",
            "2025-12-23 03:09:39.285909: Epoch 813\n",
            "2025-12-23 03:09:39.286054: Current learning rate: 0.00221\n",
            "2025-12-23 03:10:01.820297: train_loss -0.9759\n",
            "2025-12-23 03:10:01.820568: val_loss -0.985\n",
            "2025-12-23 03:10:01.820665: Pseudo dice [np.float32(0.9892)]\n",
            "2025-12-23 03:10:01.820753: Epoch time: 22.54 s\n",
            "2025-12-23 03:10:03.201566: \n",
            "2025-12-23 03:10:03.201907: Epoch 814\n",
            "2025-12-23 03:10:03.202048: Current learning rate: 0.0022\n",
            "2025-12-23 03:10:25.636400: train_loss -0.976\n",
            "2025-12-23 03:10:25.636715: val_loss -0.9873\n",
            "2025-12-23 03:10:25.636828: Pseudo dice [np.float32(0.9905)]\n",
            "2025-12-23 03:10:25.636953: Epoch time: 22.44 s\n",
            "2025-12-23 03:10:27.040372: \n",
            "2025-12-23 03:10:27.040598: Epoch 815\n",
            "2025-12-23 03:10:27.040728: Current learning rate: 0.00219\n",
            "2025-12-23 03:10:49.557299: train_loss -0.9757\n",
            "2025-12-23 03:10:49.557574: val_loss -0.9865\n",
            "2025-12-23 03:10:49.557707: Pseudo dice [np.float32(0.9892)]\n",
            "2025-12-23 03:10:49.557829: Epoch time: 22.52 s\n",
            "2025-12-23 03:10:50.944294: \n",
            "2025-12-23 03:10:50.944650: Epoch 816\n",
            "2025-12-23 03:10:50.944806: Current learning rate: 0.00218\n",
            "2025-12-23 03:11:13.444422: train_loss -0.9763\n",
            "2025-12-23 03:11:13.444685: val_loss -0.9865\n",
            "2025-12-23 03:11:13.444791: Pseudo dice [np.float32(0.9902)]\n",
            "2025-12-23 03:11:13.444886: Epoch time: 22.5 s\n",
            "2025-12-23 03:11:13.444962: Yayy! New best EMA pseudo Dice: 0.9894999861717224\n",
            "2025-12-23 03:11:15.419553: \n",
            "2025-12-23 03:11:15.419883: Epoch 817\n",
            "2025-12-23 03:11:15.420018: Current learning rate: 0.00217\n",
            "2025-12-23 03:11:37.953794: train_loss -0.9757\n",
            "2025-12-23 03:11:37.954022: val_loss -0.9862\n",
            "2025-12-23 03:11:37.954145: Pseudo dice [np.float32(0.989)]\n",
            "2025-12-23 03:11:37.954272: Epoch time: 22.54 s\n",
            "2025-12-23 03:11:39.391888: \n",
            "2025-12-23 03:11:39.392082: Epoch 818\n",
            "2025-12-23 03:11:39.392209: Current learning rate: 0.00216\n",
            "2025-12-23 03:12:01.853397: train_loss -0.9755\n",
            "2025-12-23 03:12:01.853676: val_loss -0.9864\n",
            "2025-12-23 03:12:01.853781: Pseudo dice [np.float32(0.9894)]\n",
            "2025-12-23 03:12:01.853875: Epoch time: 22.46 s\n",
            "2025-12-23 03:12:03.271494: \n",
            "2025-12-23 03:12:03.271698: Epoch 819\n",
            "2025-12-23 03:12:03.271831: Current learning rate: 0.00215\n",
            "2025-12-23 03:12:25.707425: train_loss -0.9755\n",
            "2025-12-23 03:12:25.707681: val_loss -0.9856\n",
            "2025-12-23 03:12:25.707839: Pseudo dice [np.float32(0.9886)]\n",
            "2025-12-23 03:12:25.708012: Epoch time: 22.44 s\n",
            "2025-12-23 03:12:27.094887: \n",
            "2025-12-23 03:12:27.095288: Epoch 820\n",
            "2025-12-23 03:12:27.095435: Current learning rate: 0.00214\n",
            "2025-12-23 03:12:49.578521: train_loss -0.9755\n",
            "2025-12-23 03:12:49.578734: val_loss -0.9859\n",
            "2025-12-23 03:12:49.578855: Pseudo dice [np.float32(0.9888)]\n",
            "2025-12-23 03:12:49.578969: Epoch time: 22.48 s\n",
            "2025-12-23 03:12:50.965872: \n",
            "2025-12-23 03:12:50.966285: Epoch 821\n",
            "2025-12-23 03:12:50.966461: Current learning rate: 0.00213\n",
            "2025-12-23 03:13:13.455010: train_loss -0.975\n",
            "2025-12-23 03:13:13.455455: val_loss -0.9856\n",
            "2025-12-23 03:13:13.455614: Pseudo dice [np.float32(0.9884)]\n",
            "2025-12-23 03:13:13.455721: Epoch time: 22.49 s\n",
            "2025-12-23 03:13:14.812700: \n",
            "2025-12-23 03:13:14.813027: Epoch 822\n",
            "2025-12-23 03:13:14.813166: Current learning rate: 0.00212\n",
            "2025-12-23 03:13:37.284304: train_loss -0.9757\n",
            "2025-12-23 03:13:37.284580: val_loss -0.9867\n",
            "2025-12-23 03:13:37.284682: Pseudo dice [np.float32(0.9892)]\n",
            "2025-12-23 03:13:37.284790: Epoch time: 22.47 s\n",
            "2025-12-23 03:13:38.666494: \n",
            "2025-12-23 03:13:38.666803: Epoch 823\n",
            "2025-12-23 03:13:38.666936: Current learning rate: 0.0021\n",
            "2025-12-23 03:14:01.070416: train_loss -0.9753\n",
            "2025-12-23 03:14:01.070661: val_loss -0.9861\n",
            "2025-12-23 03:14:01.070777: Pseudo dice [np.float32(0.9893)]\n",
            "2025-12-23 03:14:01.070894: Epoch time: 22.41 s\n",
            "2025-12-23 03:14:02.426762: \n",
            "2025-12-23 03:14:02.427050: Epoch 824\n",
            "2025-12-23 03:14:02.427243: Current learning rate: 0.00209\n",
            "2025-12-23 03:14:24.918479: train_loss -0.9755\n",
            "2025-12-23 03:14:24.918785: val_loss -0.9861\n",
            "2025-12-23 03:14:24.918914: Pseudo dice [np.float32(0.9899)]\n",
            "2025-12-23 03:14:24.919103: Epoch time: 22.49 s\n",
            "2025-12-23 03:14:26.262886: \n",
            "2025-12-23 03:14:26.263195: Epoch 825\n",
            "2025-12-23 03:14:26.263352: Current learning rate: 0.00208\n",
            "2025-12-23 03:14:48.697953: train_loss -0.9759\n",
            "2025-12-23 03:14:48.698161: val_loss -0.9852\n",
            "2025-12-23 03:14:48.698274: Pseudo dice [np.float32(0.9893)]\n",
            "2025-12-23 03:14:48.698398: Epoch time: 22.44 s\n",
            "2025-12-23 03:14:50.056593: \n",
            "2025-12-23 03:14:50.056864: Epoch 826\n",
            "2025-12-23 03:14:50.057000: Current learning rate: 0.00207\n",
            "2025-12-23 03:15:12.535506: train_loss -0.9751\n",
            "2025-12-23 03:15:12.535740: val_loss -0.9865\n",
            "2025-12-23 03:15:12.535839: Pseudo dice [np.float32(0.9893)]\n",
            "2025-12-23 03:15:12.535931: Epoch time: 22.48 s\n",
            "2025-12-23 03:15:13.874207: \n",
            "2025-12-23 03:15:13.874399: Epoch 827\n",
            "2025-12-23 03:15:13.874603: Current learning rate: 0.00206\n",
            "2025-12-23 03:15:36.294476: train_loss -0.9763\n",
            "2025-12-23 03:15:36.294762: val_loss -0.985\n",
            "2025-12-23 03:15:36.294861: Pseudo dice [np.float32(0.9889)]\n",
            "2025-12-23 03:15:36.294955: Epoch time: 22.42 s\n",
            "2025-12-23 03:15:37.625088: \n",
            "2025-12-23 03:15:37.625397: Epoch 828\n",
            "2025-12-23 03:15:37.625536: Current learning rate: 0.00205\n",
            "2025-12-23 03:16:00.006803: train_loss -0.9758\n",
            "2025-12-23 03:16:00.007074: val_loss -0.9871\n",
            "2025-12-23 03:16:00.007212: Pseudo dice [np.float32(0.9898)]\n",
            "2025-12-23 03:16:00.007350: Epoch time: 22.38 s\n",
            "2025-12-23 03:16:01.338500: \n",
            "2025-12-23 03:16:01.338735: Epoch 829\n",
            "2025-12-23 03:16:01.338944: Current learning rate: 0.00204\n",
            "2025-12-23 03:16:23.766446: train_loss -0.9755\n",
            "2025-12-23 03:16:23.766814: val_loss -0.9858\n",
            "2025-12-23 03:16:23.766915: Pseudo dice [np.float32(0.9887)]\n",
            "2025-12-23 03:16:23.767007: Epoch time: 22.43 s\n",
            "2025-12-23 03:16:25.775966: \n",
            "2025-12-23 03:16:25.776232: Epoch 830\n",
            "2025-12-23 03:16:25.776381: Current learning rate: 0.00203\n",
            "2025-12-23 03:16:48.302907: train_loss -0.9769\n",
            "2025-12-23 03:16:48.303186: val_loss -0.9863\n",
            "2025-12-23 03:16:48.303367: Pseudo dice [np.float32(0.9896)]\n",
            "2025-12-23 03:16:48.303511: Epoch time: 22.53 s\n",
            "2025-12-23 03:16:49.635587: \n",
            "2025-12-23 03:16:49.635912: Epoch 831\n",
            "2025-12-23 03:16:49.636049: Current learning rate: 0.00202\n",
            "2025-12-23 03:17:12.097346: train_loss -0.9754\n",
            "2025-12-23 03:17:12.097546: val_loss -0.9857\n",
            "2025-12-23 03:17:12.097640: Pseudo dice [np.float32(0.9892)]\n",
            "2025-12-23 03:17:12.097742: Epoch time: 22.46 s\n",
            "2025-12-23 03:17:13.426065: \n",
            "2025-12-23 03:17:13.426402: Epoch 832\n",
            "2025-12-23 03:17:13.426608: Current learning rate: 0.00201\n",
            "2025-12-23 03:17:35.930909: train_loss -0.9756\n",
            "2025-12-23 03:17:35.931142: val_loss -0.9854\n",
            "2025-12-23 03:17:35.931261: Pseudo dice [np.float32(0.9888)]\n",
            "2025-12-23 03:17:35.931369: Epoch time: 22.51 s\n",
            "2025-12-23 03:17:37.235564: \n",
            "2025-12-23 03:17:37.235830: Epoch 833\n",
            "2025-12-23 03:17:37.235959: Current learning rate: 0.002\n",
            "2025-12-23 03:17:59.724079: train_loss -0.9754\n",
            "2025-12-23 03:17:59.724306: val_loss -0.9851\n",
            "2025-12-23 03:17:59.724440: Pseudo dice [np.float32(0.9885)]\n",
            "2025-12-23 03:17:59.724775: Epoch time: 22.49 s\n",
            "2025-12-23 03:18:01.032849: \n",
            "2025-12-23 03:18:01.033159: Epoch 834\n",
            "2025-12-23 03:18:01.033300: Current learning rate: 0.00199\n",
            "2025-12-23 03:18:23.517077: train_loss -0.9754\n",
            "2025-12-23 03:18:23.517413: val_loss -0.9861\n",
            "2025-12-23 03:18:23.517592: Pseudo dice [np.float32(0.9894)]\n",
            "2025-12-23 03:18:23.517702: Epoch time: 22.49 s\n",
            "2025-12-23 03:18:24.856900: \n",
            "2025-12-23 03:18:24.857242: Epoch 835\n",
            "2025-12-23 03:18:24.857477: Current learning rate: 0.00198\n",
            "2025-12-23 03:18:47.322500: train_loss -0.9768\n",
            "2025-12-23 03:18:47.322768: val_loss -0.9867\n",
            "2025-12-23 03:18:47.322862: Pseudo dice [np.float32(0.9895)]\n",
            "2025-12-23 03:18:47.322953: Epoch time: 22.47 s\n",
            "2025-12-23 03:18:48.667946: \n",
            "2025-12-23 03:18:48.668229: Epoch 836\n",
            "2025-12-23 03:18:48.668386: Current learning rate: 0.00196\n",
            "2025-12-23 03:19:11.155122: train_loss -0.9754\n",
            "2025-12-23 03:19:11.155541: val_loss -0.986\n",
            "2025-12-23 03:19:11.155641: Pseudo dice [np.float32(0.9889)]\n",
            "2025-12-23 03:19:11.155741: Epoch time: 22.49 s\n",
            "2025-12-23 03:19:12.500196: \n",
            "2025-12-23 03:19:12.500382: Epoch 837\n",
            "2025-12-23 03:19:12.500571: Current learning rate: 0.00195\n",
            "2025-12-23 03:19:34.959251: train_loss -0.9758\n",
            "2025-12-23 03:19:34.959473: val_loss -0.9863\n",
            "2025-12-23 03:19:34.959565: Pseudo dice [np.float32(0.9885)]\n",
            "2025-12-23 03:19:34.959657: Epoch time: 22.46 s\n",
            "2025-12-23 03:19:36.291757: \n",
            "2025-12-23 03:19:36.292035: Epoch 838\n",
            "2025-12-23 03:19:36.292166: Current learning rate: 0.00194\n",
            "2025-12-23 03:19:58.783748: train_loss -0.9765\n",
            "2025-12-23 03:19:58.783996: val_loss -0.9872\n",
            "2025-12-23 03:19:58.784086: Pseudo dice [np.float32(0.9899)]\n",
            "2025-12-23 03:19:58.784176: Epoch time: 22.49 s\n",
            "2025-12-23 03:20:00.122017: \n",
            "2025-12-23 03:20:00.122225: Epoch 839\n",
            "2025-12-23 03:20:00.122385: Current learning rate: 0.00193\n",
            "2025-12-23 03:20:22.568092: train_loss -0.9758\n",
            "2025-12-23 03:20:22.568404: val_loss -0.986\n",
            "2025-12-23 03:20:22.568522: Pseudo dice [np.float32(0.9892)]\n",
            "2025-12-23 03:20:22.568640: Epoch time: 22.45 s\n",
            "2025-12-23 03:20:23.906952: \n",
            "2025-12-23 03:20:23.907243: Epoch 840\n",
            "2025-12-23 03:20:23.907392: Current learning rate: 0.00192\n",
            "2025-12-23 03:20:46.333041: train_loss -0.9761\n",
            "2025-12-23 03:20:46.333346: val_loss -0.9857\n",
            "2025-12-23 03:20:46.333454: Pseudo dice [np.float32(0.9889)]\n",
            "2025-12-23 03:20:46.333545: Epoch time: 22.43 s\n",
            "2025-12-23 03:20:47.677213: \n",
            "2025-12-23 03:20:47.677526: Epoch 841\n",
            "2025-12-23 03:20:47.677657: Current learning rate: 0.00191\n",
            "2025-12-23 03:21:10.105014: train_loss -0.9754\n",
            "2025-12-23 03:21:10.105250: val_loss -0.9864\n",
            "2025-12-23 03:21:10.105420: Pseudo dice [np.float32(0.989)]\n",
            "2025-12-23 03:21:10.105542: Epoch time: 22.43 s\n",
            "2025-12-23 03:21:11.422595: \n",
            "2025-12-23 03:21:11.422855: Epoch 842\n",
            "2025-12-23 03:21:11.423014: Current learning rate: 0.0019\n",
            "2025-12-23 03:21:33.856695: train_loss -0.9754\n",
            "2025-12-23 03:21:33.856889: val_loss -0.9855\n",
            "2025-12-23 03:21:33.856978: Pseudo dice [np.float32(0.9883)]\n",
            "2025-12-23 03:21:33.857068: Epoch time: 22.44 s\n",
            "2025-12-23 03:21:35.171350: \n",
            "2025-12-23 03:21:35.171683: Epoch 843\n",
            "2025-12-23 03:21:35.171864: Current learning rate: 0.00189\n",
            "2025-12-23 03:21:57.654018: train_loss -0.9763\n",
            "2025-12-23 03:21:57.654240: val_loss -0.9856\n",
            "2025-12-23 03:21:57.654393: Pseudo dice [np.float32(0.9889)]\n",
            "2025-12-23 03:21:57.654490: Epoch time: 22.48 s\n",
            "2025-12-23 03:21:58.967597: \n",
            "2025-12-23 03:21:58.967801: Epoch 844\n",
            "2025-12-23 03:21:58.967927: Current learning rate: 0.00188\n",
            "2025-12-23 03:22:21.433213: train_loss -0.9756\n",
            "2025-12-23 03:22:21.433444: val_loss -0.9864\n",
            "2025-12-23 03:22:21.433533: Pseudo dice [np.float32(0.99)]\n",
            "2025-12-23 03:22:21.433628: Epoch time: 22.47 s\n",
            "2025-12-23 03:22:22.771323: \n",
            "2025-12-23 03:22:22.771634: Epoch 845\n",
            "2025-12-23 03:22:22.771804: Current learning rate: 0.00187\n",
            "2025-12-23 03:22:45.191769: train_loss -0.9763\n",
            "2025-12-23 03:22:45.192013: val_loss -0.9865\n",
            "2025-12-23 03:22:45.192107: Pseudo dice [np.float32(0.9893)]\n",
            "2025-12-23 03:22:45.192202: Epoch time: 22.42 s\n",
            "2025-12-23 03:22:46.568443: \n",
            "2025-12-23 03:22:46.568637: Epoch 846\n",
            "2025-12-23 03:22:46.568764: Current learning rate: 0.00186\n",
            "2025-12-23 03:23:09.060656: train_loss -0.976\n",
            "2025-12-23 03:23:09.060869: val_loss -0.9873\n",
            "2025-12-23 03:23:09.060958: Pseudo dice [np.float32(0.9895)]\n",
            "2025-12-23 03:23:09.061050: Epoch time: 22.49 s\n",
            "2025-12-23 03:23:10.405032: \n",
            "2025-12-23 03:23:10.405226: Epoch 847\n",
            "2025-12-23 03:23:10.405359: Current learning rate: 0.00185\n",
            "2025-12-23 03:23:32.848877: train_loss -0.976\n",
            "2025-12-23 03:23:32.849072: val_loss -0.9862\n",
            "2025-12-23 03:23:32.849258: Pseudo dice [np.float32(0.9894)]\n",
            "2025-12-23 03:23:32.849399: Epoch time: 22.45 s\n",
            "2025-12-23 03:23:34.909979: \n",
            "2025-12-23 03:23:34.910158: Epoch 848\n",
            "2025-12-23 03:23:34.910347: Current learning rate: 0.00184\n",
            "2025-12-23 03:23:57.445355: train_loss -0.9761\n",
            "2025-12-23 03:23:57.445577: val_loss -0.9862\n",
            "2025-12-23 03:23:57.445680: Pseudo dice [np.float32(0.9895)]\n",
            "2025-12-23 03:23:57.445772: Epoch time: 22.54 s\n",
            "2025-12-23 03:23:58.763469: \n",
            "2025-12-23 03:23:58.763800: Epoch 849\n",
            "2025-12-23 03:23:58.763933: Current learning rate: 0.00182\n",
            "2025-12-23 03:24:21.203081: train_loss -0.9768\n",
            "2025-12-23 03:24:21.203389: val_loss -0.9869\n",
            "2025-12-23 03:24:21.203503: Pseudo dice [np.float32(0.99)]\n",
            "2025-12-23 03:24:21.203594: Epoch time: 22.44 s\n",
            "2025-12-23 03:24:23.082527: \n",
            "2025-12-23 03:24:23.082936: Epoch 850\n",
            "2025-12-23 03:24:23.083083: Current learning rate: 0.00181\n",
            "2025-12-23 03:24:45.576607: train_loss -0.9771\n",
            "2025-12-23 03:24:45.576837: val_loss -0.9862\n",
            "2025-12-23 03:24:45.576924: Pseudo dice [np.float32(0.9895)]\n",
            "2025-12-23 03:24:45.577012: Epoch time: 22.5 s\n",
            "2025-12-23 03:24:46.884487: \n",
            "2025-12-23 03:24:46.884785: Epoch 851\n",
            "2025-12-23 03:24:46.884984: Current learning rate: 0.0018\n",
            "2025-12-23 03:25:09.373299: train_loss -0.9762\n",
            "2025-12-23 03:25:09.373611: val_loss -0.9856\n",
            "2025-12-23 03:25:09.373939: Pseudo dice [np.float32(0.989)]\n",
            "2025-12-23 03:25:09.374096: Epoch time: 22.49 s\n",
            "2025-12-23 03:25:10.741561: \n",
            "2025-12-23 03:25:10.741914: Epoch 852\n",
            "2025-12-23 03:25:10.742052: Current learning rate: 0.00179\n",
            "2025-12-23 03:25:33.207003: train_loss -0.976\n",
            "2025-12-23 03:25:33.207211: val_loss -0.9873\n",
            "2025-12-23 03:25:33.207326: Pseudo dice [np.float32(0.99)]\n",
            "2025-12-23 03:25:33.207418: Epoch time: 22.47 s\n",
            "2025-12-23 03:25:34.539886: \n",
            "2025-12-23 03:25:34.540101: Epoch 853\n",
            "2025-12-23 03:25:34.540288: Current learning rate: 0.00178\n",
            "2025-12-23 03:25:56.973167: train_loss -0.9756\n",
            "2025-12-23 03:25:56.973471: val_loss -0.9855\n",
            "2025-12-23 03:25:56.973565: Pseudo dice [np.float32(0.9889)]\n",
            "2025-12-23 03:25:56.973679: Epoch time: 22.43 s\n",
            "2025-12-23 03:25:58.348397: \n",
            "2025-12-23 03:25:58.348827: Epoch 854\n",
            "2025-12-23 03:25:58.348974: Current learning rate: 0.00177\n",
            "2025-12-23 03:26:20.814315: train_loss -0.9756\n",
            "2025-12-23 03:26:20.814613: val_loss -0.9861\n",
            "2025-12-23 03:26:20.814710: Pseudo dice [np.float32(0.9893)]\n",
            "2025-12-23 03:26:20.814800: Epoch time: 22.47 s\n",
            "2025-12-23 03:26:22.132644: \n",
            "2025-12-23 03:26:22.133019: Epoch 855\n",
            "2025-12-23 03:26:22.133153: Current learning rate: 0.00176\n",
            "2025-12-23 03:26:44.561352: train_loss -0.9767\n",
            "2025-12-23 03:26:44.561559: val_loss -0.986\n",
            "2025-12-23 03:26:44.561710: Pseudo dice [np.float32(0.9891)]\n",
            "2025-12-23 03:26:44.561957: Epoch time: 22.43 s\n",
            "2025-12-23 03:26:45.912521: \n",
            "2025-12-23 03:26:45.912884: Epoch 856\n",
            "2025-12-23 03:26:45.913035: Current learning rate: 0.00175\n",
            "2025-12-23 03:27:08.330827: train_loss -0.9757\n",
            "2025-12-23 03:27:08.331139: val_loss -0.9854\n",
            "2025-12-23 03:27:08.331266: Pseudo dice [np.float32(0.9897)]\n",
            "2025-12-23 03:27:08.331550: Epoch time: 22.42 s\n",
            "2025-12-23 03:27:09.676093: \n",
            "2025-12-23 03:27:09.676400: Epoch 857\n",
            "2025-12-23 03:27:09.676540: Current learning rate: 0.00174\n",
            "2025-12-23 03:27:32.124420: train_loss -0.9759\n",
            "2025-12-23 03:27:32.124752: val_loss -0.9868\n",
            "2025-12-23 03:27:32.125032: Pseudo dice [np.float32(0.9895)]\n",
            "2025-12-23 03:27:32.125267: Epoch time: 22.45 s\n",
            "2025-12-23 03:27:33.452670: \n",
            "2025-12-23 03:27:33.452982: Epoch 858\n",
            "2025-12-23 03:27:33.453116: Current learning rate: 0.00173\n",
            "2025-12-23 03:27:55.859449: train_loss -0.9764\n",
            "2025-12-23 03:27:55.859718: val_loss -0.9871\n",
            "2025-12-23 03:27:55.859821: Pseudo dice [np.float32(0.9895)]\n",
            "2025-12-23 03:27:55.859921: Epoch time: 22.41 s\n",
            "2025-12-23 03:27:57.186177: \n",
            "2025-12-23 03:27:57.186510: Epoch 859\n",
            "2025-12-23 03:27:57.186668: Current learning rate: 0.00172\n",
            "2025-12-23 03:28:19.576683: train_loss -0.9762\n",
            "2025-12-23 03:28:19.576895: val_loss -0.9869\n",
            "2025-12-23 03:28:19.577012: Pseudo dice [np.float32(0.9896)]\n",
            "2025-12-23 03:28:19.577106: Epoch time: 22.39 s\n",
            "2025-12-23 03:28:20.896758: \n",
            "2025-12-23 03:28:20.897004: Epoch 860\n",
            "2025-12-23 03:28:20.897141: Current learning rate: 0.0017\n",
            "2025-12-23 03:28:43.378552: train_loss -0.9763\n",
            "2025-12-23 03:28:43.378900: val_loss -0.9867\n",
            "2025-12-23 03:28:43.379067: Pseudo dice [np.float32(0.9895)]\n",
            "2025-12-23 03:28:43.379179: Epoch time: 22.48 s\n",
            "2025-12-23 03:28:44.682180: \n",
            "2025-12-23 03:28:44.682445: Epoch 861\n",
            "2025-12-23 03:28:44.682576: Current learning rate: 0.00169\n",
            "2025-12-23 03:29:07.094146: train_loss -0.9756\n",
            "2025-12-23 03:29:07.094520: val_loss -0.9865\n",
            "2025-12-23 03:29:07.094651: Pseudo dice [np.float32(0.9892)]\n",
            "2025-12-23 03:29:07.094779: Epoch time: 22.41 s\n",
            "2025-12-23 03:29:08.433561: \n",
            "2025-12-23 03:29:08.433851: Epoch 862\n",
            "2025-12-23 03:29:08.434005: Current learning rate: 0.00168\n",
            "2025-12-23 03:29:30.893502: train_loss -0.9754\n",
            "2025-12-23 03:29:30.893720: val_loss -0.9846\n",
            "2025-12-23 03:29:30.893839: Pseudo dice [np.float32(0.9882)]\n",
            "2025-12-23 03:29:30.893953: Epoch time: 22.46 s\n",
            "2025-12-23 03:29:32.238511: \n",
            "2025-12-23 03:29:32.238847: Epoch 863\n",
            "2025-12-23 03:29:32.238989: Current learning rate: 0.00167\n",
            "2025-12-23 03:29:54.679262: train_loss -0.976\n",
            "2025-12-23 03:29:54.679590: val_loss -0.9857\n",
            "2025-12-23 03:29:54.679725: Pseudo dice [np.float32(0.989)]\n",
            "2025-12-23 03:29:54.679881: Epoch time: 22.44 s\n",
            "2025-12-23 03:29:55.997970: \n",
            "2025-12-23 03:29:55.998332: Epoch 864\n",
            "2025-12-23 03:29:55.998477: Current learning rate: 0.00166\n",
            "2025-12-23 03:30:18.493150: train_loss -0.9763\n",
            "2025-12-23 03:30:18.493491: val_loss -0.9844\n",
            "2025-12-23 03:30:18.493605: Pseudo dice [np.float32(0.9887)]\n",
            "2025-12-23 03:30:18.493698: Epoch time: 22.5 s\n",
            "2025-12-23 03:30:19.815087: \n",
            "2025-12-23 03:30:19.815293: Epoch 865\n",
            "2025-12-23 03:30:19.815465: Current learning rate: 0.00165\n",
            "2025-12-23 03:30:42.238372: train_loss -0.9756\n",
            "2025-12-23 03:30:42.238567: val_loss -0.9867\n",
            "2025-12-23 03:30:42.238657: Pseudo dice [np.float32(0.9893)]\n",
            "2025-12-23 03:30:42.238747: Epoch time: 22.42 s\n",
            "2025-12-23 03:30:43.556940: \n",
            "2025-12-23 03:30:43.557280: Epoch 866\n",
            "2025-12-23 03:30:43.557419: Current learning rate: 0.00164\n",
            "2025-12-23 03:31:05.974774: train_loss -0.9764\n",
            "2025-12-23 03:31:05.974990: val_loss -0.9865\n",
            "2025-12-23 03:31:05.975077: Pseudo dice [np.float32(0.9893)]\n",
            "2025-12-23 03:31:05.975164: Epoch time: 22.42 s\n",
            "2025-12-23 03:31:07.924026: \n",
            "2025-12-23 03:31:07.924386: Epoch 867\n",
            "2025-12-23 03:31:07.924537: Current learning rate: 0.00163\n",
            "2025-12-23 03:31:30.457814: train_loss -0.976\n",
            "2025-12-23 03:31:30.458044: val_loss -0.9858\n",
            "2025-12-23 03:31:30.458131: Pseudo dice [np.float32(0.9894)]\n",
            "2025-12-23 03:31:30.458238: Epoch time: 22.54 s\n",
            "2025-12-23 03:31:31.768205: \n",
            "2025-12-23 03:31:31.768575: Epoch 868\n",
            "2025-12-23 03:31:31.768723: Current learning rate: 0.00162\n",
            "2025-12-23 03:31:54.193201: train_loss -0.9766\n",
            "2025-12-23 03:31:54.193506: val_loss -0.9868\n",
            "2025-12-23 03:31:54.193617: Pseudo dice [np.float32(0.9897)]\n",
            "2025-12-23 03:31:54.193717: Epoch time: 22.43 s\n",
            "2025-12-23 03:31:55.498076: \n",
            "2025-12-23 03:31:55.498428: Epoch 869\n",
            "2025-12-23 03:31:55.498584: Current learning rate: 0.00161\n",
            "2025-12-23 03:32:17.938660: train_loss -0.9766\n",
            "2025-12-23 03:32:17.939075: val_loss -0.9865\n",
            "2025-12-23 03:32:17.939180: Pseudo dice [np.float32(0.9895)]\n",
            "2025-12-23 03:32:17.939316: Epoch time: 22.44 s\n",
            "2025-12-23 03:32:19.240851: \n",
            "2025-12-23 03:32:19.241279: Epoch 870\n",
            "2025-12-23 03:32:19.241455: Current learning rate: 0.00159\n",
            "2025-12-23 03:32:41.667922: train_loss -0.9762\n",
            "2025-12-23 03:32:41.668156: val_loss -0.9838\n",
            "2025-12-23 03:32:41.668262: Pseudo dice [np.float32(0.9882)]\n",
            "2025-12-23 03:32:41.668354: Epoch time: 22.43 s\n",
            "2025-12-23 03:32:43.004606: \n",
            "2025-12-23 03:32:43.004805: Epoch 871\n",
            "2025-12-23 03:32:43.004949: Current learning rate: 0.00158\n",
            "2025-12-23 03:33:05.474776: train_loss -0.9759\n",
            "2025-12-23 03:33:05.474992: val_loss -0.9859\n",
            "2025-12-23 03:33:05.475105: Pseudo dice [np.float32(0.9898)]\n",
            "2025-12-23 03:33:05.475235: Epoch time: 22.47 s\n",
            "2025-12-23 03:33:06.798924: \n",
            "2025-12-23 03:33:06.799211: Epoch 872\n",
            "2025-12-23 03:33:06.799367: Current learning rate: 0.00157\n",
            "2025-12-23 03:33:29.237864: train_loss -0.9771\n",
            "2025-12-23 03:33:29.238115: val_loss -0.9857\n",
            "2025-12-23 03:33:29.238208: Pseudo dice [np.float32(0.9899)]\n",
            "2025-12-23 03:33:29.238339: Epoch time: 22.44 s\n",
            "2025-12-23 03:33:30.563593: \n",
            "2025-12-23 03:33:30.563913: Epoch 873\n",
            "2025-12-23 03:33:30.564052: Current learning rate: 0.00156\n",
            "2025-12-23 03:33:53.019735: train_loss -0.9765\n",
            "2025-12-23 03:33:53.019945: val_loss -0.9858\n",
            "2025-12-23 03:33:53.020035: Pseudo dice [np.float32(0.989)]\n",
            "2025-12-23 03:33:53.020124: Epoch time: 22.46 s\n",
            "2025-12-23 03:33:54.360052: \n",
            "2025-12-23 03:33:54.360339: Epoch 874\n",
            "2025-12-23 03:33:54.360490: Current learning rate: 0.00155\n",
            "2025-12-23 03:34:16.831505: train_loss -0.9761\n",
            "2025-12-23 03:34:16.831734: val_loss -0.9855\n",
            "2025-12-23 03:34:16.831845: Pseudo dice [np.float32(0.9891)]\n",
            "2025-12-23 03:34:16.832044: Epoch time: 22.47 s\n",
            "2025-12-23 03:34:18.165982: \n",
            "2025-12-23 03:34:18.166302: Epoch 875\n",
            "2025-12-23 03:34:18.166459: Current learning rate: 0.00154\n",
            "2025-12-23 03:34:40.631557: train_loss -0.9757\n",
            "2025-12-23 03:34:40.631752: val_loss -0.9861\n",
            "2025-12-23 03:34:40.631839: Pseudo dice [np.float32(0.9894)]\n",
            "2025-12-23 03:34:40.631962: Epoch time: 22.47 s\n",
            "2025-12-23 03:34:41.970867: \n",
            "2025-12-23 03:34:41.971127: Epoch 876\n",
            "2025-12-23 03:34:41.971286: Current learning rate: 0.00153\n",
            "2025-12-23 03:35:04.413626: train_loss -0.9759\n",
            "2025-12-23 03:35:04.413847: val_loss -0.9865\n",
            "2025-12-23 03:35:04.413984: Pseudo dice [np.float32(0.9896)]\n",
            "2025-12-23 03:35:04.414082: Epoch time: 22.44 s\n",
            "2025-12-23 03:35:05.739259: \n",
            "2025-12-23 03:35:05.739605: Epoch 877\n",
            "2025-12-23 03:35:05.739738: Current learning rate: 0.00152\n",
            "2025-12-23 03:35:28.182842: train_loss -0.9763\n",
            "2025-12-23 03:35:28.183353: val_loss -0.9863\n",
            "2025-12-23 03:35:28.183511: Pseudo dice [np.float32(0.9889)]\n",
            "2025-12-23 03:35:28.183709: Epoch time: 22.44 s\n",
            "2025-12-23 03:35:29.532912: \n",
            "2025-12-23 03:35:29.533302: Epoch 878\n",
            "2025-12-23 03:35:29.533461: Current learning rate: 0.00151\n",
            "2025-12-23 03:35:52.002177: train_loss -0.9766\n",
            "2025-12-23 03:35:52.002517: val_loss -0.9869\n",
            "2025-12-23 03:35:52.002746: Pseudo dice [np.float32(0.99)]\n",
            "2025-12-23 03:35:52.002918: Epoch time: 22.47 s\n",
            "2025-12-23 03:35:53.345544: \n",
            "2025-12-23 03:35:53.345881: Epoch 879\n",
            "2025-12-23 03:35:53.346022: Current learning rate: 0.00149\n",
            "2025-12-23 03:36:15.768929: train_loss -0.9764\n",
            "2025-12-23 03:36:15.769209: val_loss -0.9869\n",
            "2025-12-23 03:36:15.769343: Pseudo dice [np.float32(0.9901)]\n",
            "2025-12-23 03:36:15.769438: Epoch time: 22.42 s\n",
            "2025-12-23 03:36:17.089121: \n",
            "2025-12-23 03:36:17.089311: Epoch 880\n",
            "2025-12-23 03:36:17.089443: Current learning rate: 0.00148\n",
            "2025-12-23 03:36:39.567735: train_loss -0.9764\n",
            "2025-12-23 03:36:39.568088: val_loss -0.9868\n",
            "2025-12-23 03:36:39.568255: Pseudo dice [np.float32(0.9897)]\n",
            "2025-12-23 03:36:39.568401: Epoch time: 22.48 s\n",
            "2025-12-23 03:36:40.938523: \n",
            "2025-12-23 03:36:40.938731: Epoch 881\n",
            "2025-12-23 03:36:40.938868: Current learning rate: 0.00147\n",
            "2025-12-23 03:37:03.373385: train_loss -0.9767\n",
            "2025-12-23 03:37:03.373652: val_loss -0.9863\n",
            "2025-12-23 03:37:03.373759: Pseudo dice [np.float32(0.9893)]\n",
            "2025-12-23 03:37:03.373858: Epoch time: 22.44 s\n",
            "2025-12-23 03:37:04.716512: \n",
            "2025-12-23 03:37:04.716714: Epoch 882\n",
            "2025-12-23 03:37:04.716848: Current learning rate: 0.00146\n",
            "2025-12-23 03:37:27.113951: train_loss -0.9765\n",
            "2025-12-23 03:37:27.114312: val_loss -0.9868\n",
            "2025-12-23 03:37:27.114547: Pseudo dice [np.float32(0.99)]\n",
            "2025-12-23 03:37:27.114716: Epoch time: 22.4 s\n",
            "2025-12-23 03:37:28.476796: \n",
            "2025-12-23 03:37:28.477103: Epoch 883\n",
            "2025-12-23 03:37:28.477289: Current learning rate: 0.00145\n",
            "2025-12-23 03:37:50.897650: train_loss -0.9762\n",
            "2025-12-23 03:37:50.897879: val_loss -0.9866\n",
            "2025-12-23 03:37:50.898044: Pseudo dice [np.float32(0.9896)]\n",
            "2025-12-23 03:37:50.898145: Epoch time: 22.42 s\n",
            "2025-12-23 03:37:50.898257: Yayy! New best EMA pseudo Dice: 0.9894999861717224\n",
            "2025-12-23 03:37:52.767832: \n",
            "2025-12-23 03:37:52.768051: Epoch 884\n",
            "2025-12-23 03:37:52.768184: Current learning rate: 0.00144\n",
            "2025-12-23 03:38:15.195867: train_loss -0.9762\n",
            "2025-12-23 03:38:15.196069: val_loss -0.9859\n",
            "2025-12-23 03:38:15.196200: Pseudo dice [np.float32(0.9897)]\n",
            "2025-12-23 03:38:15.196428: Epoch time: 22.43 s\n",
            "2025-12-23 03:38:15.196552: Yayy! New best EMA pseudo Dice: 0.9894999861717224\n",
            "2025-12-23 03:38:17.062614: \n",
            "2025-12-23 03:38:17.062971: Epoch 885\n",
            "2025-12-23 03:38:17.063154: Current learning rate: 0.00143\n",
            "2025-12-23 03:38:39.524298: train_loss -0.9756\n",
            "2025-12-23 03:38:39.524760: val_loss -0.9868\n",
            "2025-12-23 03:38:39.524874: Pseudo dice [np.float32(0.99)]\n",
            "2025-12-23 03:38:39.524961: Epoch time: 22.46 s\n",
            "2025-12-23 03:38:39.525036: Yayy! New best EMA pseudo Dice: 0.9896000027656555\n",
            "2025-12-23 03:38:42.069669: \n",
            "2025-12-23 03:38:42.069921: Epoch 886\n",
            "2025-12-23 03:38:42.070097: Current learning rate: 0.00142\n",
            "2025-12-23 03:39:04.517931: train_loss -0.9763\n",
            "2025-12-23 03:39:04.518252: val_loss -0.985\n",
            "2025-12-23 03:39:04.518422: Pseudo dice [np.float32(0.9883)]\n",
            "2025-12-23 03:39:04.518631: Epoch time: 22.45 s\n",
            "2025-12-23 03:39:05.844674: \n",
            "2025-12-23 03:39:05.844947: Epoch 887\n",
            "2025-12-23 03:39:05.845087: Current learning rate: 0.00141\n",
            "2025-12-23 03:39:28.286754: train_loss -0.9772\n",
            "2025-12-23 03:39:28.287019: val_loss -0.9869\n",
            "2025-12-23 03:39:28.287124: Pseudo dice [np.float32(0.99)]\n",
            "2025-12-23 03:39:28.287260: Epoch time: 22.44 s\n",
            "2025-12-23 03:39:29.643483: \n",
            "2025-12-23 03:39:29.643806: Epoch 888\n",
            "2025-12-23 03:39:29.643948: Current learning rate: 0.00139\n",
            "2025-12-23 03:39:52.134508: train_loss -0.9758\n",
            "2025-12-23 03:39:52.134742: val_loss -0.9864\n",
            "2025-12-23 03:39:52.134841: Pseudo dice [np.float32(0.9897)]\n",
            "2025-12-23 03:39:52.134930: Epoch time: 22.49 s\n",
            "2025-12-23 03:39:53.479037: \n",
            "2025-12-23 03:39:53.479423: Epoch 889\n",
            "2025-12-23 03:39:53.479569: Current learning rate: 0.00138\n",
            "2025-12-23 03:40:15.916953: train_loss -0.9768\n",
            "2025-12-23 03:40:15.917470: val_loss -0.9856\n",
            "2025-12-23 03:40:15.917629: Pseudo dice [np.float32(0.9889)]\n",
            "2025-12-23 03:40:15.917781: Epoch time: 22.44 s\n",
            "2025-12-23 03:40:17.282547: \n",
            "2025-12-23 03:40:17.282725: Epoch 890\n",
            "2025-12-23 03:40:17.282854: Current learning rate: 0.00137\n",
            "2025-12-23 03:40:39.753669: train_loss -0.9766\n",
            "2025-12-23 03:40:39.754138: val_loss -0.9862\n",
            "2025-12-23 03:40:39.754277: Pseudo dice [np.float32(0.9894)]\n",
            "2025-12-23 03:40:39.754390: Epoch time: 22.47 s\n",
            "2025-12-23 03:40:41.094579: \n",
            "2025-12-23 03:40:41.094936: Epoch 891\n",
            "2025-12-23 03:40:41.095092: Current learning rate: 0.00136\n",
            "2025-12-23 03:41:03.587350: train_loss -0.9768\n",
            "2025-12-23 03:41:03.587601: val_loss -0.9864\n",
            "2025-12-23 03:41:03.587887: Pseudo dice [np.float32(0.9896)]\n",
            "2025-12-23 03:41:03.588179: Epoch time: 22.49 s\n",
            "2025-12-23 03:41:04.953304: \n",
            "2025-12-23 03:41:04.953651: Epoch 892\n",
            "2025-12-23 03:41:04.953792: Current learning rate: 0.00135\n",
            "2025-12-23 03:41:27.404792: train_loss -0.9767\n",
            "2025-12-23 03:41:27.405009: val_loss -0.9849\n",
            "2025-12-23 03:41:27.405103: Pseudo dice [np.float32(0.9893)]\n",
            "2025-12-23 03:41:27.405265: Epoch time: 22.45 s\n",
            "2025-12-23 03:41:28.765493: \n",
            "2025-12-23 03:41:28.765735: Epoch 893\n",
            "2025-12-23 03:41:28.765929: Current learning rate: 0.00134\n",
            "2025-12-23 03:41:51.233063: train_loss -0.9764\n",
            "2025-12-23 03:41:51.233341: val_loss -0.9872\n",
            "2025-12-23 03:41:51.233536: Pseudo dice [np.float32(0.9902)]\n",
            "2025-12-23 03:41:51.233666: Epoch time: 22.47 s\n",
            "2025-12-23 03:41:52.570879: \n",
            "2025-12-23 03:41:52.571291: Epoch 894\n",
            "2025-12-23 03:41:52.571459: Current learning rate: 0.00133\n",
            "2025-12-23 03:42:15.004302: train_loss -0.9764\n",
            "2025-12-23 03:42:15.004525: val_loss -0.9866\n",
            "2025-12-23 03:42:15.004616: Pseudo dice [np.float32(0.9893)]\n",
            "2025-12-23 03:42:15.004707: Epoch time: 22.43 s\n",
            "2025-12-23 03:42:16.336406: \n",
            "2025-12-23 03:42:16.336790: Epoch 895\n",
            "2025-12-23 03:42:16.336937: Current learning rate: 0.00132\n",
            "2025-12-23 03:42:38.787487: train_loss -0.9765\n",
            "2025-12-23 03:42:38.787700: val_loss -0.9861\n",
            "2025-12-23 03:42:38.787877: Pseudo dice [np.float32(0.9896)]\n",
            "2025-12-23 03:42:38.787991: Epoch time: 22.45 s\n",
            "2025-12-23 03:42:40.108464: \n",
            "2025-12-23 03:42:40.108747: Epoch 896\n",
            "2025-12-23 03:42:40.108880: Current learning rate: 0.0013\n",
            "2025-12-23 03:43:02.551975: train_loss -0.9773\n",
            "2025-12-23 03:43:02.552198: val_loss -0.9864\n",
            "2025-12-23 03:43:02.552333: Pseudo dice [np.float32(0.9893)]\n",
            "2025-12-23 03:43:02.552430: Epoch time: 22.44 s\n",
            "2025-12-23 03:43:03.887952: \n",
            "2025-12-23 03:43:03.888315: Epoch 897\n",
            "2025-12-23 03:43:03.888464: Current learning rate: 0.00129\n",
            "2025-12-23 03:43:26.362810: train_loss -0.9769\n",
            "2025-12-23 03:43:26.363143: val_loss -0.9849\n",
            "2025-12-23 03:43:26.363261: Pseudo dice [np.float32(0.9891)]\n",
            "2025-12-23 03:43:26.363359: Epoch time: 22.48 s\n",
            "2025-12-23 03:43:27.664117: \n",
            "2025-12-23 03:43:27.664439: Epoch 898\n",
            "2025-12-23 03:43:27.664575: Current learning rate: 0.00128\n",
            "2025-12-23 03:43:50.116444: train_loss -0.9762\n",
            "2025-12-23 03:43:50.116682: val_loss -0.9863\n",
            "2025-12-23 03:43:50.116778: Pseudo dice [np.float32(0.9895)]\n",
            "2025-12-23 03:43:50.116880: Epoch time: 22.45 s\n",
            "2025-12-23 03:43:51.441280: \n",
            "2025-12-23 03:43:51.441453: Epoch 899\n",
            "2025-12-23 03:43:51.441605: Current learning rate: 0.00127\n",
            "2025-12-23 03:44:13.918764: train_loss -0.9761\n",
            "2025-12-23 03:44:13.919164: val_loss -0.9866\n",
            "2025-12-23 03:44:13.919276: Pseudo dice [np.float32(0.9901)]\n",
            "2025-12-23 03:44:13.919379: Epoch time: 22.48 s\n",
            "2025-12-23 03:44:15.834619: \n",
            "2025-12-23 03:44:15.834864: Epoch 900\n",
            "2025-12-23 03:44:15.835022: Current learning rate: 0.00126\n",
            "2025-12-23 03:44:38.311939: train_loss -0.9768\n",
            "2025-12-23 03:44:38.312299: val_loss -0.9865\n",
            "2025-12-23 03:44:38.312502: Pseudo dice [np.float32(0.9893)]\n",
            "2025-12-23 03:44:38.312652: Epoch time: 22.48 s\n",
            "2025-12-23 03:44:39.651141: \n",
            "2025-12-23 03:44:39.651437: Epoch 901\n",
            "2025-12-23 03:44:39.651601: Current learning rate: 0.00125\n",
            "2025-12-23 03:45:02.097642: train_loss -0.977\n",
            "2025-12-23 03:45:02.097866: val_loss -0.9863\n",
            "2025-12-23 03:45:02.097954: Pseudo dice [np.float32(0.9898)]\n",
            "2025-12-23 03:45:02.098042: Epoch time: 22.45 s\n",
            "2025-12-23 03:45:03.443993: \n",
            "2025-12-23 03:45:03.444184: Epoch 902\n",
            "2025-12-23 03:45:03.444410: Current learning rate: 0.00124\n",
            "2025-12-23 03:45:25.854780: train_loss -0.9762\n",
            "2025-12-23 03:45:25.855005: val_loss -0.9866\n",
            "2025-12-23 03:45:25.855114: Pseudo dice [np.float32(0.9893)]\n",
            "2025-12-23 03:45:25.855206: Epoch time: 22.41 s\n",
            "2025-12-23 03:45:27.173853: \n",
            "2025-12-23 03:45:27.174135: Epoch 903\n",
            "2025-12-23 03:45:27.174286: Current learning rate: 0.00122\n",
            "2025-12-23 03:45:49.638810: train_loss -0.9771\n",
            "2025-12-23 03:45:49.639081: val_loss -0.9862\n",
            "2025-12-23 03:45:49.639255: Pseudo dice [np.float32(0.9896)]\n",
            "2025-12-23 03:45:49.639375: Epoch time: 22.47 s\n",
            "2025-12-23 03:45:51.626356: \n",
            "2025-12-23 03:45:51.626647: Epoch 904\n",
            "2025-12-23 03:45:51.626812: Current learning rate: 0.00121\n",
            "2025-12-23 03:46:14.141428: train_loss -0.9762\n",
            "2025-12-23 03:46:14.141726: val_loss -0.9867\n",
            "2025-12-23 03:46:14.141830: Pseudo dice [np.float32(0.9899)]\n",
            "2025-12-23 03:46:14.141923: Epoch time: 22.52 s\n",
            "2025-12-23 03:46:15.454162: \n",
            "2025-12-23 03:46:15.454519: Epoch 905\n",
            "2025-12-23 03:46:15.454663: Current learning rate: 0.0012\n",
            "2025-12-23 03:46:37.943332: train_loss -0.9773\n",
            "2025-12-23 03:46:37.943653: val_loss -0.987\n",
            "2025-12-23 03:46:37.943754: Pseudo dice [np.float32(0.99)]\n",
            "2025-12-23 03:46:37.943862: Epoch time: 22.49 s\n",
            "2025-12-23 03:46:37.943937: Yayy! New best EMA pseudo Dice: 0.9896000027656555\n",
            "2025-12-23 03:46:39.743000: \n",
            "2025-12-23 03:46:39.743297: Epoch 906\n",
            "2025-12-23 03:46:39.743432: Current learning rate: 0.00119\n",
            "2025-12-23 03:47:02.238898: train_loss -0.9772\n",
            "2025-12-23 03:47:02.239247: val_loss -0.986\n",
            "2025-12-23 03:47:02.239371: Pseudo dice [np.float32(0.9896)]\n",
            "2025-12-23 03:47:02.239517: Epoch time: 22.5 s\n",
            "2025-12-23 03:47:02.239601: Yayy! New best EMA pseudo Dice: 0.9896000027656555\n",
            "2025-12-23 03:47:04.065554: \n",
            "2025-12-23 03:47:04.065909: Epoch 907\n",
            "2025-12-23 03:47:04.066045: Current learning rate: 0.00118\n",
            "2025-12-23 03:47:26.487413: train_loss -0.9773\n",
            "2025-12-23 03:47:26.487619: val_loss -0.9865\n",
            "2025-12-23 03:47:26.487716: Pseudo dice [np.float32(0.9891)]\n",
            "2025-12-23 03:47:26.487808: Epoch time: 22.42 s\n",
            "2025-12-23 03:47:27.825043: \n",
            "2025-12-23 03:47:27.825394: Epoch 908\n",
            "2025-12-23 03:47:27.825568: Current learning rate: 0.00117\n",
            "2025-12-23 03:47:50.288313: train_loss -0.9776\n",
            "2025-12-23 03:47:50.288519: val_loss -0.9853\n",
            "2025-12-23 03:47:50.288612: Pseudo dice [np.float32(0.9886)]\n",
            "2025-12-23 03:47:50.288715: Epoch time: 22.46 s\n",
            "2025-12-23 03:47:51.619344: \n",
            "2025-12-23 03:47:51.619676: Epoch 909\n",
            "2025-12-23 03:47:51.619814: Current learning rate: 0.00116\n",
            "2025-12-23 03:48:14.072388: train_loss -0.9762\n",
            "2025-12-23 03:48:14.072620: val_loss -0.9868\n",
            "2025-12-23 03:48:14.072707: Pseudo dice [np.float32(0.9903)]\n",
            "2025-12-23 03:48:14.072795: Epoch time: 22.45 s\n",
            "2025-12-23 03:48:15.393775: \n",
            "2025-12-23 03:48:15.394132: Epoch 910\n",
            "2025-12-23 03:48:15.394275: Current learning rate: 0.00115\n",
            "2025-12-23 03:48:37.876149: train_loss -0.9764\n",
            "2025-12-23 03:48:37.876416: val_loss -0.9862\n",
            "2025-12-23 03:48:37.876516: Pseudo dice [np.float32(0.9889)]\n",
            "2025-12-23 03:48:37.876613: Epoch time: 22.48 s\n",
            "2025-12-23 03:48:39.180938: \n",
            "2025-12-23 03:48:39.181191: Epoch 911\n",
            "2025-12-23 03:48:39.181390: Current learning rate: 0.00113\n",
            "2025-12-23 03:49:01.656491: train_loss -0.9767\n",
            "2025-12-23 03:49:01.656752: val_loss -0.987\n",
            "2025-12-23 03:49:01.656854: Pseudo dice [np.float32(0.9896)]\n",
            "2025-12-23 03:49:01.656947: Epoch time: 22.48 s\n",
            "2025-12-23 03:49:02.985974: \n",
            "2025-12-23 03:49:02.986178: Epoch 912\n",
            "2025-12-23 03:49:02.986366: Current learning rate: 0.00112\n",
            "2025-12-23 03:49:25.435440: train_loss -0.9768\n",
            "2025-12-23 03:49:25.435735: val_loss -0.9867\n",
            "2025-12-23 03:49:25.435834: Pseudo dice [np.float32(0.9898)]\n",
            "2025-12-23 03:49:25.435942: Epoch time: 22.45 s\n",
            "2025-12-23 03:49:26.754043: \n",
            "2025-12-23 03:49:26.754239: Epoch 913\n",
            "2025-12-23 03:49:26.754370: Current learning rate: 0.00111\n",
            "2025-12-23 03:49:49.155557: train_loss -0.9776\n",
            "2025-12-23 03:49:49.155757: val_loss -0.987\n",
            "2025-12-23 03:49:49.155857: Pseudo dice [np.float32(0.9898)]\n",
            "2025-12-23 03:49:49.155948: Epoch time: 22.4 s\n",
            "2025-12-23 03:49:50.487186: \n",
            "2025-12-23 03:49:50.487525: Epoch 914\n",
            "2025-12-23 03:49:50.487664: Current learning rate: 0.0011\n",
            "2025-12-23 03:50:12.947452: train_loss -0.9766\n",
            "2025-12-23 03:50:12.947660: val_loss -0.9865\n",
            "2025-12-23 03:50:12.947747: Pseudo dice [np.float32(0.9895)]\n",
            "2025-12-23 03:50:12.947837: Epoch time: 22.46 s\n",
            "2025-12-23 03:50:14.255857: \n",
            "2025-12-23 03:50:14.256042: Epoch 915\n",
            "2025-12-23 03:50:14.256168: Current learning rate: 0.00109\n",
            "2025-12-23 03:50:36.696939: train_loss -0.9764\n",
            "2025-12-23 03:50:36.697275: val_loss -0.9871\n",
            "2025-12-23 03:50:36.697391: Pseudo dice [np.float32(0.9896)]\n",
            "2025-12-23 03:50:36.697496: Epoch time: 22.44 s\n",
            "2025-12-23 03:50:38.003167: \n",
            "2025-12-23 03:50:38.003381: Epoch 916\n",
            "2025-12-23 03:50:38.003509: Current learning rate: 0.00108\n",
            "2025-12-23 03:51:00.436620: train_loss -0.9758\n",
            "2025-12-23 03:51:00.436928: val_loss -0.9863\n",
            "2025-12-23 03:51:00.437082: Pseudo dice [np.float32(0.9897)]\n",
            "2025-12-23 03:51:00.437253: Epoch time: 22.43 s\n",
            "2025-12-23 03:51:01.774969: \n",
            "2025-12-23 03:51:01.775252: Epoch 917\n",
            "2025-12-23 03:51:01.775393: Current learning rate: 0.00106\n",
            "2025-12-23 03:51:24.180528: train_loss -0.9774\n",
            "2025-12-23 03:51:24.180751: val_loss -0.9847\n",
            "2025-12-23 03:51:24.180886: Pseudo dice [np.float32(0.9887)]\n",
            "2025-12-23 03:51:24.181007: Epoch time: 22.41 s\n",
            "2025-12-23 03:51:25.469100: \n",
            "2025-12-23 03:51:25.469406: Epoch 918\n",
            "2025-12-23 03:51:25.469580: Current learning rate: 0.00105\n",
            "2025-12-23 03:51:47.983822: train_loss -0.9774\n",
            "2025-12-23 03:51:47.984046: val_loss -0.9859\n",
            "2025-12-23 03:51:47.984161: Pseudo dice [np.float32(0.9899)]\n",
            "2025-12-23 03:51:47.984289: Epoch time: 22.52 s\n",
            "2025-12-23 03:51:49.340778: \n",
            "2025-12-23 03:51:49.341110: Epoch 919\n",
            "2025-12-23 03:51:49.341273: Current learning rate: 0.00104\n",
            "2025-12-23 03:52:11.755762: train_loss -0.9772\n",
            "2025-12-23 03:52:11.756015: val_loss -0.987\n",
            "2025-12-23 03:52:11.756327: Pseudo dice [np.float32(0.9896)]\n",
            "2025-12-23 03:52:11.756603: Epoch time: 22.42 s\n",
            "2025-12-23 03:52:13.088021: \n",
            "2025-12-23 03:52:13.088294: Epoch 920\n",
            "2025-12-23 03:52:13.088457: Current learning rate: 0.00103\n",
            "2025-12-23 03:52:35.494354: train_loss -0.9773\n",
            "2025-12-23 03:52:35.494771: val_loss -0.9859\n",
            "2025-12-23 03:52:35.494933: Pseudo dice [np.float32(0.9896)]\n",
            "2025-12-23 03:52:35.495070: Epoch time: 22.41 s\n",
            "2025-12-23 03:52:36.854928: \n",
            "2025-12-23 03:52:36.855171: Epoch 921\n",
            "2025-12-23 03:52:36.855313: Current learning rate: 0.00102\n",
            "2025-12-23 03:52:59.227514: train_loss -0.9783\n",
            "2025-12-23 03:52:59.227772: val_loss -0.9859\n",
            "2025-12-23 03:52:59.227920: Pseudo dice [np.float32(0.9898)]\n",
            "2025-12-23 03:52:59.228056: Epoch time: 22.37 s\n",
            "2025-12-23 03:53:00.564070: \n",
            "2025-12-23 03:53:00.564341: Epoch 922\n",
            "2025-12-23 03:53:00.564495: Current learning rate: 0.00101\n",
            "2025-12-23 03:53:22.929205: train_loss -0.9777\n",
            "2025-12-23 03:53:22.929503: val_loss -0.9865\n",
            "2025-12-23 03:53:22.929688: Pseudo dice [np.float32(0.99)]\n",
            "2025-12-23 03:53:22.929805: Epoch time: 22.37 s\n",
            "2025-12-23 03:53:22.929879: Yayy! New best EMA pseudo Dice: 0.9896000027656555\n",
            "2025-12-23 03:53:25.453908: \n",
            "2025-12-23 03:53:25.454258: Epoch 923\n",
            "2025-12-23 03:53:25.454408: Current learning rate: 0.001\n",
            "2025-12-23 03:53:47.987576: train_loss -0.9769\n",
            "2025-12-23 03:53:47.987934: val_loss -0.9856\n",
            "2025-12-23 03:53:47.988156: Pseudo dice [np.float32(0.9888)]\n",
            "2025-12-23 03:53:47.988379: Epoch time: 22.53 s\n",
            "2025-12-23 03:53:49.335045: \n",
            "2025-12-23 03:53:49.335416: Epoch 924\n",
            "2025-12-23 03:53:49.335564: Current learning rate: 0.00098\n",
            "2025-12-23 03:54:11.745074: train_loss -0.9769\n",
            "2025-12-23 03:54:11.745404: val_loss -0.986\n",
            "2025-12-23 03:54:11.745568: Pseudo dice [np.float32(0.9904)]\n",
            "2025-12-23 03:54:11.745754: Epoch time: 22.41 s\n",
            "2025-12-23 03:54:11.745908: Yayy! New best EMA pseudo Dice: 0.9896000027656555\n",
            "2025-12-23 03:54:13.637914: \n",
            "2025-12-23 03:54:13.638141: Epoch 925\n",
            "2025-12-23 03:54:13.638303: Current learning rate: 0.00097\n",
            "2025-12-23 03:54:36.120918: train_loss -0.9771\n",
            "2025-12-23 03:54:36.121255: val_loss -0.9869\n",
            "2025-12-23 03:54:36.121377: Pseudo dice [np.float32(0.9894)]\n",
            "2025-12-23 03:54:36.121500: Epoch time: 22.48 s\n",
            "2025-12-23 03:54:37.439438: \n",
            "2025-12-23 03:54:37.439685: Epoch 926\n",
            "2025-12-23 03:54:37.439850: Current learning rate: 0.00096\n",
            "2025-12-23 03:54:59.883756: train_loss -0.9781\n",
            "2025-12-23 03:54:59.884005: val_loss -0.9858\n",
            "2025-12-23 03:54:59.884100: Pseudo dice [np.float32(0.9885)]\n",
            "2025-12-23 03:54:59.884190: Epoch time: 22.45 s\n",
            "2025-12-23 03:55:01.268242: \n",
            "2025-12-23 03:55:01.268532: Epoch 927\n",
            "2025-12-23 03:55:01.268676: Current learning rate: 0.00095\n",
            "2025-12-23 03:55:23.712909: train_loss -0.9769\n",
            "2025-12-23 03:55:23.713106: val_loss -0.986\n",
            "2025-12-23 03:55:23.713196: Pseudo dice [np.float32(0.9896)]\n",
            "2025-12-23 03:55:23.713331: Epoch time: 22.45 s\n",
            "2025-12-23 03:55:25.059495: \n",
            "2025-12-23 03:55:25.059751: Epoch 928\n",
            "2025-12-23 03:55:25.059885: Current learning rate: 0.00094\n",
            "2025-12-23 03:55:47.489060: train_loss -0.9772\n",
            "2025-12-23 03:55:47.489282: val_loss -0.9867\n",
            "2025-12-23 03:55:47.489389: Pseudo dice [np.float32(0.9898)]\n",
            "2025-12-23 03:55:47.489482: Epoch time: 22.43 s\n",
            "2025-12-23 03:55:48.828905: \n",
            "2025-12-23 03:55:48.829179: Epoch 929\n",
            "2025-12-23 03:55:48.829327: Current learning rate: 0.00092\n",
            "2025-12-23 03:56:11.240876: train_loss -0.9768\n",
            "2025-12-23 03:56:11.241205: val_loss -0.9879\n",
            "2025-12-23 03:56:11.241393: Pseudo dice [np.float32(0.9904)]\n",
            "2025-12-23 03:56:11.241511: Epoch time: 22.41 s\n",
            "2025-12-23 03:56:12.632680: \n",
            "2025-12-23 03:56:12.632869: Epoch 930\n",
            "2025-12-23 03:56:12.633113: Current learning rate: 0.00091\n",
            "2025-12-23 03:56:35.066042: train_loss -0.9775\n",
            "2025-12-23 03:56:35.066304: val_loss -0.9868\n",
            "2025-12-23 03:56:35.066463: Pseudo dice [np.float32(0.9899)]\n",
            "2025-12-23 03:56:35.066622: Epoch time: 22.43 s\n",
            "2025-12-23 03:56:35.066709: Yayy! New best EMA pseudo Dice: 0.9896000027656555\n",
            "2025-12-23 03:56:37.054302: \n",
            "2025-12-23 03:56:37.054621: Epoch 931\n",
            "2025-12-23 03:56:37.054755: Current learning rate: 0.0009\n",
            "2025-12-23 03:56:59.499118: train_loss -0.9771\n",
            "2025-12-23 03:56:59.499364: val_loss -0.987\n",
            "2025-12-23 03:56:59.499463: Pseudo dice [np.float32(0.9897)]\n",
            "2025-12-23 03:56:59.499562: Epoch time: 22.45 s\n",
            "2025-12-23 03:56:59.499640: Yayy! New best EMA pseudo Dice: 0.9896000027656555\n",
            "2025-12-23 03:57:01.343331: \n",
            "2025-12-23 03:57:01.343629: Epoch 932\n",
            "2025-12-23 03:57:01.343784: Current learning rate: 0.00089\n",
            "2025-12-23 03:57:23.804946: train_loss -0.9772\n",
            "2025-12-23 03:57:23.805166: val_loss -0.9856\n",
            "2025-12-23 03:57:23.805290: Pseudo dice [np.float32(0.9895)]\n",
            "2025-12-23 03:57:23.805391: Epoch time: 22.46 s\n",
            "2025-12-23 03:57:25.162984: \n",
            "2025-12-23 03:57:25.163379: Epoch 933\n",
            "2025-12-23 03:57:25.163516: Current learning rate: 0.00088\n",
            "2025-12-23 03:57:47.577650: train_loss -0.9774\n",
            "2025-12-23 03:57:47.577948: val_loss -0.9868\n",
            "2025-12-23 03:57:47.578148: Pseudo dice [np.float32(0.9898)]\n",
            "2025-12-23 03:57:47.578402: Epoch time: 22.42 s\n",
            "2025-12-23 03:57:47.578554: Yayy! New best EMA pseudo Dice: 0.9897000193595886\n",
            "2025-12-23 03:57:49.463842: \n",
            "2025-12-23 03:57:49.464163: Epoch 934\n",
            "2025-12-23 03:57:49.464322: Current learning rate: 0.00087\n",
            "2025-12-23 03:58:11.910709: train_loss -0.9781\n",
            "2025-12-23 03:58:11.910910: val_loss -0.9873\n",
            "2025-12-23 03:58:11.911027: Pseudo dice [np.float32(0.9903)]\n",
            "2025-12-23 03:58:11.911414: Epoch time: 22.45 s\n",
            "2025-12-23 03:58:11.911551: Yayy! New best EMA pseudo Dice: 0.9897000193595886\n",
            "2025-12-23 03:58:13.787474: \n",
            "2025-12-23 03:58:13.787663: Epoch 935\n",
            "2025-12-23 03:58:13.787794: Current learning rate: 0.00085\n",
            "2025-12-23 03:58:36.252969: train_loss -0.9781\n",
            "2025-12-23 03:58:36.253168: val_loss -0.9862\n",
            "2025-12-23 03:58:36.253300: Pseudo dice [np.float32(0.9896)]\n",
            "2025-12-23 03:58:36.253402: Epoch time: 22.47 s\n",
            "2025-12-23 03:58:37.615978: \n",
            "2025-12-23 03:58:37.616294: Epoch 936\n",
            "2025-12-23 03:58:37.616462: Current learning rate: 0.00084\n",
            "2025-12-23 03:59:00.058151: train_loss -0.9775\n",
            "2025-12-23 03:59:00.058398: val_loss -0.9872\n",
            "2025-12-23 03:59:00.058496: Pseudo dice [np.float32(0.9899)]\n",
            "2025-12-23 03:59:00.058587: Epoch time: 22.44 s\n",
            "2025-12-23 03:59:00.058685: Yayy! New best EMA pseudo Dice: 0.9897000193595886\n",
            "2025-12-23 03:59:01.937200: \n",
            "2025-12-23 03:59:01.937396: Epoch 937\n",
            "2025-12-23 03:59:01.937586: Current learning rate: 0.00083\n",
            "2025-12-23 03:59:24.374661: train_loss -0.9785\n",
            "2025-12-23 03:59:24.375070: val_loss -0.9879\n",
            "2025-12-23 03:59:24.375162: Pseudo dice [np.float32(0.9905)]\n",
            "2025-12-23 03:59:24.375288: Epoch time: 22.44 s\n",
            "2025-12-23 03:59:24.375380: Yayy! New best EMA pseudo Dice: 0.989799976348877\n",
            "2025-12-23 03:59:26.263168: \n",
            "2025-12-23 03:59:26.263441: Epoch 938\n",
            "2025-12-23 03:59:26.263613: Current learning rate: 0.00082\n",
            "2025-12-23 03:59:48.736020: train_loss -0.9779\n",
            "2025-12-23 03:59:48.736362: val_loss -0.9869\n",
            "2025-12-23 03:59:48.736569: Pseudo dice [np.float32(0.99)]\n",
            "2025-12-23 03:59:48.736756: Epoch time: 22.47 s\n",
            "2025-12-23 03:59:48.736871: Yayy! New best EMA pseudo Dice: 0.989799976348877\n",
            "2025-12-23 03:59:50.622546: \n",
            "2025-12-23 03:59:50.622819: Epoch 939\n",
            "2025-12-23 03:59:50.623004: Current learning rate: 0.00081\n",
            "2025-12-23 04:00:13.093045: train_loss -0.9771\n",
            "2025-12-23 04:00:13.093297: val_loss -0.9879\n",
            "2025-12-23 04:00:13.093426: Pseudo dice [np.float32(0.9906)]\n",
            "2025-12-23 04:00:13.093521: Epoch time: 22.47 s\n",
            "2025-12-23 04:00:13.093610: Yayy! New best EMA pseudo Dice: 0.9898999929428101\n",
            "2025-12-23 04:00:15.593767: \n",
            "2025-12-23 04:00:15.594008: Epoch 940\n",
            "2025-12-23 04:00:15.594133: Current learning rate: 0.00079\n",
            "2025-12-23 04:00:38.059157: train_loss -0.9773\n",
            "2025-12-23 04:00:38.059502: val_loss -0.9864\n",
            "2025-12-23 04:00:38.059653: Pseudo dice [np.float32(0.9896)]\n",
            "2025-12-23 04:00:38.059828: Epoch time: 22.47 s\n",
            "2025-12-23 04:00:39.382192: \n",
            "2025-12-23 04:00:39.382531: Epoch 941\n",
            "2025-12-23 04:00:39.382675: Current learning rate: 0.00078\n",
            "2025-12-23 04:01:01.821895: train_loss -0.9775\n",
            "2025-12-23 04:01:01.822104: val_loss -0.9868\n",
            "2025-12-23 04:01:01.822196: Pseudo dice [np.float32(0.9902)]\n",
            "2025-12-23 04:01:01.822307: Epoch time: 22.44 s\n",
            "2025-12-23 04:01:01.822424: Yayy! New best EMA pseudo Dice: 0.9898999929428101\n",
            "2025-12-23 04:01:03.667093: \n",
            "2025-12-23 04:01:03.667348: Epoch 942\n",
            "2025-12-23 04:01:03.667523: Current learning rate: 0.00077\n",
            "2025-12-23 04:01:26.160418: train_loss -0.9774\n",
            "2025-12-23 04:01:26.160726: val_loss -0.9871\n",
            "2025-12-23 04:01:26.160824: Pseudo dice [np.float32(0.9899)]\n",
            "2025-12-23 04:01:26.160917: Epoch time: 22.49 s\n",
            "2025-12-23 04:01:27.477749: \n",
            "2025-12-23 04:01:27.477982: Epoch 943\n",
            "2025-12-23 04:01:27.478137: Current learning rate: 0.00076\n",
            "2025-12-23 04:01:49.956078: train_loss -0.978\n",
            "2025-12-23 04:01:49.956449: val_loss -0.9878\n",
            "2025-12-23 04:01:49.956672: Pseudo dice [np.float32(0.9902)]\n",
            "2025-12-23 04:01:49.956834: Epoch time: 22.48 s\n",
            "2025-12-23 04:01:49.956964: Yayy! New best EMA pseudo Dice: 0.9898999929428101\n",
            "2025-12-23 04:01:51.835863: \n",
            "2025-12-23 04:01:51.836282: Epoch 944\n",
            "2025-12-23 04:01:51.836428: Current learning rate: 0.00075\n",
            "2025-12-23 04:02:14.339680: train_loss -0.9765\n",
            "2025-12-23 04:02:14.340016: val_loss -0.9878\n",
            "2025-12-23 04:02:14.340241: Pseudo dice [np.float32(0.9903)]\n",
            "2025-12-23 04:02:14.340411: Epoch time: 22.51 s\n",
            "2025-12-23 04:02:14.340554: Yayy! New best EMA pseudo Dice: 0.9900000095367432\n",
            "2025-12-23 04:02:16.186213: \n",
            "2025-12-23 04:02:16.186525: Epoch 945\n",
            "2025-12-23 04:02:16.186660: Current learning rate: 0.00074\n",
            "2025-12-23 04:02:38.628359: train_loss -0.9772\n",
            "2025-12-23 04:02:38.628616: val_loss -0.9859\n",
            "2025-12-23 04:02:38.628708: Pseudo dice [np.float32(0.9896)]\n",
            "2025-12-23 04:02:38.628872: Epoch time: 22.44 s\n",
            "2025-12-23 04:02:39.952824: \n",
            "2025-12-23 04:02:39.953132: Epoch 946\n",
            "2025-12-23 04:02:39.953280: Current learning rate: 0.00072\n",
            "2025-12-23 04:03:02.410148: train_loss -0.9776\n",
            "2025-12-23 04:03:02.410540: val_loss -0.9871\n",
            "2025-12-23 04:03:02.410684: Pseudo dice [np.float32(0.9902)]\n",
            "2025-12-23 04:03:02.410816: Epoch time: 22.46 s\n",
            "2025-12-23 04:03:03.768537: \n",
            "2025-12-23 04:03:03.768735: Epoch 947\n",
            "2025-12-23 04:03:03.768861: Current learning rate: 0.00071\n",
            "2025-12-23 04:03:26.225221: train_loss -0.978\n",
            "2025-12-23 04:03:26.225454: val_loss -0.9871\n",
            "2025-12-23 04:03:26.225603: Pseudo dice [np.float32(0.9899)]\n",
            "2025-12-23 04:03:26.225704: Epoch time: 22.46 s\n",
            "2025-12-23 04:03:27.578540: \n",
            "2025-12-23 04:03:27.578844: Epoch 948\n",
            "2025-12-23 04:03:27.578978: Current learning rate: 0.0007\n",
            "2025-12-23 04:03:50.033211: train_loss -0.9779\n",
            "2025-12-23 04:03:50.033586: val_loss -0.9876\n",
            "2025-12-23 04:03:50.033761: Pseudo dice [np.float32(0.9903)]\n",
            "2025-12-23 04:03:50.033891: Epoch time: 22.46 s\n",
            "2025-12-23 04:03:50.034002: Yayy! New best EMA pseudo Dice: 0.9900000095367432\n",
            "2025-12-23 04:03:51.917131: \n",
            "2025-12-23 04:03:51.917457: Epoch 949\n",
            "2025-12-23 04:03:51.917593: Current learning rate: 0.00069\n",
            "2025-12-23 04:04:14.375855: train_loss -0.9782\n",
            "2025-12-23 04:04:14.376061: val_loss -0.9871\n",
            "2025-12-23 04:04:14.376147: Pseudo dice [np.float32(0.9903)]\n",
            "2025-12-23 04:04:14.376263: Epoch time: 22.46 s\n",
            "2025-12-23 04:04:14.918650: Yayy! New best EMA pseudo Dice: 0.9900000095367432\n",
            "2025-12-23 04:04:16.767728: \n",
            "2025-12-23 04:04:16.768032: Epoch 950\n",
            "2025-12-23 04:04:16.768162: Current learning rate: 0.00067\n",
            "2025-12-23 04:04:39.248709: train_loss -0.9781\n",
            "2025-12-23 04:04:39.248996: val_loss -0.9879\n",
            "2025-12-23 04:04:39.249132: Pseudo dice [np.float32(0.9905)]\n",
            "2025-12-23 04:04:39.249298: Epoch time: 22.48 s\n",
            "2025-12-23 04:04:39.249391: Yayy! New best EMA pseudo Dice: 0.9901000261306763\n",
            "2025-12-23 04:04:41.134164: \n",
            "2025-12-23 04:04:41.134439: Epoch 951\n",
            "2025-12-23 04:04:41.134581: Current learning rate: 0.00066\n",
            "2025-12-23 04:05:03.644102: train_loss -0.978\n",
            "2025-12-23 04:05:03.644509: val_loss -0.9864\n",
            "2025-12-23 04:05:03.644652: Pseudo dice [np.float32(0.9889)]\n",
            "2025-12-23 04:05:03.644771: Epoch time: 22.51 s\n",
            "2025-12-23 04:05:04.995051: \n",
            "2025-12-23 04:05:04.995285: Epoch 952\n",
            "2025-12-23 04:05:04.995426: Current learning rate: 0.00065\n",
            "2025-12-23 04:05:27.416810: train_loss -0.9776\n",
            "2025-12-23 04:05:27.417054: val_loss -0.9871\n",
            "2025-12-23 04:05:27.417173: Pseudo dice [np.float32(0.9898)]\n",
            "2025-12-23 04:05:27.417324: Epoch time: 22.42 s\n",
            "2025-12-23 04:05:28.802743: \n",
            "2025-12-23 04:05:28.802980: Epoch 953\n",
            "2025-12-23 04:05:28.803113: Current learning rate: 0.00064\n",
            "2025-12-23 04:05:51.212672: train_loss -0.9774\n",
            "2025-12-23 04:05:51.212926: val_loss -0.9872\n",
            "2025-12-23 04:05:51.213040: Pseudo dice [np.float32(0.9902)]\n",
            "2025-12-23 04:05:51.213150: Epoch time: 22.41 s\n",
            "2025-12-23 04:05:52.625954: \n",
            "2025-12-23 04:05:52.626136: Epoch 954\n",
            "2025-12-23 04:05:52.626326: Current learning rate: 0.00063\n",
            "2025-12-23 04:06:15.038008: train_loss -0.9772\n",
            "2025-12-23 04:06:15.038414: val_loss -0.9858\n",
            "2025-12-23 04:06:15.038666: Pseudo dice [np.float32(0.9893)]\n",
            "2025-12-23 04:06:15.038853: Epoch time: 22.41 s\n",
            "2025-12-23 04:06:16.406762: \n",
            "2025-12-23 04:06:16.406935: Epoch 955\n",
            "2025-12-23 04:06:16.407060: Current learning rate: 0.00061\n",
            "2025-12-23 04:06:38.823695: train_loss -0.9766\n",
            "2025-12-23 04:06:38.824003: val_loss -0.9864\n",
            "2025-12-23 04:06:38.824237: Pseudo dice [np.float32(0.9894)]\n",
            "2025-12-23 04:06:38.824571: Epoch time: 22.42 s\n",
            "2025-12-23 04:06:40.168160: \n",
            "2025-12-23 04:06:40.168417: Epoch 956\n",
            "2025-12-23 04:06:40.168570: Current learning rate: 0.0006\n",
            "2025-12-23 04:07:02.615892: train_loss -0.9774\n",
            "2025-12-23 04:07:02.616346: val_loss -0.9874\n",
            "2025-12-23 04:07:02.616495: Pseudo dice [np.float32(0.9898)]\n",
            "2025-12-23 04:07:02.616586: Epoch time: 22.45 s\n",
            "2025-12-23 04:07:03.998037: \n",
            "2025-12-23 04:07:03.998281: Epoch 957\n",
            "2025-12-23 04:07:03.998501: Current learning rate: 0.00059\n",
            "2025-12-23 04:07:27.176647: train_loss -0.9772\n",
            "2025-12-23 04:07:27.176942: val_loss -0.9873\n",
            "2025-12-23 04:07:27.177036: Pseudo dice [np.float32(0.9902)]\n",
            "2025-12-23 04:07:27.177134: Epoch time: 23.18 s\n",
            "2025-12-23 04:07:28.529084: \n",
            "2025-12-23 04:07:28.529345: Epoch 958\n",
            "2025-12-23 04:07:28.529544: Current learning rate: 0.00058\n",
            "2025-12-23 04:07:51.039648: train_loss -0.9776\n",
            "2025-12-23 04:07:51.039867: val_loss -0.9868\n",
            "2025-12-23 04:07:51.039978: Pseudo dice [np.float32(0.9906)]\n",
            "2025-12-23 04:07:51.040096: Epoch time: 22.51 s\n",
            "2025-12-23 04:07:52.356472: \n",
            "2025-12-23 04:07:52.356717: Epoch 959\n",
            "2025-12-23 04:07:52.356848: Current learning rate: 0.00056\n",
            "2025-12-23 04:08:14.812300: train_loss -0.9783\n",
            "2025-12-23 04:08:14.812570: val_loss -0.9869\n",
            "2025-12-23 04:08:14.812670: Pseudo dice [np.float32(0.9902)]\n",
            "2025-12-23 04:08:14.812769: Epoch time: 22.46 s\n",
            "2025-12-23 04:08:16.144321: \n",
            "2025-12-23 04:08:16.144668: Epoch 960\n",
            "2025-12-23 04:08:16.144830: Current learning rate: 0.00055\n",
            "2025-12-23 04:08:38.602620: train_loss -0.9779\n",
            "2025-12-23 04:08:38.602879: val_loss -0.9875\n",
            "2025-12-23 04:08:38.603043: Pseudo dice [np.float32(0.9904)]\n",
            "2025-12-23 04:08:38.603150: Epoch time: 22.46 s\n",
            "2025-12-23 04:08:39.946125: \n",
            "2025-12-23 04:08:39.946545: Epoch 961\n",
            "2025-12-23 04:08:39.946685: Current learning rate: 0.00054\n",
            "2025-12-23 04:09:02.451330: train_loss -0.9775\n",
            "2025-12-23 04:09:02.451547: val_loss -0.9863\n",
            "2025-12-23 04:09:02.451677: Pseudo dice [np.float32(0.99)]\n",
            "2025-12-23 04:09:02.451789: Epoch time: 22.51 s\n",
            "2025-12-23 04:09:03.839269: \n",
            "2025-12-23 04:09:03.839558: Epoch 962\n",
            "2025-12-23 04:09:03.839698: Current learning rate: 0.00053\n",
            "2025-12-23 04:09:26.308053: train_loss -0.9778\n",
            "2025-12-23 04:09:26.308338: val_loss -0.9879\n",
            "2025-12-23 04:09:26.308489: Pseudo dice [np.float32(0.991)]\n",
            "2025-12-23 04:09:26.308656: Epoch time: 22.47 s\n",
            "2025-12-23 04:09:26.308830: Yayy! New best EMA pseudo Dice: 0.9901000261306763\n",
            "2025-12-23 04:09:28.275677: \n",
            "2025-12-23 04:09:28.275880: Epoch 963\n",
            "2025-12-23 04:09:28.276031: Current learning rate: 0.00051\n",
            "2025-12-23 04:09:50.746059: train_loss -0.9778\n",
            "2025-12-23 04:09:50.746308: val_loss -0.9874\n",
            "2025-12-23 04:09:50.746428: Pseudo dice [np.float32(0.9896)]\n",
            "2025-12-23 04:09:50.746558: Epoch time: 22.47 s\n",
            "2025-12-23 04:09:52.098727: \n",
            "2025-12-23 04:09:52.099056: Epoch 964\n",
            "2025-12-23 04:09:52.099191: Current learning rate: 0.0005\n",
            "2025-12-23 04:10:14.531443: train_loss -0.9775\n",
            "2025-12-23 04:10:14.531658: val_loss -0.9872\n",
            "2025-12-23 04:10:14.531749: Pseudo dice [np.float32(0.9899)]\n",
            "2025-12-23 04:10:14.531839: Epoch time: 22.43 s\n",
            "2025-12-23 04:10:15.856349: \n",
            "2025-12-23 04:10:15.856688: Epoch 965\n",
            "2025-12-23 04:10:15.856845: Current learning rate: 0.00049\n",
            "2025-12-23 04:10:38.283780: train_loss -0.9781\n",
            "2025-12-23 04:10:38.284118: val_loss -0.9863\n",
            "2025-12-23 04:10:38.284231: Pseudo dice [np.float32(0.9897)]\n",
            "2025-12-23 04:10:38.284368: Epoch time: 22.43 s\n",
            "2025-12-23 04:10:39.641208: \n",
            "2025-12-23 04:10:39.641502: Epoch 966\n",
            "2025-12-23 04:10:39.641645: Current learning rate: 0.00048\n",
            "2025-12-23 04:11:02.059392: train_loss -0.9768\n",
            "2025-12-23 04:11:02.059622: val_loss -0.9863\n",
            "2025-12-23 04:11:02.059716: Pseudo dice [np.float32(0.9892)]\n",
            "2025-12-23 04:11:02.059816: Epoch time: 22.42 s\n",
            "2025-12-23 04:11:03.418969: \n",
            "2025-12-23 04:11:03.419255: Epoch 967\n",
            "2025-12-23 04:11:03.419410: Current learning rate: 0.00046\n",
            "2025-12-23 04:11:25.857456: train_loss -0.9774\n",
            "2025-12-23 04:11:25.857735: val_loss -0.9859\n",
            "2025-12-23 04:11:25.857891: Pseudo dice [np.float32(0.9897)]\n",
            "2025-12-23 04:11:25.857988: Epoch time: 22.44 s\n",
            "2025-12-23 04:11:27.194338: \n",
            "2025-12-23 04:11:27.194644: Epoch 968\n",
            "2025-12-23 04:11:27.194788: Current learning rate: 0.00045\n",
            "2025-12-23 04:11:49.653898: train_loss -0.9777\n",
            "2025-12-23 04:11:49.654141: val_loss -0.9877\n",
            "2025-12-23 04:11:49.654247: Pseudo dice [np.float32(0.9905)]\n",
            "2025-12-23 04:11:49.654342: Epoch time: 22.46 s\n",
            "2025-12-23 04:11:50.992587: \n",
            "2025-12-23 04:11:50.992809: Epoch 969\n",
            "2025-12-23 04:11:50.992975: Current learning rate: 0.00044\n",
            "2025-12-23 04:12:13.417544: train_loss -0.9779\n",
            "2025-12-23 04:12:13.417806: val_loss -0.987\n",
            "2025-12-23 04:12:13.417980: Pseudo dice [np.float32(0.9899)]\n",
            "2025-12-23 04:12:13.418193: Epoch time: 22.43 s\n",
            "2025-12-23 04:12:14.761104: \n",
            "2025-12-23 04:12:14.761440: Epoch 970\n",
            "2025-12-23 04:12:14.761592: Current learning rate: 0.00043\n",
            "2025-12-23 04:12:37.154840: train_loss -0.9779\n",
            "2025-12-23 04:12:37.155116: val_loss -0.987\n",
            "2025-12-23 04:12:37.155292: Pseudo dice [np.float32(0.9899)]\n",
            "2025-12-23 04:12:37.155404: Epoch time: 22.4 s\n",
            "2025-12-23 04:12:38.499554: \n",
            "2025-12-23 04:12:38.499845: Epoch 971\n",
            "2025-12-23 04:12:38.499982: Current learning rate: 0.00041\n",
            "2025-12-23 04:13:00.952731: train_loss -0.9771\n",
            "2025-12-23 04:13:00.952943: val_loss -0.9875\n",
            "2025-12-23 04:13:00.953038: Pseudo dice [np.float32(0.9902)]\n",
            "2025-12-23 04:13:00.953134: Epoch time: 22.45 s\n",
            "2025-12-23 04:13:02.286339: \n",
            "2025-12-23 04:13:02.286657: Epoch 972\n",
            "2025-12-23 04:13:02.286815: Current learning rate: 0.0004\n",
            "2025-12-23 04:13:24.712806: train_loss -0.9776\n",
            "2025-12-23 04:13:24.712987: val_loss -0.9865\n",
            "2025-12-23 04:13:24.713095: Pseudo dice [np.float32(0.9895)]\n",
            "2025-12-23 04:13:24.713209: Epoch time: 22.43 s\n",
            "2025-12-23 04:13:26.064372: \n",
            "2025-12-23 04:13:26.064616: Epoch 973\n",
            "2025-12-23 04:13:26.064753: Current learning rate: 0.00039\n",
            "2025-12-23 04:13:48.468920: train_loss -0.9784\n",
            "2025-12-23 04:13:48.469197: val_loss -0.9872\n",
            "2025-12-23 04:13:48.469419: Pseudo dice [np.float32(0.9906)]\n",
            "2025-12-23 04:13:48.469587: Epoch time: 22.41 s\n",
            "2025-12-23 04:13:49.815523: \n",
            "2025-12-23 04:13:49.815872: Epoch 974\n",
            "2025-12-23 04:13:49.816018: Current learning rate: 0.00037\n",
            "2025-12-23 04:14:12.211477: train_loss -0.9786\n",
            "2025-12-23 04:14:12.211706: val_loss -0.9873\n",
            "2025-12-23 04:14:12.211802: Pseudo dice [np.float32(0.9903)]\n",
            "2025-12-23 04:14:12.211897: Epoch time: 22.4 s\n",
            "2025-12-23 04:14:13.536038: \n",
            "2025-12-23 04:14:13.536241: Epoch 975\n",
            "2025-12-23 04:14:13.536375: Current learning rate: 0.00036\n",
            "2025-12-23 04:14:36.008906: train_loss -0.9781\n",
            "2025-12-23 04:14:36.009277: val_loss -0.9866\n",
            "2025-12-23 04:14:36.009400: Pseudo dice [np.float32(0.9895)]\n",
            "2025-12-23 04:14:36.009520: Epoch time: 22.47 s\n",
            "2025-12-23 04:14:38.066811: \n",
            "2025-12-23 04:14:38.067026: Epoch 976\n",
            "2025-12-23 04:14:38.067169: Current learning rate: 0.00035\n",
            "2025-12-23 04:15:00.559428: train_loss -0.9785\n",
            "2025-12-23 04:15:00.559650: val_loss -0.9874\n",
            "2025-12-23 04:15:00.559737: Pseudo dice [np.float32(0.9902)]\n",
            "2025-12-23 04:15:00.559839: Epoch time: 22.49 s\n",
            "2025-12-23 04:15:01.863405: \n",
            "2025-12-23 04:15:01.863701: Epoch 977\n",
            "2025-12-23 04:15:01.863835: Current learning rate: 0.00034\n",
            "2025-12-23 04:15:24.342965: train_loss -0.9782\n",
            "2025-12-23 04:15:24.343253: val_loss -0.9873\n",
            "2025-12-23 04:15:24.343367: Pseudo dice [np.float32(0.9896)]\n",
            "2025-12-23 04:15:24.343462: Epoch time: 22.48 s\n",
            "2025-12-23 04:15:25.684804: \n",
            "2025-12-23 04:15:25.685136: Epoch 978\n",
            "2025-12-23 04:15:25.685291: Current learning rate: 0.00032\n",
            "2025-12-23 04:15:48.163854: train_loss -0.9778\n",
            "2025-12-23 04:15:48.164107: val_loss -0.9869\n",
            "2025-12-23 04:15:48.164196: Pseudo dice [np.float32(0.9895)]\n",
            "2025-12-23 04:15:48.164326: Epoch time: 22.48 s\n",
            "2025-12-23 04:15:49.466115: \n",
            "2025-12-23 04:15:49.466404: Epoch 979\n",
            "2025-12-23 04:15:49.466577: Current learning rate: 0.00031\n",
            "2025-12-23 04:16:11.942704: train_loss -0.9789\n",
            "2025-12-23 04:16:11.942933: val_loss -0.9866\n",
            "2025-12-23 04:16:11.943023: Pseudo dice [np.float32(0.9898)]\n",
            "2025-12-23 04:16:11.943145: Epoch time: 22.48 s\n",
            "2025-12-23 04:16:13.283639: \n",
            "2025-12-23 04:16:13.283828: Epoch 980\n",
            "2025-12-23 04:16:13.283956: Current learning rate: 0.0003\n",
            "2025-12-23 04:16:35.810985: train_loss -0.9784\n",
            "2025-12-23 04:16:35.811378: val_loss -0.9863\n",
            "2025-12-23 04:16:35.811517: Pseudo dice [np.float32(0.9893)]\n",
            "2025-12-23 04:16:35.811718: Epoch time: 22.53 s\n",
            "2025-12-23 04:16:37.171283: \n",
            "2025-12-23 04:16:37.171580: Epoch 981\n",
            "2025-12-23 04:16:37.171708: Current learning rate: 0.00028\n",
            "2025-12-23 04:16:59.669698: train_loss -0.9776\n",
            "2025-12-23 04:16:59.669899: val_loss -0.9867\n",
            "2025-12-23 04:16:59.669985: Pseudo dice [np.float32(0.9902)]\n",
            "2025-12-23 04:16:59.670072: Epoch time: 22.5 s\n",
            "2025-12-23 04:17:01.047904: \n",
            "2025-12-23 04:17:01.048114: Epoch 982\n",
            "2025-12-23 04:17:01.048270: Current learning rate: 0.00027\n",
            "2025-12-23 04:17:23.533304: train_loss -0.9781\n",
            "2025-12-23 04:17:23.533504: val_loss -0.9872\n",
            "2025-12-23 04:17:23.533603: Pseudo dice [np.float32(0.99)]\n",
            "2025-12-23 04:17:23.533691: Epoch time: 22.49 s\n",
            "2025-12-23 04:17:24.859853: \n",
            "2025-12-23 04:17:24.860192: Epoch 983\n",
            "2025-12-23 04:17:24.860368: Current learning rate: 0.00026\n",
            "2025-12-23 04:17:47.307609: train_loss -0.9782\n",
            "2025-12-23 04:17:47.307859: val_loss -0.9871\n",
            "2025-12-23 04:17:47.307950: Pseudo dice [np.float32(0.99)]\n",
            "2025-12-23 04:17:47.308062: Epoch time: 22.45 s\n",
            "2025-12-23 04:17:48.653699: \n",
            "2025-12-23 04:17:48.653877: Epoch 984\n",
            "2025-12-23 04:17:48.654040: Current learning rate: 0.00024\n",
            "2025-12-23 04:18:11.149267: train_loss -0.9774\n",
            "2025-12-23 04:18:11.149488: val_loss -0.987\n",
            "2025-12-23 04:18:11.149610: Pseudo dice [np.float32(0.99)]\n",
            "2025-12-23 04:18:11.149710: Epoch time: 22.5 s\n",
            "2025-12-23 04:18:12.501658: \n",
            "2025-12-23 04:18:12.502016: Epoch 985\n",
            "2025-12-23 04:18:12.502166: Current learning rate: 0.00023\n",
            "2025-12-23 04:18:34.913891: train_loss -0.9781\n",
            "2025-12-23 04:18:34.914157: val_loss -0.9864\n",
            "2025-12-23 04:18:34.914317: Pseudo dice [np.float32(0.9899)]\n",
            "2025-12-23 04:18:34.914524: Epoch time: 22.41 s\n",
            "2025-12-23 04:18:36.287246: \n",
            "2025-12-23 04:18:36.287520: Epoch 986\n",
            "2025-12-23 04:18:36.287714: Current learning rate: 0.00021\n",
            "2025-12-23 04:18:58.716840: train_loss -0.9783\n",
            "2025-12-23 04:18:58.717056: val_loss -0.9871\n",
            "2025-12-23 04:18:58.717146: Pseudo dice [np.float32(0.9899)]\n",
            "2025-12-23 04:18:58.717257: Epoch time: 22.43 s\n",
            "2025-12-23 04:19:00.104540: \n",
            "2025-12-23 04:19:00.104810: Epoch 987\n",
            "2025-12-23 04:19:00.104949: Current learning rate: 0.0002\n",
            "2025-12-23 04:19:22.596869: train_loss -0.9777\n",
            "2025-12-23 04:19:22.597084: val_loss -0.9876\n",
            "2025-12-23 04:19:22.597170: Pseudo dice [np.float32(0.9899)]\n",
            "2025-12-23 04:19:22.597289: Epoch time: 22.49 s\n",
            "2025-12-23 04:19:23.941353: \n",
            "2025-12-23 04:19:23.941663: Epoch 988\n",
            "2025-12-23 04:19:23.941850: Current learning rate: 0.00019\n",
            "2025-12-23 04:19:46.413707: train_loss -0.9779\n",
            "2025-12-23 04:19:46.413999: val_loss -0.9874\n",
            "2025-12-23 04:19:46.414158: Pseudo dice [np.float32(0.9901)]\n",
            "2025-12-23 04:19:46.414329: Epoch time: 22.47 s\n",
            "2025-12-23 04:19:47.779349: \n",
            "2025-12-23 04:19:47.779684: Epoch 989\n",
            "2025-12-23 04:19:47.779817: Current learning rate: 0.00017\n",
            "2025-12-23 04:20:10.187947: train_loss -0.978\n",
            "2025-12-23 04:20:10.188195: val_loss -0.988\n",
            "2025-12-23 04:20:10.188421: Pseudo dice [np.float32(0.9904)]\n",
            "2025-12-23 04:20:10.188534: Epoch time: 22.41 s\n",
            "2025-12-23 04:20:11.560416: \n",
            "2025-12-23 04:20:11.560720: Epoch 990\n",
            "2025-12-23 04:20:11.560877: Current learning rate: 0.00016\n",
            "2025-12-23 04:20:33.992529: train_loss -0.9776\n",
            "2025-12-23 04:20:33.992788: val_loss -0.987\n",
            "2025-12-23 04:20:33.992905: Pseudo dice [np.float32(0.9896)]\n",
            "2025-12-23 04:20:33.993020: Epoch time: 22.43 s\n",
            "2025-12-23 04:20:35.328890: \n",
            "2025-12-23 04:20:35.329205: Epoch 991\n",
            "2025-12-23 04:20:35.329415: Current learning rate: 0.00014\n",
            "2025-12-23 04:20:57.812837: train_loss -0.9787\n",
            "2025-12-23 04:20:57.813101: val_loss -0.986\n",
            "2025-12-23 04:20:57.813195: Pseudo dice [np.float32(0.9895)]\n",
            "2025-12-23 04:20:57.813337: Epoch time: 22.49 s\n",
            "2025-12-23 04:20:59.168171: \n",
            "2025-12-23 04:20:59.168383: Epoch 992\n",
            "2025-12-23 04:20:59.168574: Current learning rate: 0.00013\n",
            "2025-12-23 04:21:21.650471: train_loss -0.978\n",
            "2025-12-23 04:21:21.650807: val_loss -0.9878\n",
            "2025-12-23 04:21:21.650954: Pseudo dice [np.float32(0.9907)]\n",
            "2025-12-23 04:21:21.651083: Epoch time: 22.48 s\n",
            "2025-12-23 04:21:23.003503: \n",
            "2025-12-23 04:21:23.003809: Epoch 993\n",
            "2025-12-23 04:21:23.003939: Current learning rate: 0.00011\n",
            "2025-12-23 04:21:45.440833: train_loss -0.9778\n",
            "2025-12-23 04:21:45.441032: val_loss -0.9869\n",
            "2025-12-23 04:21:45.441118: Pseudo dice [np.float32(0.9896)]\n",
            "2025-12-23 04:21:45.441208: Epoch time: 22.44 s\n",
            "2025-12-23 04:21:47.464292: \n",
            "2025-12-23 04:21:47.464673: Epoch 994\n",
            "2025-12-23 04:21:47.464823: Current learning rate: 0.0001\n",
            "2025-12-23 04:22:09.938090: train_loss -0.9788\n",
            "2025-12-23 04:22:09.938374: val_loss -0.9868\n",
            "2025-12-23 04:22:09.938592: Pseudo dice [np.float32(0.9901)]\n",
            "2025-12-23 04:22:09.938704: Epoch time: 22.48 s\n",
            "2025-12-23 04:22:11.276600: \n",
            "2025-12-23 04:22:11.276951: Epoch 995\n",
            "2025-12-23 04:22:11.277090: Current learning rate: 8e-05\n",
            "2025-12-23 04:22:33.789846: train_loss -0.9786\n",
            "2025-12-23 04:22:33.790100: val_loss -0.9865\n",
            "2025-12-23 04:22:33.790190: Pseudo dice [np.float32(0.9897)]\n",
            "2025-12-23 04:22:33.790331: Epoch time: 22.51 s\n",
            "2025-12-23 04:22:35.112874: \n",
            "2025-12-23 04:22:35.113086: Epoch 996\n",
            "2025-12-23 04:22:35.113229: Current learning rate: 7e-05\n",
            "2025-12-23 04:22:57.584316: train_loss -0.979\n",
            "2025-12-23 04:22:57.584578: val_loss -0.9866\n",
            "2025-12-23 04:22:57.584693: Pseudo dice [np.float32(0.9898)]\n",
            "2025-12-23 04:22:57.584899: Epoch time: 22.47 s\n",
            "2025-12-23 04:22:58.901665: \n",
            "2025-12-23 04:22:58.901969: Epoch 997\n",
            "2025-12-23 04:22:58.902104: Current learning rate: 5e-05\n",
            "2025-12-23 04:23:21.355298: train_loss -0.9787\n",
            "2025-12-23 04:23:21.355540: val_loss -0.9867\n",
            "2025-12-23 04:23:21.355670: Pseudo dice [np.float32(0.99)]\n",
            "2025-12-23 04:23:21.355766: Epoch time: 22.45 s\n",
            "2025-12-23 04:23:22.707287: \n",
            "2025-12-23 04:23:22.707515: Epoch 998\n",
            "2025-12-23 04:23:22.707733: Current learning rate: 4e-05\n",
            "2025-12-23 04:23:45.175051: train_loss -0.979\n",
            "2025-12-23 04:23:45.175286: val_loss -0.988\n",
            "2025-12-23 04:23:45.175398: Pseudo dice [np.float32(0.9903)]\n",
            "2025-12-23 04:23:45.175492: Epoch time: 22.47 s\n",
            "2025-12-23 04:23:46.509745: \n",
            "2025-12-23 04:23:46.509949: Epoch 999\n",
            "2025-12-23 04:23:46.510077: Current learning rate: 2e-05\n",
            "2025-12-23 04:24:09.031311: train_loss -0.9791\n",
            "2025-12-23 04:24:09.031530: val_loss -0.9865\n",
            "2025-12-23 04:24:09.031621: Pseudo dice [np.float32(0.9896)]\n",
            "2025-12-23 04:24:09.031716: Epoch time: 22.52 s\n",
            "2025-12-23 04:24:10.790586: Training done.\n",
            "2025-12-23 04:24:10.845422: Using splits from existing split file: /content/nnUNet/nnUNet_preprocessed/Dataset501_KSSD/splits_final.json\n",
            "2025-12-23 04:24:10.846611: The split file contains 5 splits.\n",
            "2025-12-23 04:24:10.846740: Desired fold for training: 0\n",
            "2025-12-23 04:24:10.846828: This split has 670 training and 168 validation cases.\n",
            "2025-12-23 04:24:10.852844: predicting case_0000\n",
            "2025-12-23 04:24:10.858293: case_0000, shape torch.Size([1, 1, 472, 398]), rank 0\n",
            "2025-12-23 04:24:38.001486: predicting case_0003\n",
            "2025-12-23 04:24:38.004021: case_0003, shape torch.Size([1, 1, 512, 432]), rank 0\n",
            "2025-12-23 04:24:38.035100: predicting case_0004\n",
            "2025-12-23 04:24:38.036722: case_0004, shape torch.Size([1, 1, 512, 432]), rank 0\n",
            "2025-12-23 04:24:38.066006: predicting case_0006\n",
            "2025-12-23 04:24:38.067906: case_0006, shape torch.Size([1, 1, 511, 400]), rank 0\n",
            "2025-12-23 04:24:38.097249: predicting case_0010\n",
            "2025-12-23 04:24:38.099161: case_0010, shape torch.Size([1, 1, 512, 432]), rank 0\n",
            "2025-12-23 04:24:38.126355: predicting case_0011\n",
            "2025-12-23 04:24:38.128359: case_0011, shape torch.Size([1, 1, 512, 432]), rank 0\n",
            "2025-12-23 04:24:38.154796: predicting case_0027\n",
            "2025-12-23 04:24:38.156865: case_0027, shape torch.Size([1, 1, 512, 432]), rank 0\n",
            "2025-12-23 04:24:38.185480: predicting case_0030\n",
            "2025-12-23 04:24:38.187508: case_0030, shape torch.Size([1, 1, 512, 432]), rank 0\n",
            "2025-12-23 04:24:38.213789: predicting case_0033\n",
            "2025-12-23 04:24:38.215519: case_0033, shape torch.Size([1, 1, 511, 432]), rank 0\n",
            "2025-12-23 04:24:38.244116: predicting case_0035\n",
            "2025-12-23 04:24:38.246094: case_0035, shape torch.Size([1, 1, 511, 431]), rank 0\n",
            "2025-12-23 04:24:38.272252: predicting case_0053\n",
            "2025-12-23 04:24:38.274323: case_0053, shape torch.Size([1, 1, 512, 432]), rank 0\n",
            "2025-12-23 04:24:38.300027: predicting case_0054\n",
            "2025-12-23 04:24:38.301764: case_0054, shape torch.Size([1, 1, 511, 432]), rank 0\n",
            "2025-12-23 04:24:38.327493: predicting case_0055\n",
            "2025-12-23 04:24:38.329031: case_0055, shape torch.Size([1, 1, 512, 432]), rank 0\n",
            "2025-12-23 04:24:38.355068: predicting case_0067\n",
            "2025-12-23 04:24:38.356618: case_0067, shape torch.Size([1, 1, 512, 432]), rank 0\n",
            "2025-12-23 04:24:38.388992: predicting case_0068\n",
            "2025-12-23 04:24:38.390672: case_0068, shape torch.Size([1, 1, 511, 432]), rank 0\n",
            "2025-12-23 04:24:38.416837: predicting case_0073\n",
            "2025-12-23 04:24:38.418355: case_0073, shape torch.Size([1, 1, 512, 424]), rank 0\n",
            "2025-12-23 04:24:38.444176: predicting case_0075\n",
            "2025-12-23 04:24:38.445606: case_0075, shape torch.Size([1, 1, 512, 424]), rank 0\n",
            "2025-12-23 04:24:38.470928: predicting case_0082\n",
            "2025-12-23 04:24:38.472712: case_0082, shape torch.Size([1, 1, 512, 408]), rank 0\n",
            "2025-12-23 04:24:38.498566: predicting case_0087\n",
            "2025-12-23 04:24:38.500190: case_0087, shape torch.Size([1, 1, 512, 408]), rank 0\n",
            "2025-12-23 04:24:38.526352: predicting case_0099\n",
            "2025-12-23 04:24:38.527975: case_0099, shape torch.Size([1, 1, 512, 408]), rank 0\n",
            "2025-12-23 04:24:38.554761: predicting case_0106\n",
            "2025-12-23 04:24:38.556413: case_0106, shape torch.Size([1, 1, 512, 408]), rank 0\n",
            "2025-12-23 04:24:38.583343: predicting case_0109\n",
            "2025-12-23 04:24:38.584997: case_0109, shape torch.Size([1, 1, 511, 408]), rank 0\n",
            "2025-12-23 04:24:38.617421: predicting case_0111\n",
            "2025-12-23 04:24:38.619134: case_0111, shape torch.Size([1, 1, 512, 408]), rank 0\n",
            "2025-12-23 04:24:38.644879: predicting case_0115\n",
            "2025-12-23 04:24:38.646754: case_0115, shape torch.Size([1, 1, 512, 408]), rank 0\n",
            "2025-12-23 04:24:38.672446: predicting case_0117\n",
            "2025-12-23 04:24:38.674632: case_0117, shape torch.Size([1, 1, 512, 407]), rank 0\n",
            "2025-12-23 04:24:38.703066: predicting case_0119\n",
            "2025-12-23 04:24:38.705207: case_0119, shape torch.Size([1, 1, 512, 408]), rank 0\n",
            "2025-12-23 04:24:38.731171: predicting case_0123\n",
            "2025-12-23 04:24:38.733328: case_0123, shape torch.Size([1, 1, 512, 408]), rank 0\n",
            "2025-12-23 04:24:38.760920: predicting case_0127\n",
            "2025-12-23 04:24:38.762826: case_0127, shape torch.Size([1, 1, 512, 408]), rank 0\n",
            "2025-12-23 04:24:38.789437: predicting case_0130\n",
            "2025-12-23 04:24:38.791437: case_0130, shape torch.Size([1, 1, 512, 408]), rank 0\n",
            "2025-12-23 04:24:38.819098: predicting case_0133\n",
            "2025-12-23 04:24:38.821362: case_0133, shape torch.Size([1, 1, 512, 408]), rank 0\n",
            "2025-12-23 04:24:38.847897: predicting case_0135\n",
            "2025-12-23 04:24:38.849742: case_0135, shape torch.Size([1, 1, 512, 408]), rank 0\n",
            "2025-12-23 04:24:38.875544: predicting case_0136\n",
            "2025-12-23 04:24:38.877573: case_0136, shape torch.Size([1, 1, 512, 408]), rank 0\n",
            "2025-12-23 04:24:38.903896: predicting case_0138\n",
            "2025-12-23 04:24:38.905969: case_0138, shape torch.Size([1, 1, 512, 408]), rank 0\n",
            "2025-12-23 04:24:38.931944: predicting case_0144\n",
            "2025-12-23 04:24:38.934038: case_0144, shape torch.Size([1, 1, 512, 408]), rank 0\n",
            "2025-12-23 04:24:38.959737: predicting case_0149\n",
            "2025-12-23 04:24:38.961734: case_0149, shape torch.Size([1, 1, 512, 408]), rank 0\n",
            "2025-12-23 04:24:38.988286: predicting case_0150\n",
            "2025-12-23 04:24:38.990155: case_0150, shape torch.Size([1, 1, 512, 408]), rank 0\n",
            "2025-12-23 04:24:39.016608: predicting case_0158\n",
            "2025-12-23 04:24:39.018858: case_0158, shape torch.Size([1, 1, 512, 407]), rank 0\n",
            "2025-12-23 04:24:39.045970: predicting case_0164\n",
            "2025-12-23 04:24:39.047924: case_0164, shape torch.Size([1, 1, 512, 408]), rank 0\n",
            "2025-12-23 04:24:39.074758: predicting case_0168\n",
            "2025-12-23 04:24:39.076930: case_0168, shape torch.Size([1, 1, 512, 408]), rank 0\n",
            "2025-12-23 04:24:39.103390: predicting case_0173\n",
            "2025-12-23 04:24:39.105277: case_0173, shape torch.Size([1, 1, 512, 424]), rank 0\n",
            "2025-12-23 04:24:39.131393: predicting case_0184\n",
            "2025-12-23 04:24:39.133479: case_0184, shape torch.Size([1, 1, 511, 408]), rank 0\n",
            "2025-12-23 04:24:39.158973: predicting case_0186\n",
            "2025-12-23 04:24:39.160984: case_0186, shape torch.Size([1, 1, 512, 408]), rank 0\n",
            "2025-12-23 04:24:39.186676: predicting case_0187\n",
            "2025-12-23 04:24:39.188663: case_0187, shape torch.Size([1, 1, 512, 408]), rank 0\n",
            "2025-12-23 04:24:39.219653: predicting case_0188\n",
            "2025-12-23 04:24:39.221839: case_0188, shape torch.Size([1, 1, 479, 416]), rank 0\n",
            "2025-12-23 04:24:39.250480: predicting case_0190\n",
            "2025-12-23 04:24:39.252391: case_0190, shape torch.Size([1, 1, 479, 412]), rank 0\n",
            "2025-12-23 04:24:39.278805: predicting case_0192\n",
            "2025-12-23 04:24:39.280920: case_0192, shape torch.Size([1, 1, 472, 392]), rank 0\n",
            "2025-12-23 04:24:39.306404: predicting case_0195\n",
            "2025-12-23 04:24:39.308208: case_0195, shape torch.Size([1, 1, 431, 376]), rank 0\n",
            "2025-12-23 04:24:39.333382: predicting case_0196\n",
            "2025-12-23 04:24:39.335137: case_0196, shape torch.Size([1, 1, 431, 376]), rank 0\n",
            "2025-12-23 04:24:39.360085: predicting case_0198\n",
            "2025-12-23 04:24:39.361971: case_0198, shape torch.Size([1, 1, 479, 408]), rank 0\n",
            "2025-12-23 04:24:39.386887: predicting case_0199\n",
            "2025-12-23 04:24:39.388852: case_0199, shape torch.Size([1, 1, 495, 440]), rank 0\n",
            "2025-12-23 04:24:39.414181: predicting case_0203\n",
            "2025-12-23 04:24:39.416130: case_0203, shape torch.Size([1, 1, 493, 440]), rank 0\n",
            "2025-12-23 04:24:39.442364: predicting case_0215\n",
            "2025-12-23 04:24:39.444117: case_0215, shape torch.Size([1, 1, 496, 440]), rank 0\n",
            "2025-12-23 04:24:39.469740: predicting case_0216\n",
            "2025-12-23 04:24:39.471546: case_0216, shape torch.Size([1, 1, 495, 440]), rank 0\n",
            "2025-12-23 04:24:39.498188: predicting case_0220\n",
            "2025-12-23 04:24:39.499933: case_0220, shape torch.Size([1, 1, 469, 392]), rank 0\n",
            "2025-12-23 04:24:39.525392: predicting case_0222\n",
            "2025-12-23 04:24:39.527087: case_0222, shape torch.Size([1, 1, 472, 400]), rank 0\n",
            "2025-12-23 04:24:39.552025: predicting case_0238\n",
            "2025-12-23 04:24:39.554155: case_0238, shape torch.Size([1, 1, 472, 392]), rank 0\n",
            "2025-12-23 04:24:39.579035: predicting case_0241\n",
            "2025-12-23 04:24:39.581144: case_0241, shape torch.Size([1, 1, 472, 392]), rank 0\n",
            "2025-12-23 04:24:39.606483: predicting case_0244\n",
            "2025-12-23 04:24:39.608064: case_0244, shape torch.Size([1, 1, 504, 408]), rank 0\n",
            "2025-12-23 04:24:39.633250: predicting case_0254\n",
            "2025-12-23 04:24:39.635094: case_0254, shape torch.Size([1, 1, 512, 408]), rank 0\n",
            "2025-12-23 04:24:39.660393: predicting case_0257\n",
            "2025-12-23 04:24:39.662378: case_0257, shape torch.Size([1, 1, 512, 408]), rank 0\n",
            "2025-12-23 04:24:39.688287: predicting case_0262\n",
            "2025-12-23 04:24:39.690051: case_0262, shape torch.Size([1, 1, 511, 407]), rank 0\n",
            "2025-12-23 04:24:39.716799: predicting case_0266\n",
            "2025-12-23 04:24:39.718857: case_0266, shape torch.Size([1, 1, 512, 408]), rank 0\n",
            "2025-12-23 04:24:39.745181: predicting case_0270\n",
            "2025-12-23 04:24:39.747019: case_0270, shape torch.Size([1, 1, 504, 408]), rank 0\n",
            "2025-12-23 04:24:39.772034: predicting case_0280\n",
            "2025-12-23 04:24:39.773602: case_0280, shape torch.Size([1, 1, 512, 456]), rank 0\n",
            "2025-12-23 04:24:39.819375: predicting case_0281\n",
            "2025-12-23 04:24:39.820982: case_0281, shape torch.Size([1, 1, 512, 456]), rank 0\n",
            "2025-12-23 04:24:39.862046: predicting case_0284\n",
            "2025-12-23 04:24:39.863757: case_0284, shape torch.Size([1, 1, 512, 456]), rank 0\n",
            "2025-12-23 04:24:39.905312: predicting case_0296\n",
            "2025-12-23 04:24:39.907033: case_0296, shape torch.Size([1, 1, 494, 431]), rank 0\n",
            "2025-12-23 04:24:39.934053: predicting case_0298\n",
            "2025-12-23 04:24:39.935754: case_0298, shape torch.Size([1, 1, 496, 430]), rank 0\n",
            "2025-12-23 04:24:39.961182: predicting case_0300\n",
            "2025-12-23 04:24:39.962770: case_0300, shape torch.Size([1, 1, 495, 437]), rank 0\n",
            "2025-12-23 04:24:39.988385: predicting case_0305\n",
            "2025-12-23 04:24:39.989973: case_0305, shape torch.Size([1, 1, 494, 428]), rank 0\n",
            "2025-12-23 04:24:40.015143: predicting case_0311\n",
            "2025-12-23 04:24:40.016736: case_0311, shape torch.Size([1, 1, 496, 422]), rank 0\n",
            "2025-12-23 04:24:40.042670: predicting case_0314\n",
            "2025-12-23 04:24:40.044185: case_0314, shape torch.Size([1, 1, 495, 424]), rank 0\n",
            "2025-12-23 04:24:40.074872: predicting case_0322\n",
            "2025-12-23 04:24:40.076614: case_0322, shape torch.Size([1, 1, 512, 400]), rank 0\n",
            "2025-12-23 04:24:40.102431: predicting case_0323\n",
            "2025-12-23 04:24:40.104084: case_0323, shape torch.Size([1, 1, 512, 400]), rank 0\n",
            "2025-12-23 04:24:40.129160: predicting case_0326\n",
            "2025-12-23 04:24:40.130790: case_0326, shape torch.Size([1, 1, 512, 400]), rank 0\n",
            "2025-12-23 04:24:40.156361: predicting case_0328\n",
            "2025-12-23 04:24:40.158005: case_0328, shape torch.Size([1, 1, 512, 400]), rank 0\n",
            "2025-12-23 04:24:40.182849: predicting case_0330\n",
            "2025-12-23 04:24:40.184691: case_0330, shape torch.Size([1, 1, 512, 400]), rank 0\n",
            "2025-12-23 04:24:40.210511: predicting case_0331\n",
            "2025-12-23 04:24:40.212121: case_0331, shape torch.Size([1, 1, 512, 400]), rank 0\n",
            "2025-12-23 04:24:40.237261: predicting case_0334\n",
            "2025-12-23 04:24:40.238809: case_0334, shape torch.Size([1, 1, 512, 400]), rank 0\n",
            "2025-12-23 04:24:40.263666: predicting case_0346\n",
            "2025-12-23 04:24:40.265317: case_0346, shape torch.Size([1, 1, 512, 400]), rank 0\n",
            "2025-12-23 04:24:40.296202: predicting case_0358\n",
            "2025-12-23 04:24:40.298137: case_0358, shape torch.Size([1, 1, 512, 415]), rank 0\n",
            "2025-12-23 04:24:40.324410: predicting case_0359\n",
            "2025-12-23 04:24:40.326310: case_0359, shape torch.Size([1, 1, 512, 416]), rank 0\n",
            "2025-12-23 04:24:40.351891: predicting case_0361\n",
            "2025-12-23 04:24:40.353464: case_0361, shape torch.Size([1, 1, 512, 416]), rank 0\n",
            "2025-12-23 04:24:40.380310: predicting case_0365\n",
            "2025-12-23 04:24:40.382506: case_0365, shape torch.Size([1, 1, 512, 416]), rank 0\n",
            "2025-12-23 04:24:40.408541: predicting case_0366\n",
            "2025-12-23 04:24:40.410385: case_0366, shape torch.Size([1, 1, 512, 416]), rank 0\n",
            "2025-12-23 04:24:40.435848: predicting case_0373\n",
            "2025-12-23 04:24:40.437580: case_0373, shape torch.Size([1, 1, 512, 416]), rank 0\n",
            "2025-12-23 04:24:40.463097: predicting case_0375\n",
            "2025-12-23 04:24:40.464845: case_0375, shape torch.Size([1, 1, 512, 416]), rank 0\n",
            "2025-12-23 04:24:40.490731: predicting case_0376\n",
            "2025-12-23 04:24:40.492265: case_0376, shape torch.Size([1, 1, 512, 416]), rank 0\n",
            "2025-12-23 04:24:40.523675: predicting case_0385\n",
            "2025-12-23 04:24:40.525159: case_0385, shape torch.Size([1, 1, 512, 416]), rank 0\n",
            "2025-12-23 04:24:40.550859: predicting case_0388\n",
            "2025-12-23 04:24:40.552384: case_0388, shape torch.Size([1, 1, 512, 416]), rank 0\n",
            "2025-12-23 04:24:40.578279: predicting case_0395\n",
            "2025-12-23 04:24:40.579807: case_0395, shape torch.Size([1, 1, 512, 416]), rank 0\n",
            "2025-12-23 04:24:40.605998: predicting case_0404\n",
            "2025-12-23 04:24:40.607671: case_0404, shape torch.Size([1, 1, 512, 423]), rank 0\n",
            "2025-12-23 04:24:40.634744: predicting case_0413\n",
            "2025-12-23 04:24:40.636314: case_0413, shape torch.Size([1, 1, 512, 424]), rank 0\n",
            "2025-12-23 04:24:40.662565: predicting case_0414\n",
            "2025-12-23 04:24:40.664097: case_0414, shape torch.Size([1, 1, 512, 424]), rank 0\n",
            "2025-12-23 04:24:40.692179: predicting case_0416\n",
            "2025-12-23 04:24:40.694055: case_0416, shape torch.Size([1, 1, 512, 424]), rank 0\n",
            "2025-12-23 04:24:40.720664: predicting case_0424\n",
            "2025-12-23 04:24:40.722515: case_0424, shape torch.Size([1, 1, 512, 424]), rank 0\n",
            "2025-12-23 04:24:40.754858: predicting case_0440\n",
            "2025-12-23 04:24:40.756390: case_0440, shape torch.Size([1, 1, 512, 424]), rank 0\n",
            "2025-12-23 04:24:40.782518: predicting case_0452\n",
            "2025-12-23 04:24:40.784388: case_0452, shape torch.Size([1, 1, 512, 424]), rank 0\n",
            "2025-12-23 04:24:40.811738: predicting case_0453\n",
            "2025-12-23 04:24:40.813590: case_0453, shape torch.Size([1, 1, 512, 424]), rank 0\n",
            "2025-12-23 04:24:40.838968: predicting case_0454\n",
            "2025-12-23 04:24:40.840752: case_0454, shape torch.Size([1, 1, 512, 424]), rank 0\n",
            "2025-12-23 04:24:40.866239: predicting case_0455\n",
            "2025-12-23 04:24:40.867754: case_0455, shape torch.Size([1, 1, 512, 414]), rank 0\n",
            "2025-12-23 04:24:40.894040: predicting case_0466\n",
            "2025-12-23 04:24:40.895546: case_0466, shape torch.Size([1, 1, 512, 416]), rank 0\n",
            "2025-12-23 04:24:40.921313: predicting case_0468\n",
            "2025-12-23 04:24:40.922882: case_0468, shape torch.Size([1, 1, 512, 424]), rank 0\n",
            "2025-12-23 04:24:40.948355: predicting case_0469\n",
            "2025-12-23 04:24:40.949889: case_0469, shape torch.Size([1, 1, 512, 424]), rank 0\n",
            "2025-12-23 04:24:40.980504: predicting case_0470\n",
            "2025-12-23 04:24:40.981948: case_0470, shape torch.Size([1, 1, 512, 424]), rank 0\n",
            "2025-12-23 04:24:41.006866: predicting case_0491\n",
            "2025-12-23 04:24:41.008568: case_0491, shape torch.Size([1, 1, 512, 432]), rank 0\n",
            "2025-12-23 04:24:41.034122: predicting case_0496\n",
            "2025-12-23 04:24:41.035961: case_0496, shape torch.Size([1, 1, 512, 432]), rank 0\n",
            "2025-12-23 04:24:41.062083: predicting case_0499\n",
            "2025-12-23 04:24:41.063637: case_0499, shape torch.Size([1, 1, 512, 416]), rank 0\n",
            "2025-12-23 04:24:41.091054: predicting case_0509\n",
            "2025-12-23 04:24:41.092855: case_0509, shape torch.Size([1, 1, 512, 415]), rank 0\n",
            "2025-12-23 04:24:41.118673: predicting case_0511\n",
            "2025-12-23 04:24:41.120195: case_0511, shape torch.Size([1, 1, 512, 400]), rank 0\n",
            "2025-12-23 04:24:41.146131: predicting case_0521\n",
            "2025-12-23 04:24:41.148004: case_0521, shape torch.Size([1, 1, 512, 400]), rank 0\n",
            "2025-12-23 04:24:41.174277: predicting case_0523\n",
            "2025-12-23 04:24:41.175872: case_0523, shape torch.Size([1, 1, 512, 400]), rank 0\n",
            "2025-12-23 04:24:41.206339: predicting case_0527\n",
            "2025-12-23 04:24:41.208107: case_0527, shape torch.Size([1, 1, 512, 400]), rank 0\n",
            "2025-12-23 04:24:41.233736: predicting case_0528\n",
            "2025-12-23 04:24:41.235612: case_0528, shape torch.Size([1, 1, 512, 400]), rank 0\n",
            "2025-12-23 04:24:41.261253: predicting case_0534\n",
            "2025-12-23 04:24:41.263104: case_0534, shape torch.Size([1, 1, 512, 400]), rank 0\n",
            "2025-12-23 04:24:41.288804: predicting case_0543\n",
            "2025-12-23 04:24:41.290671: case_0543, shape torch.Size([1, 1, 512, 408]), rank 0\n",
            "2025-12-23 04:24:41.315940: predicting case_0547\n",
            "2025-12-23 04:24:41.317405: case_0547, shape torch.Size([1, 1, 512, 408]), rank 0\n",
            "2025-12-23 04:24:41.342457: predicting case_0548\n",
            "2025-12-23 04:24:41.344125: case_0548, shape torch.Size([1, 1, 512, 408]), rank 0\n",
            "2025-12-23 04:24:41.368995: predicting case_0564\n",
            "2025-12-23 04:24:41.370754: case_0564, shape torch.Size([1, 1, 512, 416]), rank 0\n",
            "2025-12-23 04:24:41.396905: predicting case_0566\n",
            "2025-12-23 04:24:41.398827: case_0566, shape torch.Size([1, 1, 512, 416]), rank 0\n",
            "2025-12-23 04:24:41.429037: predicting case_0570\n",
            "2025-12-23 04:24:41.430592: case_0570, shape torch.Size([1, 1, 512, 416]), rank 0\n",
            "2025-12-23 04:24:41.455394: predicting case_0575\n",
            "2025-12-23 04:24:41.456857: case_0575, shape torch.Size([1, 1, 472, 398]), rank 0\n",
            "2025-12-23 04:24:41.481390: predicting case_0578\n",
            "2025-12-23 04:24:41.483365: case_0578, shape torch.Size([1, 1, 512, 416]), rank 0\n",
            "2025-12-23 04:24:41.508202: predicting case_0579\n",
            "2025-12-23 04:24:41.510021: case_0579, shape torch.Size([1, 1, 512, 416]), rank 0\n",
            "2025-12-23 04:24:41.535103: predicting case_0581\n",
            "2025-12-23 04:24:41.536875: case_0581, shape torch.Size([1, 1, 512, 416]), rank 0\n",
            "2025-12-23 04:24:41.561412: predicting case_0582\n",
            "2025-12-23 04:24:41.563337: case_0582, shape torch.Size([1, 1, 512, 416]), rank 0\n",
            "2025-12-23 04:24:41.590009: predicting case_0583\n",
            "2025-12-23 04:24:41.591870: case_0583, shape torch.Size([1, 1, 512, 416]), rank 0\n",
            "2025-12-23 04:24:41.616740: predicting case_0587\n",
            "2025-12-23 04:24:41.618593: case_0587, shape torch.Size([1, 1, 512, 416]), rank 0\n",
            "2025-12-23 04:24:41.648915: predicting case_0590\n",
            "2025-12-23 04:24:41.650703: case_0590, shape torch.Size([1, 1, 512, 416]), rank 0\n",
            "2025-12-23 04:24:41.675459: predicting case_0595\n",
            "2025-12-23 04:24:41.676909: case_0595, shape torch.Size([1, 1, 512, 416]), rank 0\n",
            "2025-12-23 04:24:41.702278: predicting case_0603\n",
            "2025-12-23 04:24:41.704217: case_0603, shape torch.Size([1, 1, 512, 423]), rank 0\n",
            "2025-12-23 04:24:41.729164: predicting case_0609\n",
            "2025-12-23 04:24:41.730999: case_0609, shape torch.Size([1, 1, 512, 424]), rank 0\n",
            "2025-12-23 04:24:41.755660: predicting case_0615\n",
            "2025-12-23 04:24:41.757448: case_0615, shape torch.Size([1, 1, 512, 424]), rank 0\n",
            "2025-12-23 04:24:41.782762: predicting case_0617\n",
            "2025-12-23 04:24:41.784592: case_0617, shape torch.Size([1, 1, 512, 424]), rank 0\n",
            "2025-12-23 04:24:41.811048: predicting case_0618\n",
            "2025-12-23 04:24:41.812923: case_0618, shape torch.Size([1, 1, 512, 423]), rank 0\n",
            "2025-12-23 04:24:41.838210: predicting case_0619\n",
            "2025-12-23 04:24:41.840129: case_0619, shape torch.Size([1, 1, 512, 424]), rank 0\n",
            "2025-12-23 04:24:41.871987: predicting case_0628\n",
            "2025-12-23 04:24:41.874281: case_0628, shape torch.Size([1, 1, 512, 424]), rank 0\n",
            "2025-12-23 04:24:41.899508: predicting case_0635\n",
            "2025-12-23 04:24:41.901583: case_0635, shape torch.Size([1, 1, 512, 424]), rank 0\n",
            "2025-12-23 04:24:41.927047: predicting case_0644\n",
            "2025-12-23 04:24:41.928992: case_0644, shape torch.Size([1, 1, 512, 424]), rank 0\n",
            "2025-12-23 04:24:41.954853: predicting case_0646\n",
            "2025-12-23 04:24:41.956698: case_0646, shape torch.Size([1, 1, 512, 424]), rank 0\n",
            "2025-12-23 04:24:41.981603: predicting case_0649\n",
            "2025-12-23 04:24:41.983434: case_0649, shape torch.Size([1, 1, 512, 424]), rank 0\n",
            "2025-12-23 04:24:42.009990: predicting case_0657\n",
            "2025-12-23 04:24:42.011484: case_0657, shape torch.Size([1, 1, 480, 408]), rank 0\n",
            "2025-12-23 04:24:42.036703: predicting case_0670\n",
            "2025-12-23 04:24:42.038150: case_0670, shape torch.Size([1, 1, 512, 368]), rank 0\n",
            "2025-12-23 04:24:42.063262: predicting case_0676\n",
            "2025-12-23 04:24:42.064736: case_0676, shape torch.Size([1, 1, 512, 432]), rank 0\n",
            "2025-12-23 04:24:42.094692: predicting case_0682\n",
            "2025-12-23 04:24:42.096305: case_0682, shape torch.Size([1, 1, 512, 424]), rank 0\n",
            "2025-12-23 04:24:42.121032: predicting case_0697\n",
            "2025-12-23 04:24:42.122517: case_0697, shape torch.Size([1, 1, 504, 392]), rank 0\n",
            "2025-12-23 04:24:42.147882: predicting case_0698\n",
            "2025-12-23 04:24:42.149372: case_0698, shape torch.Size([1, 1, 504, 392]), rank 0\n",
            "2025-12-23 04:24:42.174623: predicting case_0732\n",
            "2025-12-23 04:24:42.176161: case_0732, shape torch.Size([1, 1, 479, 432]), rank 0\n",
            "2025-12-23 04:24:42.201637: predicting case_0735\n",
            "2025-12-23 04:24:42.203137: case_0735, shape torch.Size([1, 1, 479, 430]), rank 0\n",
            "2025-12-23 04:24:42.228621: predicting case_0736\n",
            "2025-12-23 04:24:42.230023: case_0736, shape torch.Size([1, 1, 480, 424]), rank 0\n",
            "2025-12-23 04:24:42.255242: predicting case_0746\n",
            "2025-12-23 04:24:42.256622: case_0746, shape torch.Size([1, 1, 512, 416]), rank 0\n",
            "2025-12-23 04:24:42.281118: predicting case_0756\n",
            "2025-12-23 04:24:42.282562: case_0756, shape torch.Size([1, 1, 472, 368]), rank 0\n",
            "2025-12-23 04:24:42.312690: predicting case_0767\n",
            "2025-12-23 04:24:42.314119: case_0767, shape torch.Size([1, 1, 469, 368]), rank 0\n",
            "2025-12-23 04:24:42.338508: predicting case_0776\n",
            "2025-12-23 04:24:42.339911: case_0776, shape torch.Size([1, 1, 512, 400]), rank 0\n",
            "2025-12-23 04:24:42.364439: predicting case_0781\n",
            "2025-12-23 04:24:42.365855: case_0781, shape torch.Size([1, 1, 512, 400]), rank 0\n",
            "2025-12-23 04:24:42.391981: predicting case_0799\n",
            "2025-12-23 04:24:42.393569: case_0799, shape torch.Size([1, 1, 512, 392]), rank 0\n",
            "2025-12-23 04:24:42.419084: predicting case_0800\n",
            "2025-12-23 04:24:42.420730: case_0800, shape torch.Size([1, 1, 502, 383]), rank 0\n",
            "2025-12-23 04:24:42.445533: predicting case_0802\n",
            "2025-12-23 04:24:42.446928: case_0802, shape torch.Size([1, 1, 497, 384]), rank 0\n",
            "2025-12-23 04:24:42.471715: predicting case_0803\n",
            "2025-12-23 04:24:42.472963: case_0803, shape torch.Size([1, 1, 497, 384]), rank 0\n",
            "2025-12-23 04:24:42.497576: predicting case_0804\n",
            "2025-12-23 04:24:42.499100: case_0804, shape torch.Size([1, 1, 504, 384]), rank 0\n",
            "2025-12-23 04:24:42.529027: predicting case_0805\n",
            "2025-12-23 04:24:42.530354: case_0805, shape torch.Size([1, 1, 503, 384]), rank 0\n",
            "2025-12-23 04:24:42.554910: predicting case_0806\n",
            "2025-12-23 04:24:42.556340: case_0806, shape torch.Size([1, 1, 500, 384]), rank 0\n",
            "2025-12-23 04:24:42.580515: predicting case_0812\n",
            "2025-12-23 04:24:42.581924: case_0812, shape torch.Size([1, 1, 504, 367]), rank 0\n",
            "2025-12-23 04:24:42.607889: predicting case_0813\n",
            "2025-12-23 04:24:42.609301: case_0813, shape torch.Size([1, 1, 500, 368]), rank 0\n",
            "2025-12-23 04:24:42.634190: predicting case_0814\n",
            "2025-12-23 04:24:42.635457: case_0814, shape torch.Size([1, 1, 499, 368]), rank 0\n",
            "2025-12-23 04:24:42.665539: predicting case_0830\n",
            "2025-12-23 04:24:42.666978: case_0830, shape torch.Size([1, 1, 512, 440]), rank 0\n",
            "2025-12-23 04:24:42.692100: predicting case_0833\n",
            "2025-12-23 04:24:42.693465: case_0833, shape torch.Size([1, 1, 512, 440]), rank 0\n",
            "2025-12-23 04:24:42.718911: predicting case_0835\n",
            "2025-12-23 04:24:42.720361: case_0835, shape torch.Size([1, 1, 512, 440]), rank 0\n",
            "2025-12-23 04:24:49.808137: Validation complete\n",
            "2025-12-23 04:24:49.808265: Mean Validation Dice:  0.98538243198826\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls $nnUNet_results/Dataset501*/nnUNetTrainerV2__nnUNetPlans__2d/fold_0\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJDsPRq_A1pg",
        "outputId": "08aaeb38-2241-4897-a6df-5d726f681c32"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ls: cannot access '/content/nnUNet/nnUNet_results/Dataset501*/nnUNetTrainerV2__nnUNetPlans__2d/fold_0': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!echo $nnUNet_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wG6_Wf_weiDz",
        "outputId": "b60457b4-997d-4cde-fcf0-e1bb63c1e799"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/nnUNet/nnUNet_results\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls $nnUNet_results/Dataset501_KSSD2025\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ygjOd3MZewfB",
        "outputId": "e3223ba7-cc12-4450-8af4-3c569b8d64f9"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ls: cannot access '/content/nnUNet/nnUNet_results/Dataset501_KSSD2025': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls -lah /content/nnUNet/nnUNet_results\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WspT-3XmfHa5",
        "outputId": "9b2b3f6e-eac7-44dd-f5f2-6627ca2dedac"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 12K\n",
            "drwxr-xr-x 3 root root 4.0K Dec 22 21:42 \u001b[0m\u001b[01;34m.\u001b[0m/\n",
            "drwxr-xr-x 5 root root 4.0K Dec 22 21:38 \u001b[01;34m..\u001b[0m/\n",
            "drwxr-xr-x 3 root root 4.0K Dec 22 21:42 \u001b[01;34mDataset501_KSSD\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls -lah /content/nnUNet/nnUNet_results/Dataset501*\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9uWqeJFcfXXh",
        "outputId": "87d0d465-986e-4271-cb37-a0a519339f05"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 12K\n",
            "drwxr-xr-x 3 root root 4.0K Dec 22 21:42 \u001b[0m\u001b[01;34m.\u001b[0m/\n",
            "drwxr-xr-x 3 root root 4.0K Dec 22 21:42 \u001b[01;34m..\u001b[0m/\n",
            "drwxr-xr-x 3 root root 4.0K Dec 22 21:42 \u001b[01;34mnnUNetTrainer__nnUNetPlans__2d\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!find /content/nnUNet/nnUNet_results/Dataset501* -maxdepth 2 -type d"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ec9VlmnLfcYI",
        "outputId": "828b45e9-a2b8-47ff-b7eb-45621f9f73a4"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/nnUNet/nnUNet_results/Dataset501_KSSD\n",
            "/content/nnUNet/nnUNet_results/Dataset501_KSSD/nnUNetTrainer__nnUNetPlans__2d\n",
            "/content/nnUNet/nnUNet_results/Dataset501_KSSD/nnUNetTrainer__nnUNetPlans__2d/fold_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lah /content/nnUNet/nnUNet_results/Dataset501_KSSD/nnUNetTrainer__nnUNetPlans__2d/fold_0\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bGA8lByqfgtq",
        "outputId": "6f62d533-fdd9-4bf6-8c3b-d9ace020ed38"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 513M\n",
            "drwxr-xr-x 3 root root 4.0K Dec 23 04:24 .\n",
            "drwxr-xr-x 3 root root 4.0K Dec 22 21:42 ..\n",
            "-rw-r--r-- 1 root root 256M Dec 23 04:09 checkpoint_best.pth\n",
            "-rw-r--r-- 1 root root 256M Dec 23 04:24 checkpoint_final.pth\n",
            "-rw-r--r-- 1 root root 9.1K Dec 22 21:42 debug.json\n",
            "-rw-r--r-- 1 root root 422K Dec 23 04:24 progress.png\n",
            "-rw-r--r-- 1 root root 366K Dec 23 04:24 training_log_2025_12_22_21_42_26.txt\n",
            "drwxr-xr-x 2 root root 4.0K Dec 23 04:24 validation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json, os, glob\n",
        "\n",
        "base_dir = \"/content/nnUNet/nnUNet_results/Dataset501_KSSD/nnUNetTrainer__nnUNetPlans__2d/fold_0/\"\n",
        "log_files = glob.glob(os.path.join(base_dir, \"training_log*.json\")) + \\\n",
        "            glob.glob(os.path.join(base_dir, \"training_log*.txt\"))\n",
        "\n",
        "if not log_files:\n",
        "    print(\"Error: No training log file found in the specified directory.\")\n",
        "    best = None\n",
        "else:\n",
        "    # Sort by modification time to get the latest file, assuming newer is better\n",
        "    log_files.sort(key=os.path.getmtime, reverse=True)\n",
        "    log_path = log_files[0]\n",
        "    print(f\"Found log file: {log_path}\")\n",
        "\n",
        "    best_dice = None\n",
        "    try:\n",
        "        # Attempt to load as JSON (for future compatibility or if nnU-Net changes output format)\n",
        "        with open(log_path, \"r\") as f:\n",
        "            log = json.load(f)\n",
        "\n",
        "        # Original logic for JSON (if 'epochs' key exists)\n",
        "        if \"epochs\" in log:\n",
        "            candidates = []\n",
        "            for e in log[\"epochs\"]:\n",
        "                for path in [\n",
        "                    (\"validation\", \"Dice\"), (\"validation\", \"mean_dice\"),\n",
        "                    (\"validation\", \"foreground_dice\"), (\"val\", \"Dice\"),\n",
        "                    (\"val\", \"mean_dice\"),\n",
        "                ]:\n",
        "                    a = e\n",
        "                    ok = True\n",
        "                    for k in path:\n",
        "                        if isinstance(a, dict) and k in a:\n",
        "                            a = a[k]\n",
        "                        else:\n",
        "                            ok = False\n",
        "                            break\n",
        "                    if ok and isinstance(a, (int, float)):\n",
        "                        candidates.append(a)\n",
        "            if candidates:\n",
        "                best_dice = max(candidates)\n",
        "\n",
        "    except json.JSONDecodeError:\n",
        "        # If not JSON, try parsing as text\n",
        "        print(\"Log file is not JSON, attempting to parse as plain text...\")\n",
        "        with open(log_path, \"r\") as f:\n",
        "            lines = f.readlines()\n",
        "            for line in lines:\n",
        "                if \"Yayy! New best EMA pseudo Dice:\" in line:\n",
        "                    try:\n",
        "                        best_dice = float(line.split(':')[1].strip())\n",
        "                    except (ValueError, IndexError):\n",
        "                        pass # Continue if parsing fails for a specific line\n",
        "\n",
        "    best = best_dice\n",
        "\n",
        "print(\"Best validation Dice (fold 0):\", best)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FPd_aFDSfz5u",
        "outputId": "0db052ce-b3a6-4569-e4d9-187b6835287b"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found log file: /content/nnUNet/nnUNet_results/Dataset501_KSSD/nnUNetTrainer__nnUNetPlans__2d/fold_0/training_log_2025_12_22_21_42_26.txt\n",
            "Log file is not JSON, attempting to parse as plain text...\n",
            "Best validation Dice (fold 0): 9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lah /content/nnUNet/nnUNet_results/Dataset501_KSSD/nnUNetTrainer__nnUNetPlans__2d/fold_0 | grep -i log\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kskj8doKf_KK",
        "outputId": "2ccfea5c-5e44-4a53-b245-c48bc9a0018c"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw-r--r-- 1 root root 366K Dec 23 04:24 training_log_2025_12_22_21_42_26.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!find /content/nnUNet/nnUNet_results/Dataset501_KSSD/nnUNetTrainer__nnUNetPlans__2d/fold_0 -maxdepth 2 -type f -name \"*.json\" -print\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0rKrYPi0gnEi",
        "outputId": "a0c6fbac-3432-4405-c413-cf7477153d2b"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/nnUNet/nnUNet_results/Dataset501_KSSD/nnUNetTrainer__nnUNetPlans__2d/fold_0/validation/summary.json\n",
            "/content/nnUNet/nnUNet_results/Dataset501_KSSD/nnUNetTrainer__nnUNetPlans__2d/fold_0/debug.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "p = \"/content/nnUNet/nnUNet_results/Dataset501_KSSD/nnUNetTrainer__nnUNetPlans__2d/fold_0/validation/summary.json\"\n",
        "with open(p, \"r\") as f:\n",
        "    s = json.load(f)\n",
        "\n",
        "print(json.dumps(s, indent=2)[:4000])  # print first part nicely\n"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmnbP8hAgrWD",
        "outputId": "20160202-d02c-4c13-ccd9-23c38f55559b"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"foreground_mean\": {\n",
            "    \"Dice\": 0.98538243198826,\n",
            "    \"FN\": 1.244047619047619,\n",
            "    \"FP\": 1.8571428571428572,\n",
            "    \"IoU\": 0.9719722515597956,\n",
            "    \"TN\": 261976.04166666666,\n",
            "    \"TP\": 164.85714285714286,\n",
            "    \"n_pred\": 166.71428571428572,\n",
            "    \"n_ref\": 166.10119047619048\n",
            "  },\n",
            "  \"mean\": {\n",
            "    \"1\": {\n",
            "      \"Dice\": 0.98538243198826,\n",
            "      \"FN\": 1.244047619047619,\n",
            "      \"FP\": 1.8571428571428572,\n",
            "      \"IoU\": 0.9719722515597956,\n",
            "      \"TN\": 261976.04166666666,\n",
            "      \"TP\": 164.85714285714286,\n",
            "      \"n_pred\": 166.71428571428572,\n",
            "      \"n_ref\": 166.10119047619048\n",
            "    }\n",
            "  },\n",
            "  \"metric_per_case\": [\n",
            "    {\n",
            "      \"metrics\": {\n",
            "        \"1\": {\n",
            "          \"Dice\": 0.9901960784313726,\n",
            "          \"FN\": 0,\n",
            "          \"FP\": 2,\n",
            "          \"IoU\": 0.9805825242718447,\n",
            "          \"TN\": 262041,\n",
            "          \"TP\": 101,\n",
            "          \"n_pred\": 103,\n",
            "          \"n_ref\": 101\n",
            "        }\n",
            "      },\n",
            "      \"prediction_file\": \"/content/nnUNet/nnUNet_results/Dataset501_KSSD/nnUNetTrainer__nnUNetPlans__2d/fold_0/validation/case_0000.nii.gz\",\n",
            "      \"reference_file\": \"/content/nnUNet/nnUNet_preprocessed/Dataset501_KSSD/gt_segmentations/case_0000.nii.gz\"\n",
            "    },\n",
            "    {\n",
            "      \"metrics\": {\n",
            "        \"1\": {\n",
            "          \"Dice\": 0.958904109589041,\n",
            "          \"FN\": 2,\n",
            "          \"FP\": 1,\n",
            "          \"IoU\": 0.9210526315789473,\n",
            "          \"TN\": 262106,\n",
            "          \"TP\": 35,\n",
            "          \"n_pred\": 36,\n",
            "          \"n_ref\": 37\n",
            "        }\n",
            "      },\n",
            "      \"prediction_file\": \"/content/nnUNet/nnUNet_results/Dataset501_KSSD/nnUNetTrainer__nnUNetPlans__2d/fold_0/validation/case_0003.nii.gz\",\n",
            "      \"reference_file\": \"/content/nnUNet/nnUNet_preprocessed/Dataset501_KSSD/gt_segmentations/case_0003.nii.gz\"\n",
            "    },\n",
            "    {\n",
            "      \"metrics\": {\n",
            "        \"1\": {\n",
            "          \"Dice\": 1.0,\n",
            "          \"FN\": 0,\n",
            "          \"FP\": 0,\n",
            "          \"IoU\": 1.0,\n",
            "          \"TN\": 262120,\n",
            "          \"TP\": 24,\n",
            "          \"n_pred\": 24,\n",
            "          \"n_ref\": 24\n",
            "        }\n",
            "      },\n",
            "      \"prediction_file\": \"/content/nnUNet/nnUNet_results/Dataset501_KSSD/nnUNetTrainer__nnUNetPlans__2d/fold_0/validation/case_0004.nii.gz\",\n",
            "      \"reference_file\": \"/content/nnUNet/nnUNet_preprocessed/Dataset501_KSSD/gt_segmentations/case_0004.nii.gz\"\n",
            "    },\n",
            "    {\n",
            "      \"metrics\": {\n",
            "        \"1\": {\n",
            "          \"Dice\": 0.9333333333333333,\n",
            "          \"FN\": 1,\n",
            "          \"FP\": 0,\n",
            "          \"IoU\": 0.875,\n",
            "          \"TN\": 262136,\n",
            "          \"TP\": 7,\n",
            "          \"n_pred\": 7,\n",
            "          \"n_ref\": 8\n",
            "        }\n",
            "      },\n",
            "      \"prediction_file\": \"/content/nnUNet/nnUNet_results/Dataset501_KSSD/nnUNetTrainer__nnUNetPlans__2d/fold_0/validation/case_0006.nii.gz\",\n",
            "      \"reference_file\": \"/content/nnUNet/nnUNet_preprocessed/Dataset501_KSSD/gt_segmentations/case_0006.nii.gz\"\n",
            "    },\n",
            "    {\n",
            "      \"metrics\": {\n",
            "        \"1\": {\n",
            "          \"Dice\": 0.9585798816568047,\n",
            "          \"FN\": 6,\n",
            "          \"FP\": 1,\n",
            "          \"IoU\": 0.9204545454545454,\n",
            "          \"TN\": 262056,\n",
            "          \"TP\": 81,\n",
            "          \"n_pred\": 82,\n",
            "          \"n_ref\": 87\n",
            "        }\n",
            "      },\n",
            "      \"prediction_file\": \"/content/nnUNet/nnUNet_results/Dataset501_KSSD/nnUNetTrainer__nnUNetPlans__2d/fold_0/validation/case_0010.nii.gz\",\n",
            "      \"reference_file\": \"/content/nnUNet/nnUNet_preprocessed/Dataset501_KSSD/gt_segmentations/case_0010.nii.gz\"\n",
            "    },\n",
            "    {\n",
            "      \"metrics\": {\n",
            "        \"1\": {\n",
            "          \"Dice\": 0.9771689497716894,\n",
            "          \"FN\": 5,\n",
            "          \"FP\": 0,\n",
            "          \"IoU\": 0.9553571428571429,\n",
            "          \"TN\": 262032,\n",
            "          \"TP\": 107,\n",
            "          \"n_pred\": 107,\n",
            "          \"n_ref\": 112\n",
            "        }\n",
            "      },\n",
            "      \"prediction_file\": \"/content/nnUNet/nnUNet_results/Dataset501_KSSD/nnUNetTrainer__nnUNetPlans__2d/fold_0/validation/case_0011.nii.gz\",\n",
            "      \"reference_file\": \"/content/nnUNet/nnUNet_preprocessed/Dataset501_KSSD/gt_segmentations/case_0011.nii.gz\"\n",
            "    },\n",
            "    {\n",
            "      \"metrics\": {\n",
            "        \"1\": {\n",
            "          \"Dice\": 0.9917898193760263,\n",
            "          \"FN\": 3,\n",
            "          \"FP\": 2,\n",
            "          \"IoU\": 0.9837133550488599,\n",
            "          \"TN\": 261837,\n",
            "          \"TP\": 302,\n",
            "          \"n_pred\": 304,\n",
            "          \"n_ref\": 305\n",
            "        }\n",
            "      },\n",
            "      \"prediction_file\": \"/content/nnUNet/nnUNet_\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "p = \"/content/nnUNet/nnUNet_results/Dataset501_KSSD/nnUNetTrainer__nnUNetPlans__2d/fold_0/validation/summary.json\"\n",
        "with open(p, \"r\") as f:\n",
        "    s = json.load(f)\n",
        "\n",
        "def find_dice(obj, path=\"\"):\n",
        "    hits = []\n",
        "    if isinstance(obj, dict):\n",
        "        for k, v in obj.items():\n",
        "            newp = f\"{path}.{k}\" if path else k\n",
        "            if isinstance(v, (int, float)) and (\"dice\" in k.lower() or \"dsc\" in k.lower()):\n",
        "                hits.append((newp, v))\n",
        "            else:\n",
        "                hits += find_dice(v, newp)\n",
        "    elif isinstance(obj, list):\n",
        "        for i, v in enumerate(obj):\n",
        "            hits += find_dice(v, f\"{path}[{i}]\")\n",
        "    return hits\n",
        "\n",
        "hits = find_dice(s)\n",
        "print(\"Dice-like fields found:\")\n",
        "for k, v in hits:\n",
        "    print(k, \"=\", v)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8U--_XQg5vh",
        "outputId": "dbd2580f-5ddf-4d6a-9867-1968a10047d0"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dice-like fields found:\n",
            "foreground_mean.Dice = 0.98538243198826\n",
            "mean.1.Dice = 0.98538243198826\n",
            "metric_per_case[0].metrics.1.Dice = 0.9901960784313726\n",
            "metric_per_case[1].metrics.1.Dice = 0.958904109589041\n",
            "metric_per_case[2].metrics.1.Dice = 1.0\n",
            "metric_per_case[3].metrics.1.Dice = 0.9333333333333333\n",
            "metric_per_case[4].metrics.1.Dice = 0.9585798816568047\n",
            "metric_per_case[5].metrics.1.Dice = 0.9771689497716894\n",
            "metric_per_case[6].metrics.1.Dice = 0.9917898193760263\n",
            "metric_per_case[7].metrics.1.Dice = 0.9896907216494846\n",
            "metric_per_case[8].metrics.1.Dice = 0.9968152866242038\n",
            "metric_per_case[9].metrics.1.Dice = 0.9758620689655172\n",
            "metric_per_case[10].metrics.1.Dice = 0.9818956336528222\n",
            "metric_per_case[11].metrics.1.Dice = 0.9898785425101214\n",
            "metric_per_case[12].metrics.1.Dice = 0.9812382739212008\n",
            "metric_per_case[13].metrics.1.Dice = 0.9923857868020305\n",
            "metric_per_case[14].metrics.1.Dice = 0.983225806451613\n",
            "metric_per_case[15].metrics.1.Dice = 0.9807280513918629\n",
            "metric_per_case[16].metrics.1.Dice = 0.9761904761904762\n",
            "metric_per_case[17].metrics.1.Dice = 0.9902912621359223\n",
            "metric_per_case[18].metrics.1.Dice = 0.9894736842105263\n",
            "metric_per_case[19].metrics.1.Dice = 0.9898373983739838\n",
            "metric_per_case[20].metrics.1.Dice = 0.9791376912378303\n",
            "metric_per_case[21].metrics.1.Dice = 0.994535519125683\n",
            "metric_per_case[22].metrics.1.Dice = 0.9877750611246944\n",
            "metric_per_case[23].metrics.1.Dice = 0.9769230769230769\n",
            "metric_per_case[24].metrics.1.Dice = 0.9770992366412213\n",
            "metric_per_case[25].metrics.1.Dice = 0.9827160493827161\n",
            "metric_per_case[26].metrics.1.Dice = 0.9938900203665988\n",
            "metric_per_case[27].metrics.1.Dice = 0.9904761904761905\n",
            "metric_per_case[28].metrics.1.Dice = 0.986013986013986\n",
            "metric_per_case[29].metrics.1.Dice = 0.9851632047477745\n",
            "metric_per_case[30].metrics.1.Dice = 0.9832775919732442\n",
            "metric_per_case[31].metrics.1.Dice = 0.9888475836431226\n",
            "metric_per_case[32].metrics.1.Dice = 0.995\n",
            "metric_per_case[33].metrics.1.Dice = 0.9740932642487047\n",
            "metric_per_case[34].metrics.1.Dice = 0.9826589595375722\n",
            "metric_per_case[35].metrics.1.Dice = 0.968944099378882\n",
            "metric_per_case[36].metrics.1.Dice = 0.9885057471264368\n",
            "metric_per_case[37].metrics.1.Dice = 0.9903846153846154\n",
            "metric_per_case[38].metrics.1.Dice = 0.9936305732484076\n",
            "metric_per_case[39].metrics.1.Dice = 0.9946524064171123\n",
            "metric_per_case[40].metrics.1.Dice = 0.9777777777777777\n",
            "metric_per_case[41].metrics.1.Dice = 0.9821428571428571\n",
            "metric_per_case[42].metrics.1.Dice = 0.918918918918919\n",
            "metric_per_case[43].metrics.1.Dice = 1.0\n",
            "metric_per_case[44].metrics.1.Dice = 0.975609756097561\n",
            "metric_per_case[45].metrics.1.Dice = 0.9883720930232558\n",
            "metric_per_case[46].metrics.1.Dice = 1.0\n",
            "metric_per_case[47].metrics.1.Dice = 1.0\n",
            "metric_per_case[48].metrics.1.Dice = 1.0\n",
            "metric_per_case[49].metrics.1.Dice = 0.9583333333333334\n",
            "metric_per_case[50].metrics.1.Dice = 0.9710144927536232\n",
            "metric_per_case[51].metrics.1.Dice = 1.0\n",
            "metric_per_case[52].metrics.1.Dice = 1.0\n",
            "metric_per_case[53].metrics.1.Dice = 0.8947368421052632\n",
            "metric_per_case[54].metrics.1.Dice = 1.0\n",
            "metric_per_case[55].metrics.1.Dice = 0.9803921568627451\n",
            "metric_per_case[56].metrics.1.Dice = 0.967741935483871\n",
            "metric_per_case[57].metrics.1.Dice = 0.9714285714285714\n",
            "metric_per_case[58].metrics.1.Dice = 0.99609375\n",
            "metric_per_case[59].metrics.1.Dice = 0.9943502824858758\n",
            "metric_per_case[60].metrics.1.Dice = 0.98989898989899\n",
            "metric_per_case[61].metrics.1.Dice = 0.994535519125683\n",
            "metric_per_case[62].metrics.1.Dice = 0.9824561403508771\n",
            "metric_per_case[63].metrics.1.Dice = 0.9375\n",
            "metric_per_case[64].metrics.1.Dice = 0.9583333333333334\n",
            "metric_per_case[65].metrics.1.Dice = 0.9866666666666667\n",
            "metric_per_case[66].metrics.1.Dice = 0.9\n",
            "metric_per_case[67].metrics.1.Dice = 0.8181818181818182\n",
            "metric_per_case[68].metrics.1.Dice = 1.0\n",
            "metric_per_case[69].metrics.1.Dice = 1.0\n",
            "metric_per_case[70].metrics.1.Dice = 1.0\n",
            "metric_per_case[71].metrics.1.Dice = 1.0\n",
            "metric_per_case[72].metrics.1.Dice = 0.9958368026644463\n",
            "metric_per_case[73].metrics.1.Dice = 0.9976284584980237\n",
            "metric_per_case[74].metrics.1.Dice = 0.9923780487804879\n",
            "metric_per_case[75].metrics.1.Dice = 0.9851411589895989\n",
            "metric_per_case[76].metrics.1.Dice = 0.9979605710401088\n",
            "metric_per_case[77].metrics.1.Dice = 0.9979879275653923\n",
            "metric_per_case[78].metrics.1.Dice = 0.9979716024340771\n",
            "metric_per_case[79].metrics.1.Dice = 0.9736842105263158\n",
            "metric_per_case[80].metrics.1.Dice = 0.9968847352024922\n",
            "metric_per_case[81].metrics.1.Dice = 0.991869918699187\n",
            "metric_per_case[82].metrics.1.Dice = 0.9903181189488244\n",
            "metric_per_case[83].metrics.1.Dice = 0.9753914988814317\n",
            "metric_per_case[84].metrics.1.Dice = 1.0\n",
            "metric_per_case[85].metrics.1.Dice = 0.9961977186311787\n",
            "metric_per_case[86].metrics.1.Dice = 0.9848484848484849\n",
            "metric_per_case[87].metrics.1.Dice = 0.9805194805194806\n",
            "metric_per_case[88].metrics.1.Dice = 0.9896907216494846\n",
            "metric_per_case[89].metrics.1.Dice = 0.9333333333333333\n",
            "metric_per_case[90].metrics.1.Dice = 0.994413407821229\n",
            "metric_per_case[91].metrics.1.Dice = 1.0\n",
            "metric_per_case[92].metrics.1.Dice = 0.9937888198757764\n",
            "metric_per_case[93].metrics.1.Dice = 1.0\n",
            "metric_per_case[94].metrics.1.Dice = 0.9945945945945946\n",
            "metric_per_case[95].metrics.1.Dice = 0.9848484848484849\n",
            "metric_per_case[96].metrics.1.Dice = 0.9865125240847784\n",
            "metric_per_case[97].metrics.1.Dice = 0.9875\n",
            "metric_per_case[98].metrics.1.Dice = 0.9942857142857143\n",
            "metric_per_case[99].metrics.1.Dice = 0.9947089947089947\n",
            "metric_per_case[100].metrics.1.Dice = 0.965034965034965\n",
            "metric_per_case[101].metrics.1.Dice = 0.9784172661870504\n",
            "metric_per_case[102].metrics.1.Dice = 0.98989898989899\n",
            "metric_per_case[103].metrics.1.Dice = 0.9850746268656716\n",
            "metric_per_case[104].metrics.1.Dice = 0.9719626168224299\n",
            "metric_per_case[105].metrics.1.Dice = 0.9809523809523809\n",
            "metric_per_case[106].metrics.1.Dice = 0.9848484848484849\n",
            "metric_per_case[107].metrics.1.Dice = 1.0\n",
            "metric_per_case[108].metrics.1.Dice = 0.9655172413793104\n",
            "metric_per_case[109].metrics.1.Dice = 0.9958437240232751\n",
            "metric_per_case[110].metrics.1.Dice = 0.9993089149965446\n",
            "metric_per_case[111].metrics.1.Dice = 0.99637943519189\n",
            "metric_per_case[112].metrics.1.Dice = 0.9962825278810409\n",
            "metric_per_case[113].metrics.1.Dice = 0.9957173447537473\n",
            "metric_per_case[114].metrics.1.Dice = 0.9850746268656716\n",
            "metric_per_case[115].metrics.1.Dice = 0.9885931558935361\n",
            "metric_per_case[116].metrics.1.Dice = 0.9965870307167235\n",
            "metric_per_case[117].metrics.1.Dice = 0.9963636363636363\n",
            "metric_per_case[118].metrics.1.Dice = 0.9934640522875817\n",
            "metric_per_case[119].metrics.1.Dice = 0.9791666666666666\n",
            "metric_per_case[120].metrics.1.Dice = 0.9936305732484076\n",
            "metric_per_case[121].metrics.1.Dice = 0.9966777408637874\n",
            "metric_per_case[122].metrics.1.Dice = 1.0\n",
            "metric_per_case[123].metrics.1.Dice = 0.967741935483871\n",
            "metric_per_case[124].metrics.1.Dice = 0.9848484848484849\n",
            "metric_per_case[125].metrics.1.Dice = 0.9868421052631579\n",
            "metric_per_case[126].metrics.1.Dice = 0.9803921568627451\n",
            "metric_per_case[127].metrics.1.Dice = 0.9916897506925207\n",
            "metric_per_case[128].metrics.1.Dice = 0.9977528089887641\n",
            "metric_per_case[129].metrics.1.Dice = 0.9938757655293088\n",
            "metric_per_case[130].metrics.1.Dice = 0.9868421052631579\n",
            "metric_per_case[131].metrics.1.Dice = 0.9959595959595959\n",
            "metric_per_case[132].metrics.1.Dice = 0.9948586118251928\n",
            "metric_per_case[133].metrics.1.Dice = 0.9951690821256038\n",
            "metric_per_case[134].metrics.1.Dice = 0.9949066213921901\n",
            "metric_per_case[135].metrics.1.Dice = 0.9964539007092199\n",
            "metric_per_case[136].metrics.1.Dice = 0.9938398357289527\n",
            "metric_per_case[137].metrics.1.Dice = 1.0\n",
            "metric_per_case[138].metrics.1.Dice = 0.9871794871794872\n",
            "metric_per_case[139].metrics.1.Dice = 0.9928057553956835\n",
            "metric_per_case[140].metrics.1.Dice = 0.9911504424778761\n",
            "metric_per_case[141].metrics.1.Dice = 1.0\n",
            "metric_per_case[142].metrics.1.Dice = 0.9696969696969697\n",
            "metric_per_case[143].metrics.1.Dice = 1.0\n",
            "metric_per_case[144].metrics.1.Dice = 1.0\n",
            "metric_per_case[145].metrics.1.Dice = 0.9965397923875432\n",
            "metric_per_case[146].metrics.1.Dice = 1.0\n",
            "metric_per_case[147].metrics.1.Dice = 1.0\n",
            "metric_per_case[148].metrics.1.Dice = 1.0\n",
            "metric_per_case[149].metrics.1.Dice = 1.0\n",
            "metric_per_case[150].metrics.1.Dice = 1.0\n",
            "metric_per_case[151].metrics.1.Dice = 0.9734513274336283\n",
            "metric_per_case[152].metrics.1.Dice = 1.0\n",
            "metric_per_case[153].metrics.1.Dice = 0.9777777777777777\n",
            "metric_per_case[154].metrics.1.Dice = 1.0\n",
            "metric_per_case[155].metrics.1.Dice = 1.0\n",
            "metric_per_case[156].metrics.1.Dice = 1.0\n",
            "metric_per_case[157].metrics.1.Dice = 0.9795918367346939\n",
            "metric_per_case[158].metrics.1.Dice = 0.9866666666666667\n",
            "metric_per_case[159].metrics.1.Dice = 0.9882352941176471\n",
            "metric_per_case[160].metrics.1.Dice = 1.0\n",
            "metric_per_case[161].metrics.1.Dice = 0.9736842105263158\n",
            "metric_per_case[162].metrics.1.Dice = 0.9883720930232558\n",
            "metric_per_case[163].metrics.1.Dice = 0.9894736842105263\n",
            "metric_per_case[164].metrics.1.Dice = 0.9882352941176471\n",
            "metric_per_case[165].metrics.1.Dice = 1.0\n",
            "metric_per_case[166].metrics.1.Dice = 0.9714285714285714\n",
            "metric_per_case[167].metrics.1.Dice = 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train Fold 1"
      ],
      "metadata": {
        "id": "Bvm3yRxBi6s9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nnUNetv2_train 501 2d 1 --device cuda\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tsl_3U0-jAe2",
        "outputId": "29d5fba3-655b-47ae-da1b-2717b97213e5"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: nnUNetv2_train [-h] [-tr TR] [-p P]\n",
            "                      [-pretrained_weights PRETRAINED_WEIGHTS]\n",
            "                      [-num_gpus NUM_GPUS] [--npz] [--c] [--val] [--val_best]\n",
            "                      [--disable_checkpointing] [-device DEVICE]\n",
            "                      dataset_name_or_id configuration fold\n",
            "nnUNetv2_train: error: unrecognized arguments: --device cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nnUNetv2_train 501 2d 1\n"
      ],
      "metadata": {
        "id": "cpBgTZxcjqQm",
        "outputId": "d4f370f5-e287-4b1b-fbc5-3da70ff0f495",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "############################\n",
            "INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md\n",
            "############################\n",
            "\n",
            "Using device: cuda:0\n",
            "\n",
            "#######################################################################\n",
            "Please cite the following paper when using nnU-Net:\n",
            "Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.\n",
            "#######################################################################\n",
            "\n",
            "2025-12-23 04:55:12.660636: Using torch.compile...\n",
            "2025-12-23 04:55:14.191850: do_dummy_2d_data_aug: False\n",
            "2025-12-23 04:55:14.194009: Using splits from existing split file: /content/nnUNet/nnUNet_preprocessed/Dataset501_KSSD/splits_final.json\n",
            "2025-12-23 04:55:14.194555: The split file contains 5 splits.\n",
            "2025-12-23 04:55:14.194623: Desired fold for training: 1\n",
            "2025-12-23 04:55:14.194667: This split has 670 training and 168 validation cases.\n",
            "/usr/local/lib/python3.12/dist-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  output = output[crop_slices].contiguous()\n",
            "using pin_memory on device 0\n",
            "/usr/local/lib/python3.12/dist-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  output = output[crop_slices].contiguous()\n",
            "/usr/local/lib/python3.12/dist-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  output = output[crop_slices].contiguous()\n",
            "/usr/local/lib/python3.12/dist-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  output = output[crop_slices].contiguous()\n",
            "/usr/local/lib/python3.12/dist-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  output = output[crop_slices].contiguous()\n",
            "/usr/local/lib/python3.12/dist-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  output = output[crop_slices].contiguous()\n",
            "/usr/local/lib/python3.12/dist-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  output = output[crop_slices].contiguous()\n",
            "/usr/local/lib/python3.12/dist-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  output = output[crop_slices].contiguous()\n",
            "/usr/local/lib/python3.12/dist-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  output = output[crop_slices].contiguous()\n",
            "/usr/local/lib/python3.12/dist-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  output = output[crop_slices].contiguous()\n",
            "/usr/local/lib/python3.12/dist-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  output = output[crop_slices].contiguous()\n",
            "/usr/local/lib/python3.12/dist-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  output = output[crop_slices].contiguous()\n",
            "using pin_memory on device 0\n",
            "\n",
            "This is the configuration used by this training:\n",
            "Configuration name: 2d\n",
            " {'data_identifier': 'nnUNetPlans_2d', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 14, 'patch_size': [512, 448], 'median_image_size_in_voxels': [512.0, 416.0], 'spacing': [1.0, 1.0], 'normalization_schemes': ['CTNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 7, 'features_per_stage': [32, 64, 128, 256, 512, 512, 512], 'conv_op': 'torch.nn.modules.conv.Conv2d', 'kernel_sizes': [[3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3]], 'strides': [[1, 1], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm2d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True} \n",
            "\n",
            "These are the global plan.json settings:\n",
            " {'dataset_name': 'Dataset501_KSSD', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [1, 512, 416], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 255.0, 'mean': 237.29641723632812, 'median': 255.0, 'min': 95.0, 'percentile_00_5': 162.0, 'percentile_99_5': 255.0, 'std': 27.940317153930664}}} \n",
            "\n",
            "2025-12-23 04:55:16.674852: Unable to plot network architecture: nnUNet_compile is enabled!\n",
            "2025-12-23 04:55:16.691810: \n",
            "2025-12-23 04:55:16.692256: Epoch 0\n",
            "2025-12-23 04:55:16.693943: Current learning rate: 0.01\n",
            "2025-12-23 04:56:54.343931: train_loss -0.0198\n",
            "2025-12-23 04:56:54.344197: val_loss -0.324\n",
            "2025-12-23 04:56:54.344343: Pseudo dice [np.float32(0.4745)]\n",
            "2025-12-23 04:56:54.344469: Epoch time: 97.66 s\n",
            "2025-12-23 04:56:54.344546: Yayy! New best EMA pseudo Dice: 0.47450000047683716\n",
            "2025-12-23 04:56:55.809955: \n",
            "2025-12-23 04:56:55.810315: Epoch 1\n",
            "2025-12-23 04:56:55.810474: Current learning rate: 0.00999\n",
            "2025-12-23 04:57:18.154950: train_loss -0.6092\n",
            "2025-12-23 04:57:18.155159: val_loss -0.8204\n",
            "2025-12-23 04:57:18.155272: Pseudo dice [np.float32(0.9275)]\n",
            "2025-12-23 04:57:18.155385: Epoch time: 22.35 s\n",
            "2025-12-23 04:57:18.155541: Yayy! New best EMA pseudo Dice: 0.5198000073432922\n",
            "2025-12-23 04:57:20.270233: \n",
            "2025-12-23 04:57:20.270571: Epoch 2\n",
            "2025-12-23 04:57:20.270703: Current learning rate: 0.00998\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ri-sl_oAj4Wr"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Making the most of your colab subscription",
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}